---
title: "10 The General Linear Model & Multivariate Statistics"
subtitle: |
   | Julius-Maximilians-University Würzburg 
   | Course: "Biostatistics"    
   | Translational Neuroscience
author: "Dr. Lea Hildebrandt"
format: 
  revealjs:
    smaller: true
    scrollable: true
    slide-number: true
    theme: serif
    chalkboard: true
    width: 1280
    height: 720
from: markdown+emoji
---

# General Linear Model (GLM)

```{css}
code.sourceCode {
  font-size: 1.4em;
}

div.cell-output-stdout {
  font-size: 1.4em;
}
```

```{r}
#| message: false

library(tidyverse)
library(ggplot2)
library(fivethirtyeight)
#library(caret)
library(MASS)
library(cowplot)
library(knitr)
set.seed(123456) # set random seed to exactly replicate results
opts_chunk$set(tidy.opts=list(width.cutoff=80))
options(tibble.width = 60)
# load the NHANES data library
library(NHANES)
# drop duplicated IDs within the NHANES dataset
NHANES <- 
  NHANES %>% 
  dplyr::distinct(ID,.keep_all=TRUE)
NHANES_adult <- 
  NHANES %>%
  drop_na(Weight) %>%
  subset(Age>=18)
```

Remember the basic model of statistics:

$$
data = model + error
$$

Our general goal is to find the model with the *best fit*, i.e., that minimizes the error.

## GLM

One approach is the GLM. You might be surprised that a lot of the common models can be viewed as linear models:

![All models can be thought of as linear models](images/linear_tests_cheat_sheet.png)

## Definitions

**Dependent variable (DV)**: The outcome variable that the model aims to explain ($Y$).

**Independent variable (IV)**: The variable(s) that we use to explain the DV ($X$).

**Linear model**: The model for the DV is composed of a *linear combination* of IVs (that are multiplied by different [weights]{.underline}!)

. . .

The weights are the *parameters* $\beta$ and determine the relative contribution of each IV. (This is what the model estimates! The weights thus give us the important information we're usually interested in: How strong are IV and DV related.)

There may also be several DVs ("*multivariate statistics*"), but usually that's not the case except for specific biopsychological methods (e.g., fMRI). Thus, we will focus on those cases with one DV!

## Example

::: columns
::: column
Let's use some simulated data:

```{r}
#| echo: false
#| fig.width: 3
#| fig.height: 3
#| out.height: '50%'
#| 
# create simulated data for example
set.seed(12345)
# the number of points that having a prior class increases grades
betas <- c(6, 5)
df <-
  tibble(
    studyTime = c(2, 3, 5, 6, 6, 8, 10, 12) / 3,
    priorClass = c(0, 1, 1, 0, 1, 0, 1, 0)
  ) %>%
  mutate(
    grade = 
      studyTime * betas[1] + 
      priorClass * betas[2] + 
      round(rnorm(8, mean = 70, sd = 5))
  )

p <- ggplot(df,aes(studyTime,grade)) +
  geom_point(size=3) +
  xlab('Study time (hours)') +
  ylab('Grade (percent)') +
  xlim(0,5) + 
  ylim(70,100)
print(p)
```
:::

::: column
We can calculate the *correlation* between the two variables:

```{r}
#| echo: false
# compute correlation between grades and study time
corTestResult <- cor.test(df$grade, df$studyTime)
corTestResult
corTestResult %>% apa::cor_apa(r_ci=T)
```

The correlation is quite high (.63), but the CI is also pretty wide.
:::
:::

. . .

Fundamental activities of statistics:

-   *Describe*: How strong is the relationship between grade and study time?

-   *Decide*: Is there a statistically significant relationship between grade and study time?

-   *Predict*: Given a particular amount of study time, what grade do we expect?

::: notes
relationship study time and grade
:::

## Linear Regression

Use the GLM to...

::: incremental
-   describe the relation between two variables (similar to correlation)

-   decide whether an IV is a significant predictor of the DV

-   predict DV for new values of IV (new observations)

-   add multiple IVs!
:::

. . .

::: columns
::: column
Simple GLM (here: equivalent to linear regression):

$$
y = \beta_0+ x * \beta_x + \epsilon
$$

$\beta_0$ = *intercept*: the overall offset of the line when $x=0$ (this cannot always be interpreted)\
$\beta_x$ = *slope*: how much do we expect $y$ to change with each change in $x$?\
$y$ = *DV*\
$x$ = *IV* or *predictor\
*$\epsilon$ = error term\* or *residuals*: whatever variance is left once the model is fit (Think of the model as the blue line and the residuals are the vertical deviations of the data points from the line)

(If we refer to *predicted* $y$-values, after we have estimated the model, we can drop the error term: $\hat{y} = \hat{\beta_0} + x * \hat{\beta_x}$.)
:::

::: column
```{r}
#| echo: false

lmResult <- lm(grade~studyTime,data=df)
p2 <- p+geom_abline(slope=lmResult$coefficients[2],
                  intercept=lmResult$coefficients[1],
                  color='blue')
p3 <- p2 +
  geom_hline(yintercept=lmResult$coefficients[1],color='black',size=0.5,linetype='dotted') +
  annotate('segment',x=2,xend=3,color='red',linetype='dashed',
           y=predict(lmResult,newdata=data.frame(studyTime=2))[1],
           yend=predict(lmResult,newdata=data.frame(studyTime=2))[1]) +
   annotate('segment',x=3,xend=3,color='red',linetype='dashed',
           y=predict(lmResult,newdata=data.frame(studyTime=2))[1],
           yend=predict(lmResult,newdata=data.frame(studyTime=3))[1])
 
print(p3)

```
:::
:::

## The Relation Between Correlation and Regression

There is a close relation and we can convert $r$ to $\hat{\beta_x}$.

$\hat{r} = \frac{covariance_{xy}}{s_x * s_y}$

$\hat{\beta_x} = \frac{covariance_{xy}}{s_x*s_x}$

$covariance_{xy} = \hat{r} * s_x * s_y$

$\hat{\beta_x} = \frac{\hat{r} * s_x * s_y}{s_x * s_x} = r * \frac{s_y}{s_x}$

--\> Regression slope = correlation multiplied by ratio of SDs (if SDs are equal, $r$ = $\hat{\beta}$ )

::: notes
Estimation of GLM:

linear algebra (R will do that for us!) --\> Appendix book
:::

## Standard Errors for Regression Models

We usually want to make inferences about the regression parameter estimates. For this we need an estimate of their variability.

We first need an estimate of how much variability is *not* explained by the model: the **residual variance** (or **error variance**):

Compute *residuals*:

$$
residual = y - \hat{y} = y - (x*\hat{\beta_x} + \hat{\beta_0})
$$

Compute *Sum of Squared Errors* (remember from ANOVA?):

$$
SS_{error} = \sum_{i=1}^n{(y_i - \hat{y_i})^2} = \sum_{i=1}^n{residuals^2}
$$

Compute *Mean Squared Error*:

$$
MS_{error} = \frac{SS_{error}}{df} = \frac{\sum_{i=1}^n{(y_i - \hat{y_i})^2} }{N - p}
$$

where the $df$ are the number of observations $N$ - the number of estimated parameter $p$ (in this case 2: $\hat{\beta_0}$ and $\hat{\beta_x}$).

Finally, we can calculate the *standard error* for the *full* model:

$$
SE_{model} = \sqrt{MS_{error}}
$$

We can also calculate the SE for specific regression parameter estimates by rescaling the $SE_{model}$:

$$
SE_{\hat{\beta_x}} = \frac{SE_{model}}{\sqrt{\sum{(x_i - \bar{x})^2}}}
$$

::: notes
rescaling SE: by square root of the SS of the X variable
:::

## Statistical Tests for Regression Parameters

With the parameter estimates and their standard errors, we can compute $t$-statistics, which represent the likelihood of the observed estimate vs. the expected value under $H_0$ (usually 0, no effect).

$$
\begin{array}{c}
t_{N - p} = \frac{\hat{\beta} - \beta_{expected}}{SE_{\hat{\beta}}}\\
t_{N - p} = \frac{\hat{\beta} - 0}{SE_{\hat{\beta}}}\\
t_{N - p} = \frac{\hat{\beta} }{SE_{\hat{\beta}}}
\end{array}
$$

## GLM results

Usually, we would just let R do the calculations:

```{r}
#| echo: true
summary(lmResult)
```

The intercept is significantly different from zero (which is usually not very relevant: with 0 study time, you don't get 0%) and the effect of `studyTime` is not (or only "marginally") significant. So for every hour that we study more, the effect on the grade is *descriptively* rather small (\~4%) but possibly not present at all (because it is not *statistically* significant).

::: notes
$t$ ratio of $\beta$ to its $SE$!

intercept: expected grade without studying at all
:::

## Quantifying Goodness of Fit of the Model

Often, it is useful to check how good the (total) model we estimated fits the data.

. . .

We can do that easily by asking *how much of the variability in the data is accounted for by the model?*

. . .

If we only have one IV ($x$), then we can simply square the correlation coefficient:

$$
R^2 = r^2
$$

In study time example, $R^2$ = 0.63² = 0.4 --\> we accounted for 40% of the overall variance in grades!

. . .

More generally, we can calculate $R^2$ with the Sum of Squared Variances:

$$
R^2 = \frac{SS_{model}}{SS_{total}} = 1-\frac{SS_{error}}{SS_{total}}
$$

. . .

With a sample of sufficient size, it is possible to get highly significant values that still explain very little of the total variance (i.e., little *practical* significance despite *statistical* significance)

::: notes
$R^2$ is the name of the Goodness of Fit stat!

A small R² tells us that even though a model might be significant, it may only explain a small amount of information in the DV
:::

## Fitting More Complex Models

Often we want to know the effects of *multiple variables* (IVs) on some outcome.

Example:\
Some students have taken a very similar class before, so there might not only be the effect of `studyTime` on `grades`, but also of having taken a `priorClass`.

. . .

::: columns
::: column
We can built a model that takes both into account by simply adding the "weight" and the IV (`priorClass`) to the model:

$\hat{y} = \hat{\beta_1}*studyTime + \hat{\beta_2}*priorClass + \hat{\beta_0}$

::: incremental
-   To model `priorClass`, i.e. whether each individual has taken a previous class or not, we use **dummy coding** (0=no, 1=yes).
-   This means, for those who have *not* taken a class, the whole part of the equation ($\hat{\beta_2} * priorClass$) will be zero - we will add it for the others.
-   $\hat{\beta_2}$ is thus the difference in means between the two groups!
-   $\hat{\beta_1}$ is the regression slope of `studyTime` across data points/regardless of whether someone has taken a class before.
:::
:::

::: column
If we plot the data, we can see that both IVs seem to have an effect on grades:

```{r, fig.width=5, fig.height=3}
df$priorClass <- as.factor(df$priorClass)
lmResultTwoVars <- lm(grade ~ studyTime + priorClass, data = df)
# summary(lmResultTwoVars)

p <- ggplot(df,aes(studyTime,grade,shape=priorClass)) +
  geom_point(size=3) + xlim(0,5) + ylim(70,100)
p <- p+
  geom_abline(slope=lmResultTwoVars$coefficients[2],
              intercept=lmResultTwoVars$coefficients[1],lineype='dotted')
# p <- p+
#   annotate('segment',x=2,xend=3,
#            y=lmResultTwoVars$coefficients[1]+
#              2*lmResultTwoVars$coefficients[2],
#            yend=lmResultTwoVars$coefficients[1]+
#              2*lmResultTwoVars$coefficients[2],
#            color='blue') +
#   annotate('segment',x=3,xend=3,
#            y=lmResultTwoVars$coefficients[1]+
#              2*lmResultTwoVars$coefficients[2],
#            yend=lmResultTwoVars$coefficients[1]+
#              3*lmResultTwoVars$coefficients[2],
#            color='blue')
p <- p+
  geom_abline(slope=lmResultTwoVars$coefficients[2],
              intercept=lmResultTwoVars$coefficients[1]+
                lmResultTwoVars$coefficients[3],
              linetype='dashed') 
p <- p+
  annotate('segment',x=2,xend=2,
           y=lmResultTwoVars$coefficients[1]+
             2*lmResultTwoVars$coefficients[2],
           yend=lmResultTwoVars$coefficients[1]+
             lmResultTwoVars$coefficients[3] +
             2*lmResultTwoVars$coefficients[2],
           linetype='dotted',size=1) +
  scale_color_discrete(
    limits = c(0, 1),
    labels = c("No", "Yes")
  ) +
  labs(
    color = "Previous course"
  )
print(p)
```
:::
:::

::: notes
How can we tell from the plot that both IVs might have an effect?
:::

## Interactions Between Variables

We previously assumed that the effect of `studyTime` on `grade` was the same for both groups - but sometimes we expect that this regression slope differs per group!

. . .

E.g., due to prior knowledge from another class, it may be *easier* to profit from studying the new materials (i.e., steeper slope). Or it may be *harder* due to diminishing marginal returns (i.e., flatter slope).

. . .

\
This is what we call an **interaction**: The effect of one variable depends on the value of another variable.

Thus, `priorClass` can have a **main effect** on `grade` (i.e., independent of `studyTime`): "I already know more, regardless wether I study".\
But it can also **interact** with `studyTime`: "Due to my prior knowledge, I can study more efficiently."

## Interaction Example

Example: What is the effect of caffeine on public speaking?

\

::: columns
::: {.column width="60%"}
```{r}
set.seed(1234567)
df <- 
  data.frame(
    group=c(rep(-1,10),
            rep(1,10)
          )
  ) %>%
  mutate(caffeine=runif(n())*100) %>%
  mutate(speaking=0.5*caffeine*-group + group*20 + rnorm(20)*10) %>%
  mutate(anxiety=ifelse(group==1,'anxious','notAnxious'))


```

```{r}
#| echo: true
# perform linear regression with caffeine as independent variable
lmResultCaffeine <- lm(speaking ~ caffeine, data = df)
summary(lmResultCaffeine)
```
:::

::: {.column width="40%"}
```{r, fig.width=4, fig.height=3}
p1 <- ggplot(df,aes(caffeine,speaking)) +
  stat_smooth(method="lm") + geom_point()
p1
```
:::
:::

. . .

There doesn't seem to be a "direct" (bivariate) effect:

## Interaction Example: Two main effects

What if we have the hypothesis that `anxiety` also affects public speaking?

\

::: columns
::: {.column width="60%"}
```{r}
# compute linear regression adding anxiety to model
lmResultCafAnx <- lm(speaking ~ caffeine + anxiety, data = df)
summary(lmResultCafAnx)
```
:::

::: {.column width="40%"}
```{r, fig.width=4, fig.height=3}
#TODO add individual regression lines per group but with identical slope?
p2 <- ggplot(df,aes(caffeine,speaking,color=anxiety,group=NA)) +
  geom_hline(data=df %>% summarise(speaking.m=mean(speaking), .by=anxiety),
             mapping=aes(yintercept=speaking.m, color=anxiety),
             linetype="dashed", size=1.2) +
  stat_smooth(method="lm") + geom_point() + 
  theme(legend.position = "top")
p2
```
:::
:::

. . .

The new model is still not significant. This is due to the fact that we only look at **additive effects** (main effects). But neither caffeine nor anxiety alone/independently predict public speaking performance.

. . .

From the plot, however, it looks like the effect of caffeine is indeed different for the two anxiety groups: Increasing for non-anxious people and decreasing for anxious ones.

In other words: We need to have a model that allows to fit different regression *slopes* to both groups.

::: notes
explain additive effects: look at average caffeine effect, then add mean for anxiety groups (but: both not significant here!)

problem of independent main effects: also if you bring together two findings from two different papers =\> no information about interaction of predictors
:::

## Interaction Example: Full model

To allow for different slopes for each group (i.e. for the effect of caffeine to vary between the anxiety groups), we have to model the *interaction* as well.

::: columns
::: {.column width="60%"}
```{r}
#| echo: false
# compute linear regression including caffeine X anxiety interaction
lmResultInteraction <- lm(
  speaking ~ caffeine + anxiety + caffeine:anxiety,
  # speaking ~ caffeine * anxiety,  # same!
  data = df
)
summary(lmResultInteraction)

```
:::

::: {.column width="40%"}
```{r, fig.width=4, fig.height=3}
df_anx <- 
  df %>%
  subset(anxiety=='anxious') %>%
  mutate(y=lmResultInteraction$fitted.values[df$anxiety=='anxious'])

df_notanx <- 
  df %>%
  subset(anxiety=='notAnxious')%>%
  mutate(y=lmResultInteraction$fitted.values[df$anxiety=='notAnxious'])

p3 <- ggplot(df,aes(caffeine,speaking,color=anxiety,group=anxiety)) +
  stat_smooth(method="lm") +
  geom_point() + 
  theme(legend.position = "top")
  # geom_line(data=df_anx,
  #            aes(caffeine,y),linetype='dashed') +
  # geom_line(data=df_notanx,
  #            aes(caffeine,y),linetype='dotted')

p3
```
:::
:::

. . .

The model is now significant! There is an *interaction* between `caffeine` and `anxiety`. Interestingly, the *main effects* are also significant now even though they were not in the previous models (because the residual variance has been reduced drastically from 18 to 8 by the highly significant interaction).

Note: `speaking ~ caffeine * anxiety` is shorthand for `speaking ~ caffeine + anxiety + caffeine:anxiety`

::: notes
interpretation coefficients:

intercept: intercept of anxious group!

intercept not anxious: difference intercept anxious vs. notanxious

slope anxious: only for the anxious group!

slope not anxious: diff in slopes

no main effects!!!
:::

## Convert GLM to ANOVA

The interpretation of the coefficients of a GLM when interactions are included is not as straight forward compared to an ANOVA!

. . .

If you want to report the "typical" ANOVA table with main effects and the general interaction:

```{r}
#| eval: true
#| echo: true

anova(lmResultInteraction)
```

. . .

Note: The *p*-values are different between the GLM and the ANOVA outputs because effects including factors are coded slightly differently (dummy vs. effect coding):

Dummy coding uses the first level as a default and compares the remaining levels to it ($0$ vs. $1$).\
Effect coding takes the overall mean as an abstract comparison ($-.5$ vs. $+.5$).\
This difference is resembled in the output tables as `anxietynotAnxious` (GLM) vs. `anxiety` (ANOVA).

::: notes
interpretation coefficients:

intercept: intercept of anxious group!

intercept not anxious: difference intercept anxious vs. notanxious

slope anxious: only for the anxious group!

slope not anxious: diff in slopes

no main effects!!!
:::

## Model Comparison

Sometimes, we want to compare two (*nested*!) models to see which one fits the data better.

We can do so by using the `anova()`\* function in R:

```{r}
#| echo: true
anova(lmResultCafAnx, lmResultInteraction)

```

This shows that Model 2, incl. the interaction, is to be preferred.

. . .

*Note*: We can only meaningfully use this method with so-called nested models, which means that the simpler (*reduced*) model only contains variables also included in the more complex (*full*) model.

::: aside
\*Yes, it is *kind of* an ANOVA as well, in that (a ratio of) squared errors is compared to an $F$-distribution...
:::

::: notes
Wald compares the ratio of squared errors to an F-distribution (sound familiar from ANOVA?), while likelihood ratio compares the ratio of likelihoods to a χ2 distribution
:::

## Criticizing Our Model and Checking Assumptions

"Garbage in, garbage out" - we have to make sure our model is properly specified!

. . .

*Properly specified* = having included the appropriate IVs.

. . .

The model also needs to satisfy the **assumptions** of the statistical method (= GLM).

One important assumption of the GLM is that *the residuals are normally distributed*.

This assumption can be violated by a not properly specified model or because the data are inappropriate for the statistical model.

. . .

We can use a **Q-Q plot**, which represents the quantiles of two distributions/variables (e.g., the data and a normal distribution of the same data) against each other.

If the data points diverge substantially from the line (especially in the extremes), we can conclude that the residuals are not normally distributed.

## Model Diagnostics

To check the assumptions, we can easily run a function for model diagnostics (incl. Q-Q plots) in R. The function, `check_model()`, is included in the `performance` package by the [*easystats*](https://easystats.github.io/easystats/index.html) team (who make great packages for everything related to statistical modeling!)

```{r}
#| echo: true

# install.packages("easystats")
library(performance)

check_model(lmResultInteraction)
```

::: notes
We're not going into detail about all these diagnostics (and hard to see!), but it is always a good idea to run diagnostics/check assumptions for your models!
:::

## Remark: "Predicting" in a Statistical Context

We neither mean "predicting before seeing the data/in the future" nor mean to imply *causality*!

. . .

It simply refers to fitting a model to the data: We estimate (or predict) values for the DV ($\hat{y}$) and the IVs are often referred to as *predictors*.

::: notes
Related to: predicting future values
:::

# Multivariate Statistics

*Multivariate* = involving more than one (dependent) variable.

Usually, the distinction between dependent and independent variables is not very strict (rather depends on the model than on the data itself). In the following section, we are interested in how a number of variables relate to one another (no clear predictor vs. criterion).

::: incremental
1.  **Clustering**: Understand the structure that exists in the data, find clusters of observations or variables that are similar.\
    =\> From dimensions to categories

2.  **Dimensionality reduction**: Reduce a large set of variables to fewer, retaining as much information as possible.\
    =\> From many dimensions to fewer dimensions
:::

## Multivariate Statistics 2

These two methods belong to the class of *unsupervised learning*. Before, we have only dealt with *supervised learning* (e.g., regression, ANOVA). There are also methods for multivariate, supervised learning (e.g., MANOVA).

Note: Unsupervised learning techniques are very *explorative* in nature. It is a different approach to learning from data compared to hypothesis testing. Explorative methods are usually used in science to create a first insight into new phenomena that can develop into hypotheses further down the line.

::: notes
Supervised: we know the value of the DV that we're trying to predict (try to find best model predictions).

Unsupervised: we don't have specific value to predict, we try to discover (patterns that help understand). Requires some assumptions about pattern.

--\> does not necessarily have a "right" answer!
:::

## Multivariate Data: An Example

```{r}
# import MASS first because it otherwise will mask dplyr::select
library(MASS)
library(tidyverse)
library(ggdendro)
library(psych)
library(gplots)
library(pdist)
library(factoextra)
library(viridis)
library(mclust)
theme_set(theme_minimal())

behavdata <- read_csv('Data/meaningful_variables.csv',
                      show_col_types = FALSE)  

demoghealthdata <- read_csv('Data/demographic_health.csv',
                            show_col_types = FALSE) 
# recode Sex variable from 0/1 to Male/Female
demoghealthdata <- demoghealthdata %>%
  mutate(Sex = recode_factor(Sex, `0`="Male", `1`="Female"))
# combine the data into a single data frame by subcode
alldata <- merge(behavdata, demoghealthdata, by='subcode')
rename_list = list('upps_impulsivity_survey' = 'UPPS', 'sensation_seeking_survey' = 'SSS',
                   'dickman_survey' = 'Dickman',  'bis11_survey' = 'BIS11', 
                   'spatial_span' = 'spatial', 'digit_span' = 'digit',
                   'adaptive_n_back' = 'nback', 'dospert_rt_survey' = 'dospert',
                   'motor_selective_stop_signal.SSRT' = 'SSRT_motorsel',
                   'stim_selective_stop_signal.SSRT' = 'SSRT_stimsel',
                   'stop_signal.SSRT_low' = 'SSRT_low',
                   'stop_signal.SSRT_high' = 'SSRT_high')
                   
impulsivity_variables = c('Sex')
keep_variables <- c("spatial.forward_span", "spatial.reverse_span", "digit.forward_span","digit.reverse_span", "nback.mean_load")
for (potential_match in names(alldata)){
  for (n in names(rename_list)){
    if (str_detect(potential_match, n)){
      # print(sprintf('found match: %s %s', n, potential_match))
      replacement_name <- str_replace(potential_match, n, toString(rename_list[n]))
      names(alldata)[names(alldata) == potential_match] <- replacement_name
      impulsivity_variables <- c(impulsivity_variables, replacement_name)
    }
  }
}
impulsivity_data <- alldata[,impulsivity_variables] %>%
  drop_na()
ssrtdata = alldata[,c('subcode', names(alldata)[grep('SSRT_', names(alldata))])] %>% 
  drop_na() %>% 
  dplyr::select(-stop_signal.proactive_SSRT_speeding)
upps_data <- alldata %>%
  dplyr::select(starts_with('UPPS'), 'subcode') %>%
  setNames(gsub("UPPS.", "", names(.)))
impdata <- inner_join(ssrtdata, upps_data) %>% 
  drop_na() %>% 
  dplyr::select(-subcode) %>% 
  scale() %>%
  as.data.frame() %>%
  dplyr::rename(SSRT_motor = SSRT_motorsel,
                SSRT_stim = SSRT_stimsel,
                UPPS_pers = lack_of_perseverance,
                UPPS_premed = lack_of_premeditation,
                UPPS_negurg = negative_urgency,
                UPPS_posurg = positive_urgency,
                UPPS_senseek = sensation_seeking
                )
```

How do different aspects of psychological function (self-control etc.) relate to one another?

10h battery of cognitive tests and surveys, N = 522, 9 measures of interest!

Measures:

-   *Response inhibition*: ability to quickly stop an action (measured with the stop-signal task, measure is called *stop-signal reaction time (SSRT)* - we have 4 different versions of this measure).

-   *Impulsivity*: tendency to make decisions on impulse, without regard of potential consequences (*UPPS-P* survey, assesses 5 facets of impulsivity).

## Visualizing Multivariate Data

Hard (impossible?) for us to visualize more than three dimensions/variables.

```{r echo=FALSE, fig.width=8, fig.height=8, fig.cap='Scatterplot of matrices for the nine variables in the self-control dataset.  The diagonal elements in the matrix show the histogram for each of the individual variables.  The lower left panels show scatterplots of the relationship between each pair of variables, and the upper right panel shows the correlation coefficient for each pair of variables.'}
pairs.panels(impdata, lm=TRUE)
```

::: notes
What do you see?

each row/col --\> single variable\
diagonal: dist each var\
lower triangle: scatterplot each pair, regression line --\> relationship\
upper: correlation coefficient
:::

## Heatmap

Visualize correlations:

```{r hmap, echo=FALSE, fig.width=8, fig.height=8, fig.cap='Heatmap of the correlation matrix for the nine self-control variables.  The brighter yellow areas in the top left and bottom right highlight the higher correlations within the two subsets of variables.'}
cc = cor(impdata)
par(mai=c(2, 1, 1, 1)+0.1) 
heatmap.2(cc, trace='none', dendrogram='none', 
          cellnote=round(cc, 2), notecol='black', key=FALSE,
          margins=c(12,8), srtCol=45, symm=TRUE, revC=TRUE, #notecex=4, 
          cexRow=1, cexCol=1, offsetRow=-150, col=viridis(50))

```

We can see clear clusters: SSRT and UPPS have greater intercorrelations than correlations with the other measure.

## Heatmap 2

Heatmaps are especially helpful if we have a large number of variables, such as in neuroimaging! Below you can see the *functional connectivity* of \>300 brain regions:

![Heatmap of correlation coefficients of brain activity between 316 regions of the left hemisphere of a single individual. Yellow: strong positive correlations, blue: strong negative correlations](images/Heatmap2.png){width="445"}

::: notes
large blocks of pos corr: major connected networks in the brain
:::

## Clustering

**Clustering**: Identifying groups of related observations or variables within a dataset, based on the similarity of the values of the observations.

. . .

**Similarity**: Distance between values.

**Euclidean Distance**: Length of the line that connects two data points:

```{r, fig.width=3, fig.height=3}
euc_df <- data.frame(x=c(1, 4), y=c(2, 3))
ggplot(euc_df, aes(x,y)) + geom_point() + 
  xlim(0, 5) + ylim(0, 4) + 
  annotate('segment', x=1, y=2, xend=4, yend=3, linetype='dotted')

```

$$
d(x,y) = \sqrt{(x_1 - x_2)^2 + (y_1 - y_2)^2}
$$

. . .

Euclidean Distance is sensitive to variability in both variables --\> *scale* data before! (e.g., z-transformation using R's `scale` function)

::: notes
Clustering: finding set of groups that have the lowest distance between their members.

Euclidean Dist: Pythagorean theorem\
--\> can be extended to more than two dimensions!

Scale: calculate z-score, standardizing

Example: Relation between height and weight =\> it shouldn't make a difference if we rescale the data from m to cm or from kg to pounds.
:::

## K-Means Clustering

**K-means clustering**: Identifies a set of cluster centers & then assigns each data point to the cluster whose center is the closest (Euclidean Distance!).

::: incremental
1.  Decide on value for *K*, the number of clusters to be found (e.g. based on previous knowledge/expectations).
2.  Come up with *k* locations for the centers - or *centroids* - of the clusters (e.g. choose data points at random to start with).
3.  Compute Euclidean distance of each data point to each centroid.
4.  Assign each point to a cluster, based on closest distance to centroid.
5.  Recompute centroid by averaging the location of all points assigned to that cluster.
6.  Repeat steps 1-5 until a stable solution is found (*iterative* process).
:::

## Example: Latitude/Longitude Data

::: columns
::: column
Here we can see the starting points (black squares) and end points (big colored dots) for each cluster, as well as the final cluster assignment of each data point (color):

```{r}
countries <- read_delim('Data/country_data.csv', na=c('')) %>%
  # filter out countries with less than 1M population
  filter(Population2020 > 500000)
latlong <- countries %>% 
  dplyr::select(latitude, longitude) 

k = 6
set.seed(123456)
# select random starting points as the means - i.e. Forgy method
centroids = latlong[sample.int(nrow(latlong), k),]
iterations = 0
oldCentroids = data.frame()
MAX_ITERATIONS <- 100
shouldStop <- function(oldCentroids, centroids, iterations){
    if (iterations > MAX_ITERATIONS){
      return(TRUE)
    } 
    if (dim(oldCentroids)[1] == 0){
      return(FALSE)
    }
    return(all.equal(as.matrix(centroids), as.matrix(oldCentroids)) == TRUE)
}
getLabels <- function(dataSet, centroids){
    d <- as.matrix(pdist::pdist(dataSet, centroids))
    
    # For each element in the dataset, chose the closest centroid. 
    # Make that centroid the element's label.
    return(apply(d, 1, which.min))
}
getCentroids <- function(dataSet, labels, k){
    # Each centroid is the geometric mean of the points that
    # have that centroid's label. Important: If a centroid is empty (no points have
    # that centroid's label) you should randomly re-initialize it.
    newCentroids <- NULL
    for (i in 1:k){
      labeldata <- dataSet[labels==i,]
      newCentroids <- rbind(newCentroids, apply(labeldata, 2, mean))
    }
    return(newCentroids)
}
all_centroids_df = data.frame(centroids) %>%
  mutate(label_kmeans=as.factor(seq(1,nrow(.))),
         iter=0)
while (!shouldStop(oldCentroids, centroids, iterations)) {
          # Save old centroids for convergence test. Book keeping.
        oldCentroids = centroids
        iterations = iterations + 1
        # Assign labels to each datapoint based on centroids
        labels = getLabels(latlong, centroids)
        
        # Assign centroids based on datapoint labels
        centroids = getCentroids(latlong, labels, k)
        centroids_df = data.frame(centroids) %>%
          mutate(label_kmeans=as.factor(seq(1,nrow(.))),
                 iter=iterations)
        all_centroids_df = rbind(all_centroids_df, centroids_df)
}
#sprintf('Completed after %d iterations', iterations)
countries <- countries %>%
  mutate(label_kmeans = as.factor(labels))
centroid_df = all_centroids_df %>%
  filter(iter==iterations)
p = ggplot(countries, aes(longitude, latitude, color=label_kmeans)) + 
  geom_point() + 
  geom_point(data=centroid_df,alpha=0.5, size=4) 
for (i in 1:iterations){
  for (j in 1:k){
    iter_df = all_centroids_df %>% filter(iter==i, label_kmeans==j)
    prev_df = all_centroids_df %>% filter(iter==i-1, label_kmeans==j)
    p = p +  annotate('segment', x = iter_df$longitude, 
                              y = iter_df$latitude,
                              xend = prev_df$longitude,
                              yend = prev_df$latitude, alpha=0.7)
  }
}
p + geom_point(data=all_centroids_df %>% filter(iter==0), 
               size=2, shape=15, color='black')
```
:::

::: column
We can see that there is a reasonable overlap between clusters and continents.

We can further investigate this overlap with the **confusion matrix**, which compares membership of each cluster with the actual continents for each country:

```{r}
table(labels, countries$Continent)
```

Note: usually we don't know the ground truth (i.e. which continent) in unsupervised learning!\
=\> confusion matrix cannot be estimated

Note 2: Every time we run the iterative process, we will get a different result if we use random starting points. Make sure the result is robust, e.g. by running the Clustering algorithm several times.)
:::
:::

::: notes
-   Cluster 1 contains all European countries, as well as countries from northern Africa and Asia.

<!-- -->

-   Cluster 2 contains contains Asian countries as well as several African countries.

-   Cluster 3 contains countries from the southern part of South America.

-   Cluster 4 contains all of the North American countries as well as northern South American countries.

-   Cluster 5 contains Oceania as well as several Asian countries

-   Cluster 6 contains all of the remaining African countries.
:::

## Hierarchical Clustering

**Hierarchical Clustering:** Also uses distances to determine clusters but also visualizes relationships in *dendrograms*.

::: columns
::: column
The most common procedure is **agglomerative clustering**:

::: incremental
1.  Every data point is treated as its own cluster.
2.  Two clusters with the least distance (e.g. *average linkage*) between them are combined.
3.  Repeat 1 & 2 until only one cluster is left.
4.  Visualize, decide on a cutoff for the amount of reasonable clusters.
:::
:::

::: column
```{r}
d <- dist(t(impdata))
hc <- hclust(d, method='average')
#convert cluster object to use with ggplot
dendr <- dendro_data(hc, type="rectangle") 
# TODO: https://stackoverflow.com/questions/21474388/colorize-clusters-in-dendogram-with-ggplot2
cutoffs = c(25, 20, 19)
#your own labels (now rownames) are supplied in geom_text() and label=label
ggplot() + 
  geom_segment(data=segment(dendr), aes(x=x, y=y, xend=xend, yend=yend)) + 
  geom_text(data=label(dendr), aes(x=x, y=y,label=dendr$labels$label, hjust=0), size=3) +
  coord_flip() + scale_y_reverse(expand=c(0.2, 0)) + 
  theme(axis.line.y=element_blank(),
        axis.ticks.y=element_blank(),
        axis.text.y=element_blank(),
        axis.title.y=element_blank(),
        panel.background=element_rect(fill="white"),
        panel.grid=element_blank()) + 
   geom_hline(yintercept=cutoffs[1], color='blue') + 
   geom_hline(yintercept=cutoffs[2], color='green') + 
   geom_hline(yintercept=cutoffs[3], color='red') + 
   ylim(30, -10)

```

Colored lines: different cutoffs.
:::
:::

. . .

There seems to be a high degree of similarity within each variable set (SSRT and UPPS) compared to between sets.\
=\> The first split separates UPPS vs. SSRT perfectly.

Within UPPS, sensation seeking stands out as the most different to the rest.

::: notes
average linkage: average of all distances between each data point in each of two clusters.

Can read diagram from left to right: One supercluster that is split up more and more. Or right to left: Individual points that get joint together into bigger and bigger clusters.

colored lines: different cutoff values
:::

## Dimensionality Reduction

We often measure different variables that are highly similar to each other, e.g. because they are supposed to measure the same *construct*.

Although we might measure a particular number of variables (= *dimensionality* of data set), there may be fewer independent sources of underlying information!

**Dimensionality reduction**: Reduce the number of variables by creating composite variables that reflect the underlying information.

## Principal Component Analysis (PCA)

Aim: Find a lower-dimensional (linear) description of a set of variables (that still accounts for the maximum possible information/variance in the dataset).

Variance: Combination of signal + noise --\> find strongest common signal between variables!

. . .

**1st Component**: explains most variance between variables, 2nd component: maximum of remaining variance - but uncorrelated with 1st...

. . .

::: columns
::: column
```{r fig.width=4, fig.height=3}
#| echo: false
N <-30                           #setting my sample size             
mu <- c(0, 0)                      #setting the means
c1 <- .7
sigma <- matrix(c(1, c1, c1, 1),2, 2)  #setting the covariance matrix values. The "2,2" part at the tail end defines the number of rows and columns in the matrix
set.seed(04182019)  #setting the seed value so I can reproduce this exact sim later if need be
simdata <- mvrnorm(n=N,mu=mu,Sigma=sigma, empirical=TRUE)  #simulate the data, as specified above
sim_df <- data.frame(simdata)
names(sim_df) <- c("Y", "X")


# scale variables
sim_df <- sim_df %>%
  mutate(X = scale(X), 
         Y = scale(Y))
# compute covariance matrix
sim_df_cov<- cov(sim_df)
# Compute eigenvalues/eigenvectors
cov_eig <- eigen(sim_df_cov)

g <- ggplot(sim_df, aes(X, Y)) + 
   geom_point(size=1.5) + 
  xlim(-3, 3) + 
  ylim(-3, 3) 
# based on https://stats.stackexchange.com/questions/153564/visualizing-pca-in-r-data-points-eigenvectors-projections-confidence-ellipse
# calculate slopes as ratios
cov_eig$slopes[1] <- cov_eig$vectors[1,1]/cov_eig$vectors[2,1]  
cov_eig$slopes[2] <- cov_eig$vectors[1, 2]/cov_eig$vectors[2,2] 
g <- g + geom_segment(x = 0, y = 0, 
                      xend = cov_eig$values[1], 
                      yend = cov_eig$slopes[1] * cov_eig$values[1], 
                      colour = "green", size=1.5,
                      arrow = arrow(length = unit(0.2, "cm")))  # add arrow for pc1
g <- g + geom_segment(x = 0, y = 0, 
                      xend = cov_eig$values[2], 
                      yend = cov_eig$slopes[2] * cov_eig$values[2], 
                      colour = "red", size=1.5,
                      arrow = arrow(length = unit(0.2, "cm")))  # add arrow for pc2
g 


```
:::

::: column
**Green arrow:** 1st component, follows direction of max. variance\
**Red arrow**: 2nd component, perpendicular to 1st =\> uncorrelated!

We can run a PCA on more than two variables!
:::
:::

::: notes
obtain as many components as there are variables (assuming that there are more observations than there are variables)

in practice: find a small number of components that can explain a large portion of the variance.

1st component: similar to regression line, but minimizes perpendicular distance points to line (not vertical!)
:::

## PCA 2

If we calculate a PCA on the impulsivity data, we see that there are two components (in the *scree plot*) that account for quite some variance.

We can also look at the variable *loadings*, which show which variable "goes into" which component to better specify what that component represents. Here we can see that one components represents (=captures variance related to) the SSRT variables, the other the UPPS.

::: columns
::: column
```{r}
imp_pc = prcomp(impdata,  scale. = T)
fviz_screeplot(imp_pc, addlabels = TRUE, ylim = c(0, 50))
```
:::

::: column
```{r}
loading_df = as.data.frame(imp_pc$rotation) 
loading_df['Variable'] = rownames(loading_df)
loading_df = loading_df %>% 
  pivot_longer(!Variable, names_to='PC', values_to='Loading') %>%
  filter(PC %in% c('PC1', 'PC2'))
ggplot(loading_df ,
       aes(Variable, Loading)) + geom_bar(stat='identity')  + 
      facet_grid(PC ~ .)
```
:::
:::

::: notes
Scree plot: Sometimes used to make decisions on number of components (eigenvalues plotted)

loadings: sign is arbitrary
:::

## Factor Analysis (FA)

PCA is useful for reducing the dimensionality of a dataset.\
The components are per definition uncorrelated --\> sometimes this is a limitation.\
PCA also doesn't account for measurement error --\> possibly difficult to interpret loadings.

. . .

**Exploratory Factor Analysis**: can also be used for dimensionality reduction.

Idea: Each observed variable is created through a combination of contributions from a *latent* variable + measurement error. (*latent*: can't be directly observed!)

. . .

*How do different measures relate to underlying factor (that gives rise to these measures)?*

## FA 2

::: columns
::: column
```{r, fig.width=4, fig.height=4}
N <- 200                           #setting my sample size             
mu <- rep(0, 3)                     #setting the means
c1 <- .5 # correlation b/w WM and FR
sigma <- matrix(c(1, c1, 0, c1, 1, 0, 0, 0, 1), 3, 3)  #setting the covariance matrix values. The "2,2" part at the tail end defines the number of rows and columns in the matrix
set.seed(04182019)  #setting the seed value so I can reproduce this exact sim later if need be
simdata <- MASS::mvrnorm(n=N,mu=mu,Sigma=sigma, empirical=TRUE)  #simulate the data, as specified above
latent_df <- data.frame(simdata)
names(latent_df) = c('WM', 'FR', 'IMP')
# create observed variables by matrix-multiplying the latent variables
# by a weight matrix 
set.seed(123456)
tasknames = c('nback', 'dspan', 'sspan', 'ravens', 'crt', 'UPPS', 'BIS11', 'dickman')
ntasks = length(tasknames)
weights = matrix(data = 0, 3, ntasks)
weights[1, 1:3] = 1
weights[2, 4:5] = 1
weights[3, 6:8] = 1
noise_sd = .6
observed_vals = as.matrix(latent_df) %*% weights + 
  MASS::mvrnorm(n=N, mu=rep(0, ntasks), Sigma=diag(ntasks) * noise_sd)
observed_df <- data.frame(observed_vals)
names(observed_df) <- tasknames


cormtx = t(cor(observed_df))
gplots::heatmap.2(cormtx, trace='none', symm=TRUE, 
          revC=TRUE,col=viridis::viridis(50),
          cellnote=round(cormtx, 2), notecol='black', key=FALSE,)

```
:::

::: column
```{r}
fa_result <- fa(observed_df, nfactors = 3)
summary(fa_result)

fa.diagram(fa_result)
```
:::
:::

**RMSEA** (root mean square error of approximation): Measure of model fit, should be \< .08.

Use (lowest) **SABIC** (sample-size adjusted Bayesian information criterion) to compare models with different number of factors.

::: notes
We can "hand over" **3 factors**

**RMSEA** quantifies how far predicted covariances (between data) are from actual covariances
:::

# Thanks!

Learning objectives:

-   Describe the concept of the *general linear model/linear regression* and apply it to a dataset
-   Understand *clustering* and *dimensionality reduction,* incl. the difference between k-means and hierarchical clustering as well as PCA and FA

Next:

-   R Session on the General Linear Model
-   Linear Mixed Models
