---
title: "09 Statistical Analyses in R"
subtitle: |
   | Julius-Maximilians-University Würzburg 
   | Course: "Biostatistics"    
   | Translational Neuroscience
author: "Dr. Mario Reutter (slides adapted from Dr. Lea Hildebrandt)"
format: 
  revealjs:
    smaller: true
    scrollable: true
    slide-number: true
    theme: serif
    chalkboard: true
    width: 1280
    height: 720
from: markdown+emoji
---

# Kinds of Statistical Analyses
Last session, we have heard about Chi² tests, *t*-tests, correlations, and ANOVA. Today we will apply them in R.

```{css}
code.sourceCode {
  font-size: 1.4em;
}

div.cell-output-stdout {
  font-size: 1.4em;
}
```

```{r}
library(car)
library(effectsize)
library(tidyverse)
```

::: notes
which analysis do you use when you have two categorical etc. variables?
:::

## Set Up

1.  Open RStudio and load your Biostats R project. Create a new script called `DataAnalysis1.R`.

2.  Install and load new packages you'll need: `car`, `effectsize`, and `apa`.

# Activity 1

Which test would you choose if you wanted to test whether a variable is differently distributed between groups?

Let's say we have tested 20 mice and want to know whether they pressed the blue, green, or red lever more often than by chance. This is how often the mice pressed each lever: Blue = 5, Green = 4, Red = 11

. . .

We would run a (one-sample) **Chi²-test**!

Remember that in a Chi²-test, we would compare observed to expected counts:

$$
\chi^2 = \sum\frac{(Observed - Expected)^2}{Expected}
$$

## One Sample Chi²-Test

```{r}
#| echo: true
library(tidyverse) 

# Input data into R
lever_presses <- tibble(levers=c(rep("blue", 5), rep("green", 4), rep("red", 11))) %>% #data in usual format
  count(levers) #convert to a frequency table

# run Chi² test
chi2_lever <- chisq.test(lever_presses %>% pivot_wider(names_from="levers", values_from="n")) #wide format needed
#chi2_lever <- chisq.test(tibble(blue=5, green=4, red=11)) #enter data directly as summary table

chi2_lever %>% apa::chisq_apa() #output results

# add expected count and residuals to data
lever_presses %>% mutate(expected = chi2_lever$expected, 
                         residuals = chi2_lever$residuals)
```

:::notes
Is sample size not important for significance here?
$$\chi^2 = \sum\frac{(Observed - Expected)^2}{Expected}$$
More observations => easier to see greater absolute difference between observed and expected values => gets squared and therefore "outcompetes" denominator (what about expected reductions in sampling error?)
:::

## Chi²-Test 2

We can also run a Chi²-test for two variables, such as if we want to know whether the lever presses differ per group.

Let's say we have a treatment and a control group and we record the different lever presses:

| Group      | Blue  | Green  |  Red   | **Totals** |
|------------|:-----:|:------:|:------:|:----------:|
| Treatment  |   3   |   12   |   5    |   **20**   |
| Control    |   5   |   4    |   11   |   **20**   |
| **Totals** | **8** | **16** | **16** |   **40**   |

Remember: We use the assumption of independence to calculate the expected values.

. . .

```{r}
#| echo: true

lever_presses2 = tibble(group="treatment", blue=3, green=12, red=5) %>% 
  bind_rows(tibble(group="control", blue=5, green=4, red=11)) #clearest way to manually input data
#for long data, you create the frequency table identically to previous slide but with: count(group, levers)

chisq.test(lever_presses2 %>% column_to_rownames("group")) %>% apa::chisq_apa()
```

## Assumptions of Chi²-Tests

::: incremental
-   **Random sample**

-   **Categorical variables**, i.e., **counts** within (combinations of) groups

-   **Expected cell count**: \>5 counts per cell

-   **Independence**: Each observation is independent of the others, e.g. there are no repeated measurements/paired data (within-subjects data are correlated!)
:::

. . .

In the case of the last analysis, we got a *warning* because the expected cell sizes were too small. In this case, it is better to use **Fisher's exact test**:

```{r}
#| echo: true

fisher.test(lever_presses2 %>% column_to_rownames("group"))
```

# Activity 2

Now imagine you want to compare *reaction times* of the lever presses of the two groups of mice. You expect that those in the treatment group respond faster than those in the control group. Which statistical test would you use?

. . .

*Hint*: We now have two *independent* groups and a *continuous* dependent variable (reaction times).

. . .

An (independent samples) *t*-test!

. . .

## *t*-test setup

To run a *t*-Test, let's first simulate data. What happens here?

```{r}
#| echo: true
set.seed(31415)
sim_data_t <- tibble(
  group = rep(c("treatment", "control"), each=20),
  reaction_times = c(rnorm(20, 400, 100), rnorm(20, 450, 100)))
```

. . .

```{r}
# visualize
ggplot(sim_data_t, aes(x=group, y=reaction_times)) +
   geom_boxplot()
```

## *t*-Test

We can use the *formula* notation to run the *t*-test. This notation usually looks like this: \
`dependent variable ~ independent variable.`

Replace the `NULL`s:

```{r}
#| echo: true
#| eval: false

t.test(NULL ~ NULL,
       data = NULL,
       alternative = NULL,
       paired = NULL) 

```

. . .

```{r}
#| echo: true
#| eval: true

t.test(reaction_times ~ group,
       data = sim_data_t,
       alternative = "greater", #depends on ALPHABETICAL order of groups in data!
       paired = FALSE) #in the future, we only specify this IF the data are paired
```

. . .

As you can see in the output, R automatically chose to run Welch's *t*-test, which does not assume **variance homogeneity** in contrast to Student's *t*-test. For the latter, you can add the function parameter `var.equal = TRUE`

:::notes
When changing the name "treatment" to "a", the results change:
```{r}
#| echo: true
t.test(reaction_times ~ group,
       data = sim_data_t %>% mutate(group = ifelse(group=="treatment", "a", group)),
       alternative = "greater", #depends on order of groups in data! 1st group > 2nd group
       paired = FALSE, var.equal=T) %>% apa::t_apa()
```
:::

## *t*-Test: alternatives

You can also supply two vectors of numerical values for the groups:

```{r}
#| eval: true
#| echo: true
t.test(x = sim_data_t %>% filter(group=="treatment") %>% pull(reaction_times),
       y = sim_data_t %>% filter(group=="control") %>% pull(reaction_times),
       alternative = "less") #more intuitive now because you can filter for treatment first
```

. . .

More useful output with `apa` package (only for Student's t-tests):
```{r}
#| eval: true
#| echo: true
t.test(x = sim_data_t %>% filter(group=="treatment") %>% pull(reaction_times),
       y = sim_data_t %>% filter(group=="control") %>% pull(reaction_times),
       alternative = "less", 
       var.equal = T) %>% 
  apa::t_apa(es_ci=T) #optional: confidence interval around effect size
```

. . .

For paired t-tests, you can also work with the wide format of data (advantage: missing values become explicit):
```{r}
#| eval: true
#| echo: true
sim_data_t_wide = sim_data_t %>% mutate(subject=rep.int(1:(n()/2), 2)) %>% 
  pivot_wider(names_from="group", values_from="reaction_times")
t.test(x = sim_data_t_wide %>% pull(treatment),
       y = sim_data_t_wide %>% pull(control),
       alternative = "less", 
       paired = T) %>% 
  apa::t_apa(es_ci=T) #optional: confidence interval around effect size
```

## Assumptions of *t*-Tests

Before we run a *t*-test, we also want to check the assumptions for violations:

::: incremental
-   The dependent variable is continuous (otherwise: chi²-test)

-   The data are independent (otherwise: *paired* sample *t*-test)

-   The variance between the groups is homogeneous (only for Student's *t*-test, R uses Welch's test by default)

-   The *residuals*\ are normally distributed for each group (otherwise: `wilcox.test`)
:::

. . .

Oftentimes, the last assumption is misquoted as: "The *dependent variable* needs to be normally distributed". \
In fact, only its *sampling distribution* needs to be (blue vs. grey distribution in the lecture on sampling [[direct link](https://spressi.github.io/biostats/W5_Sampling.html#/example-sampling-distribution)]), which will always be the case for big enough samples due to the central limit theorem.

## Assumptions of *t*-Tests 2

**Test for normality of *residuals*** (for both groups): If points fall along the line nicely, assumption is met.

```{r}
#| echo: true
#| output-location: column

treatment <- sim_data_t %>%
  filter(group == "treatment") %>%
  mutate(group_resid = 
           reaction_times - mean(reaction_times)) %>%
  pull(group_resid)

#base R QQ plot
qqnorm(treatment)
qqline(treatment)
```

```{r}
#| echo: true
#| output-location: column

control <- sim_data_t %>%
  filter(group == "control") %>%
  mutate(group_resid = reaction_times - mean(reaction_times)) %>%
  pull(group_resid)

#car's version highlighting (and returning) problematic values
car::qqPlot(control)
```

## Assumptions of *t*-Tests 3

Alternatively, we can also run a **Shapiro-Wilk** test to test for **deviations** from normality:

```{r}
#| echo: true
shapiro.test(x = treatment)
shapiro.test(x = control)
```

If the test is non-significant, then we can conclude that normality of residuals is **not** violated.

::: notes
1.  Transform your data to try and normalise the distribution. Not usually recommended these days but some still use it.
2.  Use a non-parametric test. The non-parametric equivalent of the independent t-test is the Mann-Whitney and the equivalent of the paired-samples t-test is the Wilcoxon signed-ranks test. Though more modern permutation tests are better.
3.  Do nothing. [Delacre, Lakens & Leys, 2017](https://www.rips-irsp.com/articles/10.5334/irsp.82/) argue that with a large enough sample (\>30), the Welch test is robust to deviations from assumptions. With very large samples normality is even less of an issue, so design studies with large samples.
:::

# Optional Activity: Paired Samples *t*-Test

Sometimes you have dependent data: These data are somehow correlated, e.g. they belong to the same subject (that you measured repeatedly across a *within-subject manipulation*).

. . .

In this case, you would use a **paired-samples *t*-test**.

. . .

Let's run such a *t*-test with real data. We'll use the [`Mehr Song and Spelke 2016 Experiment 1.csv`](https://github.com/spressi/biostats/blob/main/Data/Mehr%20Song%20and%20Spelke%202016%20Experiment%201.csv) file (note that this is not a good file name!).

In this dataset, the authors examined whether infants exposed to certain songs would recognize strangers singing these lullabies as part of their social group. Parents sang certain lullabies to their infants for 1-2 weeks. During the experiment, the infants looked at videos of two strangers: First the strangers were just smiling (baseline phase), then they would sing either the familiar or an unfamiliar lullaby. Finally, the infants again saw the videos of the strangers smiling (test phase). Eye-tracking (duration looked at each stranger) was measured.

. . .

Load the file into your Environment. Run the code and explain what happens:

```{r}
#| echo: true
#| eval: false

gaze <- read_csv("Mehr Song and Spelke 2016 Experiment 1.csv") %>%
  filter(exp1 == 1) %>%
  rename(baseline = Baseline_Proportion_Gaze_to_Singer,
         test = Test_Proportion_Gaze_to_Singer) %>% 
  select(id, baseline, test)
```

```{r}
#| echo: false

gaze <- read_csv("Data/Mehr Song and Spelke 2016 Experiment 1.csv") %>%
  filter(exp1 == 1) %>%
  rename(baseline = Baseline_Proportion_Gaze_to_Singer,
         test = Test_Proportion_Gaze_to_Singer) %>% 
  select(id, baseline, test)
```

## Assumptions of the Paired-Samples *t*-Test

1.  The data are continuous.

2.  All participants should appear in both conditions/groups.

3.  The *residuals* are normally distributed.

. . .

A paired-samples *t*-test actually tests whether the *difference* between two measurements is significantly different from 0 (= no difference/effect).

In our example data, this means that the `test` values are subtracted from the `baseline` values, and this difference is used as data.

To test the assumption that residuals are normally distributed, we thus calculate the residuals as follow:

```{r}
#| echo: true

gaze_residual <- gaze %>%
  mutate(diff = baseline - test) %>%
  mutate(group_resid = diff - mean(diff))

qqPlot(gaze_residual$group_resid)

shapiro.test(gaze_residual$group_resid)

```

## Descriptives & Visualization

For the visualization, we need the data in long format (this is identical for between-subject groups):

```{r}
#| echo: true
#| output-location: column

gaze_tidy <- gaze %>%
  pivot_longer(names_to = "phase", 
               values_to = "looking_time", 
               cols = c(baseline, test))

# boxplot
gaze_tidy %>% 
  ggplot(aes(x=phase, y=looking_time)) +
  geom_boxplot()
```

\
We can also calculate the means and SDs per phase:

```{r}
#| echo: true
#| output-location: column

gaze_tidy %>% 
  summarise(mean_looking = mean(looking_time),
            sd_looking = sd(looking_time),
            n = n(),
            .by = phase)
```

## Paired Samples *t*-Test in R

Any ideas how you would specify the paired samples *t*-test in R?

. . .

```{r}
#| echo: true
#| eval: false


t.test(NULL ~ NULL,
       paired = NULL,
       data = NULL,
       alternative = NULL)
```

. . .

```{r}
#| echo: true
#| eval: true


t.test(looking_time ~ phase, #this assumes that data are ordered by subject!
       paired = TRUE,
       data = gaze_tidy,
       alternative = "two.sided")
```

## Effect Size for the *t*-Test

You can (and should!) of course calculate and report effect sizes for your test statistics to get an impression of how practically relevant the effect is.

For the *t*-tests, you would calculate **Cohen's d** (using the function from the `effectsize` package):

```{r}
#| echo: true

effectsize::cohens_d(looking_time ~ phase, paired=T, data=gaze_tidy)

```

. . .

\
Or just use the `apa` package and you get everything in one go:
```{r}
#| echo: true

with(gaze, #the with function allows to call columns as if they were variables (but pipe function doesn't work with it)
     t.test(baseline, test, paired = TRUE)) %>% 
  apa::t_apa(es_ci=T)

```
<!--
## Write Up of the *t*-Test

What do you think you should report in your results section?

. . .

-   the means and SDs per condition (or the mean difference and the SD of the difference between conditions)

-   the test statistic *t,* incl. degrees of freedom

-   the p-value

-   the effect size

. . .

> At test stage (M = .59, SD = .18), infants showed a significantly longer preferential looking time to the singer of the familiar melody than they had shown the same singer at baseline (M = .52, SD = .18), *t*(31) = 2.42, *p* = .022, *d* = .41.
-->

## Summary: Optional Activity

:::incremental
-  Most things are similar between independent and dependent sample *t*-tests

-  Make sure to include `paired=T` for repeated measures to get correct results

-  Recommendation: Use `apa::t_apa()` with `es.ci=T` to get (most of) the information you need

-  It is often a good idea to also report means and standard deviations per group (`summarise` with `.by` argument)

-  Plotting: Unfortunately, (between subjects) confidence intervals are not diagnostic for the significance of dependent samples. Instead, we need the **confidence interval around the paired differences (i.e., the within subject changes)**. Since this is not easy to do and there is little consensus yet, I will just raise awareness here.
:::

# Correlation: Preparation

If we have two continuous variables, we can look at their relationship using **correlation**. In the data you just used, we have two continuous variables: the repeated measures of gaze.

If you skipped the previous activity, here is what you need: 

1.  Download [`Mehr Song and Spelke 2016 Experiment 1.csv`](https://github.com/spressi/biostats/blob/main/Data/Mehr%20Song%20and%20Spelke%202016%20Experiment%201.csv) (download button near the top right) into your project folder and 

2.  run this code:

```{r}
#| echo: true
#| eval: false

gaze <- read_csv("Mehr Song and Spelke 2016 Experiment 1.csv") %>%
  filter(exp1 == 1) %>%
  rename(baseline = Baseline_Proportion_Gaze_to_Singer,
         test = Test_Proportion_Gaze_to_Singer) %>% 
  select(id, baseline, test)
```

## Correlation Test

With these data, we might be interested in whether there is a relationship between the two measures of gaze, i.e., whether those children who looked at the stranger more at baseline would also be the ones who looked at the stranger more often at the follow-up test (here: indicating stability of preferences).

. . .

Similar to the paired *t*-Test, the continuous variables are also "paired" within subjects. Thus, we can a similar syntax that is easier for a wide format of data (`gaze` instead of `gaze_tidy`):

```{r}
#| echo: true

cor.test(x = gaze %>% pull(baseline), 
         y = gaze %>% pull(test), 
         method = "pearson", 
         alternative = "two.sided")
```

Note: We don't have to indicate `paired=T` because correlations only work for paired values.

## Correlations: Write Up

Once again, the `apa` package offers help to summarize the results:

```{r}
#| echo: true
cor.test(x = gaze %>% pull(baseline), 
         y = gaze %>% pull(test), 
         method = "pearson", 
         alternative = "two.sided") %>% 
  apa::cor_apa(r_ci=T)
```

. . .

You can even use `report()` to get an automatic suggestion of how to report the results (works with a lot of different models!).

```{r}
#| echo: true

library(report)
with(gaze, cor.test(x = baseline, 
                    y = test, 
                    method = "pearson", 
                    alternative = "two.sided")) %>% 
  report()
```

::: notes
We mainly use this dataset so that you don't have to load another one, but in practice you would either run a *t*-test or a correlation, not necessarily both!

The t-tests answers whether there is a change across all participants between the two time points, the correlation answers the question whether those who score high at one time point would also score high at the next (regardless of overall level).
:::

## Correlations: Notes

There are also some **assumptions** that need to be checked:

1.  Are the data *continuous*? (for ordinal values: Spearman's rho)

2.  Is there a data point for each participant on both variables? (paired values)

3.  Are the residuals *normally distributed*?

4.  Does the relationship between variables appear *linear*?

5.  Does the spread have *homoscedasticity*? (Variance of residuals should be constant across x-axis)\
Note: This is very hard to judge by eye and is not the same as the confidence band around the regression line to show constant width. In fact, the latter will always be larger near the ends of the regression line due to how it is computed.

Please see [the text book for how to test these assumptions](https://psyteachr.github.io/quant-fun-v2/correlations.html#assumptions-of-the-test).

. . .

You should - as always - report the descriptive statistics (summary statistics such as mean and SD).

. . .

You can also report and visualize multiple correlations at once, using a scatterplot matrix or heatmaps. Check out e.g. the `corrplot` package!

# Activity 3

What if we have more than two groups and/or more than one variable? For example, what if we have one variable `treatment` (with the factor levels *treatment 1, treatment 2,* and *control*) and possibly another variable called `timepoint` (*baseline, post-test*)?

Which statistical test could we use?

. . .

We would possibly run an ANOVA!

If we have only one factor (e.g. treatment with three factor levels), we would do an **one-way ANOVA**.\
If we have more than one factor but only between-subjects variables, we would run an **ANOVA**.\
If we have at least one within-subjects factor, we would run a **repeated measures (or mixed) ANOVA**.

The names are only there for historical reasons. With the right R packages, we only have to ask ourselves if a variable changes between subjects or within subjects.

Due to their potency and wide-spread use, we will cover ANOVAs even more next week in the context of the **linear model**.

::: notes
we will today only look at one-way anovas! Next week we will cover the case when we have more than one IV/predictor!
:::

## One-Way ANOVA

For this activity, we will use data from a study about memory of traumatic events (see [the textbook](https://psyteachr.github.io/quant-fun-v2/one-way-anova.html) for details). In short, the authors of the paper were interested to find out whether:

> reconsolidation - the process during which memories become malleable when recalled - can be blocked using a cognitive task and whether such an approach can reduce these unbidden intrusions.

::: incremental
1.   Download the data [`James Holmes_Expt 2_DATA.csv`](https://github.com/spressi/biostats/blob/main/Data/James%20Holmes_Expt%202_DATA.csv). This time, put it in a **subfolder** of your project called "Data".

2.   Add a column to the dataframe called `subject` that equals the `row_number()`, which will act as a participant ID.

3.   `rename()`: `Days_One_to_Seven_Image_Based_Intrusions_in_Intrusion_Diary` to `intrusions.`

4.   Select only the columns `subject`, `Condition` and `intrusions`.

5.   Change the variable `Condition` from `numeric` to a `factor` using `as.factor()`
:::

. . .

```{r}
#| echo: true

library(tidyverse)

dat <- read_csv("Data/James Holmes_Expt 2_DATA.csv") %>% 
  rownames_to_column("subject") %>% #mutate(subject = 1:n()) %>% 
  rename(intrusions = Days_One_to_Seven_Image_Based_Intrusions_in_Intrusion_Diary) %>% 
  select(subject, Condition, intrusions) %>% 
  mutate(Condition = as.factor(Condition))
```

::: notes
we will today only look at one-way anovas! Next week we will cover the case when we have more than one IV/predictor!
:::

## One-Way ANOVA 2

Create summary/descriptive statistics and visualize the data.

As summary statistics, we want the mean, SD, and SE.

```{r}
#| echo: true

se = function(x, na.rm = FALSE) { sd(x, na.rm) / sqrt(if(!na.rm) length(x) else sum(!is.na(x))) }

sum_dat <- dat %>%
  summarise(mean = mean(intrusions),
            sd = sd(intrusions),
            se = se(intrusions),
            .by = Condition)
print(sum_dat)
```

Note: The names of the factor levels are missing. If we wanted to plot the data, we should have added them in the previous step. For brevity, we will not do it this time.

<!--
. . .

```{r}
#| echo: true
#| output-location: column-fragment

ggplot(sum_dat, aes(x = Condition, y = mean, fill = Condition)) +
  stat_summary(fun = "mean", geom = "bar", show.legend = FALSE) +
  geom_errorbar(aes(ymin = mean - se, ymax = mean + se), width = 0.25) +
  scale_x_discrete(labels = c("No-task control", "Reactivation plus Tetris", "Tetris only", "Reactivation only")) +
  scale_y_continuous(limits = c(0,7), 
                     breaks = c(0,1,2,3,4,5,6,7), 
                     name = "Intrusive-Memory Frequency (Mean for the Week)") 

```
-->
::: notes
don't use a bar chart!
:::

## One-Way ANOVA 3

1.  Run the ANOVA using `lm()` and formula notation.

```{r}
#| echo: true
mod1 <- lm(intrusions ~ Condition, data = dat)
anova(mod1)
```

2.  Using `afex::aov_ez`:

```{r}
#| echo: true
library(afex)
mod <- aov_ez(id = "subject", # the column containing the subject IDs
              dv = "intrusions", # the DV 
              between = "Condition", # the between-subject variable
              es = "pes", # can output an effect size! we want partial eta-squared
              type = 3, # there are both reasons for 2 and 3 (not covered here)
              include_aov = TRUE, # needed for some calculations with emmeans but takes longer
              data = dat)
anova(mod)

```

## Checking the Assumptions

```{r}
#| echo: true
library(performance)
library(patchwork)

check_model(mod1) # doesn't work for ezANOVA output!


# "manually" check normality of residuals 

qqPlot(mod$aov$residuals)
shapiro.test(mod$aov$residuals)

# check homogeneity of variance
test_levene(mod)
```

Both assumptions are not met!

**But** ANOVAS are quite robust to (minor) deviations...

(if the assumptions are violated more, you could...\
- run a non-parametric test (Kruskall-Wallis for between-subjects designs of Friedman for repeated measures)\
- transform the data (see Field et al., 2009)\
- use bootstrapping (see Field et al., 2009)

## Post-Hoc Tests

So now we know that there are differences between the `Condition`s, but we don't know yet which groups differ from each other. We could thus calculate *pairwise comparisons* or *post-hoc t-tests* to compare each condition to the others by by filtering the data (so that only two conditions remain) and running *t*-tests. 

A more convenient way is to use the `emmeans()` function from the package with the same name. We can also adjust the tests for *multiple comparisons* directly.

We could also define specific *contrasts* to test **a priori** (i.e., based on hypotheses). But we will skip this here.

```{r}
#| echo: true
library(emmeans)
mod %>% emmeans(pairwise ~ Condition, adjust = "bonferroni") # also works with mod1!
```

:::notes
Should we adjust for multiple comparisons? It depends. There are good reasons for either.
:::

## Effect Sizes & Power

The most common effect size estimate for an ANOVA is *partial eta squared* $\eta_p^2$ (as we calculated with `afex::aov_ez` before). We get *one per factor* and it summarizes all pairwise effects within it. \
We could also calculate Cohen's d for each pairwise comparison within the factor but this is tedious and gets confusing quickly (1 factor with 5 levels = 10 effects; 3 factors with 5 levels each = 30 effects). It's still sad that `emmeans` doesn't output it as one additional column.

<!--
```{r}
#| echo: true
library(lsr)

d_1_2 <- cohensD(intrusions ~ Condition, 
                 data = filter(dat, Condition %in% c(1,2)) %>% 
                   droplevels())

d_1_3 <- cohensD(intrusions ~ Condition, 
                 data = filter(dat, Condition %in% c(1,3)) %>%
                   droplevels()) 

# and so forth...

d_1_2
d_1_3
```
-->
. . .

\
Effect sizes are important to calculate the **statistical power** of your study before conducting it. Use an estimated effect size based on prior research:

```{r}
#| echo: true
library(pwr)
pwr.anova.test(k = 4, f = .4, sig.level = .05, power = .8) #we leave out the n parameter to let it be calculated
```



## Write Up

Once again, the `apa` package helps us here - if we used `afex::aov_ez` instead of `lm` 😅:

```{r}
#| echo: true
mod %>% apa::anova_apa()
```

. . .

```{r}
#| echo: true
mod %>% emmeans("Condition") %>% pairs(adjust="none") #follow up t-tests without adjustment
```

Looking at contrasts including condition 1 (control group): Only condition 2 different. \
Let's get the associated effect size of this specific contrast:

. . .

```{r}
#| echo: true
library(lsr)
cohensD(intrusions ~ Condition, 
        data = filter(dat, Condition %in% c(1,2)) %>% droplevels())
```

. . .

> There was a significant difference between groups in overall intrusion frequency in daily life, $F(3, 68) = 3.79, p = 0.014, \eta_p^2 = .14$. Pairwise comparisons demonstrated that relative to the no-task control group, only those in the reactivation-plus-Tetris group, $t(68) = 3.04, p < 0.01, d = 1.00$, experienced significantly fewer intrusive memories...

# Thanks!

Learning objectives:

-   Be able to run the following tests in R, check their assumptions, and report the results: Chi², *t*-tests, correlation, and one-way ANOVA

Next week:

-   General Linear Model!
