---
title: "02 Models"
subtitle: |
   | Julius-Maximilians-University WÃ¼rzburg 
   | Course: "Biostatistics"    
   | Translational Neuroscience
author: "Dr. Lea Hildebrandt"
format: 
  revealjs:
    smaller: true
    scrollable: true
    slide-number: true
    theme: serif
    chalkboard: true
    width: 1280
    height: 720
from: markdown+emoji
---

# Fitting Models to Data

. . .

Form groups of 3-4 and collect the height of each member!

```{css}
code.sourceCode {
  font-size: 1.4em;
}

div.cell-output-stdout {
  font-size: 1.4em;
}
```

```{r}
#| echo: false
#| message: false

library(tidyverse)
library(NHANES)
library(cowplot)
library(mapproj)
library(pander)
library(knitr)
library(modelr)
panderOptions('round',2)
panderOptions('digits',7)
options(digits = 2)
set.seed(123456) # set random seed to exactly replicate results
```

::: notes
"Fitting" "Models" to "Data", let's start with collecting data! \
Form teams of 4 and collect the height of each of the team members.

Tricky content, some formulas, feel free to ask questions anytime! It's important that you understand the concepts!

(We might not manage to go through all the slides, but still ask questions - slides/video online)
:::

## Why do we need Models?

What can we do with data only? Where can a model be helpful?

::: notes
summarize/simplify, generalize, explain
:::

## What is a Model?

> "models" are generally simplifications of things in the real world that nonetheless convey the essence of the thing being modeled
>
> All models are wrong but some are useful (George Box)

(ST21, Ch 5)

**Aim**: Find the model that efficiently and accurately summarizes the data.

## A Simple Model

Let's say we want to have a model of height of students in this Biostatistics class (or height of children based on the NHANES dataset, as used in ST21, see below).

What do you think would be a good model for the height of a student/child?\
(Or: Which value should we guess for a particular or new student/child?)

. . .

```{r}
#| echo: false 
#| warning: false 
#| message: false  
#| 
# drop duplicates 
NHANES <- NHANES %>%    
  dplyr::distinct(ID, .keep_all = TRUE)  

# select the appropriate children with good height measurements 
NHANES_child <- NHANES %>%   
  drop_na(Height) %>%   
  subset(Age < 18) 

NHANES_child %>%  
  ggplot(aes(Height)) +    
  geom_histogram(binwidth = 1, fill="grey", color="black") +   
  ylab("Absolute Frequency") + 
  xlab("Height (cm)") + 
  theme_bw()
```

::: notes
Have them guess the height of an anonymous new student, discuss what could be relevant information. Gender = not known (prior knowledge!).

Mean = model?
:::

## Basic structure of statistical models

$$
data=model+error
$$

::: notes
a statistical model is generally much simpler than the data being described; it is meant to capture the structure of the data as simply as possible.

Two parts:

-   one portion that is described by a statistical model, which expresses the values that we expect the data to take given our knowledge,

-   *error* that reflects the difference between the model's predictions and the observed data.

Example? (Average) age in classroom? All prior knowledge goes into model
:::

. . .

In general, we want to predict single observations (denoted by i) from the model. The fact that we are looking at predictions and not at actual values of the data/observations is denoted by the "hat":

$$ \widehat{data_i} = model_i $$ The error is then simply the deviation of the actual data from the predicted values:

$$  error_i = data_i - \widehat{data_i} $$

This means that the predicted value of the data for observation i is equal to the value of the model for that observation.

## A Simple Model 2

The simplest model would be to predict the *mean* of the height values for *every* student/child! This would imply that individual deviations of the mean would be interpreted to be (prediction) errors in such a model.

We can write such a simple model as a formula:

$$
y_i = \beta + \epsilon
$$

$y_i$ denotes the individual observations (hence the $i$) of heights, $\beta$ is a so-called *parameter*, and $\epsilon$ is the error term. In this example, the parameter $\beta$ would be the same value (= the mean height) for everyone (hence it doesn't need an $i$ subscript). *Parameters* are values that we optimize to find the best model.

::: notes
What would be Yi, beta and error for the height example?
:::

## A Simple Model 3

How do we find parameters that belong to the best fitting model?

How did you come up with model for heights, e.g. the mean?

. . .

We try to minimize the error!

Remember, the error is the difference between the actual and predicted values of $y$ (height):

$$
error_i = y_i - \hat{y_i}
$$

If we select a predicted value (or mean) of 400cm, all individuals' errors would hugely deviate (because no one is 4m tall). If we average these errors, it would still be a big value.

A better candidate for such a simple model is thus the arithmetic mean or average:

$$
\bar{X} = \frac{\sum_{i=1}^{n}x_i}{n}
$$

Summing up all individual's heights and dividing that number by the number of individuals gives us the mean. By definition, the average (directed) error is now 0 (see book for proof, the individual errors cancel out)! This means that the average has no *bias* to over- or underestimate observations (while 4m would have been a clear overestimation).

## A Note on Errors

We usually don't simply average across the individual (signed) errors, but across the *squared* errors.

The reason is that we do not want positive and negative errors to cancel each other out.

The *mean squared error* would be in a different unit than the data (e.g., cm^2^), which is why we usually take the square root of that value to bring it back to the original unit: This leaves us with the *root mean squared error (RMSE)*!

Note: We *could* also use the absolute values of errors, sum those up, and avoid any of these problems. For historical reasons, we do not.

## A Slightly More Complex Model

Obviously, the model for predicting height from the average is not very good (RMSE = 27 cm). How can we improve this model?

. . .

We can account for other information that we might have!\
For example, to account for age might be a good idea: Older children are likely taller than younger ones. We plot height against age to visually inspect the relationship:

. . .

```{r}
#| echo: false

p1 <- NHANES_child %>% 
  ggplot(aes(x = Age, y = Height)) +
  geom_point(position = "jitter",size=0.05) +
  scale_x_continuous(breaks = seq.int(0, 20, 2)) +
  ggtitle('A: original data')

lmResultHeightOnly <- lm(Height ~ Age + 0, data=NHANES_child)
rmse_heightOnly <- sqrt(mean(lmResultHeightOnly$residuals**2))

p2 <- NHANES_child %>% 
  ggplot(aes(x = Age, y = Height)) +
  geom_point(position = "jitter",size=0.05) +
  scale_x_continuous(breaks = seq.int(0, 20, 2)) + 
  annotate('segment',x=0,xend=max(NHANES_child$Age),
           y=0,yend=max(lmResultHeightOnly$fitted.values),
           color='blue',lwd=1) + 
  ggtitle('B: age')

p3 <- NHANES_child %>% 
  ggplot(aes(x = Age, y = Height)) +
  geom_point(position = "jitter",size=0.05) +
  scale_x_continuous(breaks = seq.int(0, 20, 2)) + 
  geom_smooth(method='lm',se=FALSE) + 
  ggtitle('C: age + constant')

p4 <- NHANES_child %>% mutate(Gender = factor(Gender)) %>% 
  ggplot(aes(x = Age, y = Height)) +
  geom_point(aes(colour = Gender), 
             position = "jitter", 
             alpha = 0.8,
             size=0.05) +
  geom_smooth(method='lm',aes(group = factor(Gender), 
                              colour = factor(Gender))) + 
  theme(legend.position = c(0.125,0.8)) + 
  ggtitle('D: age + constant + gender')

plot_grid(p1,p2,p3,p4,ncol=2)

```

::: notes
RMSE: On average, 27 cm "wrong" per individual!

A: raw data, visible strong relationship\
B: only age (linear relationship)\
C: intercept/constant\
D: also account for gender

--\> line fits data increasingly better!
:::

## A Slightly More Complex Model 2

As we can see, the line (\~ model) fits the data points increasingly well, e.g. if we include a constant (also called "intercept") and age. We would write this as this formula:

$$
\hat{y_i} = \hat{\beta_0} + \hat{\beta_1} * age_i
$$

Remember from linear algebra that this defines a line:

$$
y = intercept + slope * x
$$

Thus $\beta_0$ is the parameter for the intercept and $\beta_1$ for the slope of age!

The model fit is now much better: RMSE = 8.36 cm.

. . .

Adding gender? Does not improve model too much! (compared to age)

::: notes
w/o intercept: A, no $\beta_0$

Stats Software will estimate best values for $\beta$'s
:::

## What is a "Good" Model?

Two aims:

1.  Describe data well (= low error/RMSE)

2.  Generalize to new data (low error when applied to new data)

Can be conflicting!

. . .

Where does error come from?

Any ideas? Where could the error come from in your height model?

. . .

::: incremental
-   measurement error (noise): random variation in data

    -   dependent variable is hard to measure precisely (difficult/noisy conditions)

    -   cheap/inadequate equipment for measuring

-   wrong model specification

    -   important variable is missing from model (age!)

    -   e.g., height has a quadratic relationship with age (old people shrink again)
:::

## Examples Measurement Error

```{r BACrt,echo=FALSE,message=FALSE, fig.cap="Simulated relationship between blood alcohol content and reaction time on a driving test, with best-fitting linear model represented by the line. A: linear relationship with low measurement error.  B: linear relationship with higher measurement error.  C: Nonlinear relationship with low measurement error and (incorrect) linear model"}

dataDf <-  
  tibble(
    BAC = runif(100) * 0.3,
    ReactionTime = BAC * 1 + 1 + rnorm(100) * 0.01
  )

p1 <- dataDf %>% 
  ggplot(aes(x = BAC, y = ReactionTime)) + 
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) + 
  ggtitle('A: linear, low noise')
# noisy version
dataDf <-  
  tibble(
    BAC = runif(100) * 0.3,
    ReactionTime = BAC * 2 + 1 + rnorm(100) * 0.2
  )
p2 <- dataDf %>% 
  ggplot(aes(x = BAC, y = ReactionTime)) + 
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) + 
  ggtitle('B: linear, high noise')
# nonlinear (inverted-U) function
dataDf <-
  dataDf %>% 
  mutate(
    caffeineLevel = runif(100) * 10,
    caffeineLevelInvertedU = (caffeineLevel - mean(caffeineLevel))**2,
    testPerformance = -1 * caffeineLevelInvertedU + rnorm(100) * 0.5
  )
p3 <- dataDf %>% 
  ggplot(aes(x = caffeineLevel, y = testPerformance)) + 
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) + 
  ggtitle('C: nonlinear')
plot_grid(p1,p2,p3)
```

::: notes
A: very little error, all points close to fitted line\
B: same relationship much more variability across individuals\
C. wrongly specified model (caffeine!), not a linear relationship. Error high (deviations points - line)
:::

## Can a Model be "too Good"?

Yes! This is called *overfitting*.

. . .

If we fit a line too closely to the data (e.g., with an 8th degree polynomial), the model might not be able to generalize to other data well.

```{r Overfitting,echo=FALSE,message=FALSE,warning=FALSE, fig.cap='An example of overfitting. Both datasets were generated using the same model, with different random noise added to generate each set.  The left panel shows the data used to fit the model, with a simple linear fit in blue and a complex (8th order polynomial) fit in red.  The root mean square error (RMSE) values for each model are shown in the figure; in this case, the complex model has a lower RMSE than the simple model.  The right panel shows the second dataset, with the same model overlaid on it and the RMSE values computed using the model obtained from the first dataset.  Here we see that the simpler model actually fits the new dataset better than the more complex model, which was overfitted to the first dataset.',fig.width=8,fig.height=4,out.height='50%'}

#parameters for simulation
set.seed(1122)
sampleSize <- 16
#build a dataframe of simulated data
simData <- 
  tibble(
    X = rnorm(sampleSize),
    Y = X + rnorm(sampleSize, sd = 1),
    Ynew = X + rnorm(sampleSize, sd = 1)
  )
#fit models to these data
simpleModel <- lm(Y ~ X, data = simData)
complexModel <- lm(Y ~ poly(X, 8), data = simData)
#calculate root mean squared error for "current" dataset
rmse_simple <- sqrt(mean(simpleModel$residuals**2))
rmse_complex <- sqrt(mean(complexModel$residuals**2))
#calculate root mean squared error for "new" dataset
rmse_prediction_simple <- sqrt(mean((simpleModel$fitted.values - simData$Ynew)**2))
rmse_prediction_complex <- sqrt(mean((complexModel$fitted.values - simData$Ynew)**2))
#visualize
plot_original_data <- 
  simData %>% 
  ggplot(aes(X, Y)) +
  geom_point() +
  geom_smooth(
    method = "lm", 
    formula = y ~ poly(x, 8), 
    color = "red", 
    se = FALSE
  ) +
  geom_smooth(
    method = "lm", 
    color = "blue", 
    se = FALSE
  ) +
  ylim(-3, 3) +
  annotate(
    "text",
    x = -1.25, 
    y = 2.5, 
    label = sprintf("RMSE=%0.1f", rmse_simple),
    color = "blue", 
    hjust = 0, 
    cex = 4
  ) +
  annotate(
    "text",
    x = -1.25, 
    y = 2, 
    label = sprintf("RMSE=%0.1f", rmse_complex),
    color = "red", 
    hjust = 0, 
    cex = 4
  ) +
  ggtitle("original data") 
plot_new_data  <- 
  simData %>% 
  ggplot(aes(X, Ynew)) +
  geom_point() +
  geom_smooth(
    aes(X, Y), 
    method = "lm", 
    formula = y ~ poly(x, 8), 
    color = "red", 
    se = FALSE
  ) +
  geom_smooth(
    aes(X, Y), 
    method = "lm", 
    color = "blue", 
    se = FALSE
  ) +
  ylim(-3, 3) +
  annotate(
    "text",
    x = -1.25, 
    y = 2.5, 
    label = sprintf("RMSE=%0.1f", rmse_prediction_simple),
    color = "blue", 
    hjust = 0, 
    cex = 4
  ) +
  annotate(
    "text",
    x = -1.25, 
    y = 2, 
    label = sprintf("RMSE=%0.1f", rmse_prediction_complex),
    color = "red", 
    hjust = 0, 
    cex = 4
  ) +
  ggtitle("new data") 
plot_grid(plot_original_data, plot_new_data)
```

. . .

How would overfitting look like in your height examples?

::: notes
same formula, different noise (simulation) \~ different individuals

simpler model fits new data better!

overfitting in height examples?
:::

# Summarizing Data

## Central Tendency

Why summarize data?

. . .

A summary is a model & describes the data! E.g., mean = central tendency of the data

. . .

Mean, Median, Mode?

. . .

**Mean** = "Balance point" of data; minimizes sum of squared error, but highly influenced by outliers!\
**Median** = "middle" of ranked data; minimizes sum of absolute error, less influenced by extreme values\
**Mode** = most often occurring value (i.e., absolute peak)

. . .

Example:

If 4 people earn 50,000 Euros per year and 1 person earns 1,000,000:\
Mean: 240,000 Euros\
Median: (Rank order: 10,000; 10,000; **10,000**; 10,000; 1,000,000) -\> middle value = 10,000 Euros\
Mode: 10,000 Euros

::: notes
examples

mean: income --\> if one person earns a million and 3 only 10.000 --\> mean = 257.500
:::

## Variability

How widespread are the data?

. . .

**Variance** and **Standard Deviation**

**Variance** = Mean Squared Error

$$
\sigma^2 = \frac{SSE}{N} = \frac{\sum_{i=1}^n (x_i - \mu)^2}{N}
$$

(Note: $x_i$ = value of ind. observation, $\mu$ = *population* mean instead of $\hat{X}$ = *sample* mean)

. . .

**Standard Deviation** â Root Mean Squared Error

$$
SD = \sigma = \sqrt{\sigma^2}
$$

. . .

We usually don't know the population mean $\mu$, that's why we estimate the sample variance using the *sample* mean $\hat{X}$ (both with the "hat") and the *sample* size $n$ instead of the *population* size $N$:

$$
\hat\sigma^2 = \frac{\sum_{i=1}^n (x_i - \hat{X})^2}{n-1}
$$

Note: $n-1$ is used to make the estimate more robust/less biased (I cannot give you an intuition why this is better... please trust me!).

::: notes
Remember plot above: Points either close to line or wide spread

Variance = sigma\^2, deviations of data points from mean ($\mu$) squared and summed, divided by number of oberservations

$n-1$ = Degrees of Freedom, one value is fixed if we know the mean.
:::

## Comparing apples and oranges: Z-Scores

$$
Z_i(x) = \frac{x_i - \mu}{\sigma}
$$

::: incremental
-   standardizes the distribution: How far is any data point from the mean in units of SD?
-   doesn't change original relationship of data points!
    -   shifts distribution to have a mean = 0 and scales it to have SD = 1.
-   useful if we compare (or use in a model) variables on different scales/units!
:::

. . .

```{r zDensityCDF,echo=FALSE,fig.width=5,fig.height=3, fig.cap="Density (top) and cumulative distribution (bottom) of a standard normal distribution, with cutoffs at one standard deviation above/below the mean."}

# First, create a function to generate plots of the density and CDF
dnormfun <- function(x) {
  return(dnorm(x, 248))
}
plot_density_and_cdf <- 
  function(zcut, zmin = -4, zmax = 4, plot_cdf = TRUE, zmean = 0, zsd = 1) {
    zmin <- zmin * zsd + zmean
    zmax <- zmax * zsd + zmean
    x <- seq(zmin, zmax, 0.1 * zsd)
    zdist <- dnorm(x, mean = zmean, sd = zsd)
    area <- pnorm(zcut) - pnorm(-zcut)
    
    p2 <- 
      tibble(
        zdist = zdist, 
        x = x
      ) %>% 
      ggplot(aes(x, zdist)) +
      geom_line(
        aes(x, zdist), 
        color = "red", 
        size = 2
      ) +
      stat_function(
        fun = dnorm, args = list(mean = zmean, sd = zsd),
        xlim = c(zmean - zcut * zsd, zmean + zsd * zcut),
        geom = "area", fill = "orange"
      ) +
      stat_function(
        fun = dnorm, args = list(mean = zmean, sd = zsd),
        xlim = c(zmin, zmean - zcut * zsd),
        geom = "area", fill = "green"
      ) +
      stat_function(
        fun = dnorm, args = list(mean = zmean, sd = zsd),
        xlim = c(zmean + zcut * zsd, zmax),
        geom = "area", fill = "green"
      ) +
      annotate(
        "text",
        x = zmean,
        y = dnorm(zmean, mean = zmean, sd = zsd) / 2,
        label = sprintf("%0.1f%%", area * 100)
      ) +
      annotate(
        "text",
        x = zmean - zsd * zcut - 0.5 * zsd,
        y = dnorm(zmean - zcut * zsd, mean = zmean, sd = zsd) + 0.01 / zsd,
        label = sprintf("%0.1f%%", pnorm(zmean - zsd * zcut, mean = zmean, sd = zsd) * 100)
      ) +
      annotate(
        "text",
        x = zmean + zsd * zcut + 0.5 * zsd,
        y = dnorm(zmean - zcut * zsd, mean = zmean, sd = zsd) + 0.01 / zsd,
        label = sprintf("%0.1f%%", (1 - pnorm(zmean + zsd * zcut, mean = zmean, sd = zsd)) * 100)
      ) +
      xlim(zmin, zmax) +
      labs(
        x = "Z score",
        y = "density"
      )
    
    if (plot_cdf) {
      cdf2 <- 
        tibble(
          zdist = zdist, 
          x = x, 
          zcdf = pnorm(x, mean = zmean, sd = zsd)
        ) %>% 
        ggplot(aes(x, zcdf)) +
        geom_line() +
        annotate(
          "segment",
          x = zmin, 
          xend = zmean + zsd * zcut,
          y = pnorm(zmean + zsd * zcut, mean = zmean, sd = zsd),
          yend = pnorm(zmean + zsd * zcut, mean = zmean, sd = zsd),
          color = "red", 
          linetype = "dashed"
        ) +
        annotate(
          "segment",
          x = zmean + zsd * zcut, 
          xend = zmean + zsd * zcut,
          y = 0, yend = pnorm(zmean + zsd * zcut, mean = zmean, sd = zsd),
          color = "red", 
          linetype = "dashed"
        ) +
        annotate(
          "segment",
          x = zmin, 
          xend = zmean - zcut * zsd,
          y = pnorm(zmean - zcut * zsd, mean = zmean, sd = zsd),
          yend = pnorm(zmean - zcut * zsd, mean = zmean, sd = zsd),
          color = "blue", 
          linetype = "dashed"
        ) +
        annotate(
          "segment",
          x = zmean - zcut * zsd, 
          xend = zmean - zcut * zsd,
          y = 0, 
          yend = pnorm(zmean - zcut * zsd, mean = zmean, sd = zsd),
          color = "blue", 
          linetype = "dashed"
        ) +
        ylab("Cumulative density")
      
      plot_grid(p2, cdf2, nrow = 2)
    } else {
      print(p2)
    }
  }
plot_density_and_cdf(1, plot_cdf = FALSE)
```

::: notes
Z of x

x_i is single value/data point\
mu, sigma

z-scores: Now you can compare apples to oranges! Imagine two siblings from different classes battling it out on test results:\
- I got 70 points, you got only 60!\
- My test only had 70 points in total, yours had 85. I got 86% correct, you only 82%!\
- My test was harder! The average result was 60 with a standard deviation of 10, so I am 1 SD above the class average!\
- On my test, an average of 52 was achieved with an SD of 8, so I am also 1 SD above the class average. But people in your class are really stupid so that's a very low standard to compare yourself.\
Narrator: And this was the end of comparability between apples and oranges :)
:::

# Thanks

Learning objectives:

-   Describe the basic equation for statistical models (data=model + error)

-   Describe different measures of central tendency and dispersion, how they are computed, and which are appropriate under what circumstances.

-   Compute a Z-score and describe why they are useful.

. . .

Next topics: R sessions on Data Wrangling & Plotting.
