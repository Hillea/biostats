[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Biostatistics",
    "section": "",
    "text": "Welcome to the website of the course “Biostatistics” of the master’s programme “Translational Neuroscience” of the Julius-Maximilians-University Würzburg.\nPlease use the navigation on top to select the slides for each session (recommendation: Open in new tab)."
  },
  {
    "objectID": "W1_Intro.html#hello",
    "href": "W1_Intro.html#hello",
    "title": "01a Introduction to Biostatistics",
    "section": "Hello!",
    "text": "Hello!\nWho am I?\n\nWho are you?\n\nWhat is your background?\nDo you have experience with data analysis?\nWhat’s your attitude towards statistics?\n\n\n\nSurvey (Zoom) + Moodmeter (pick a stamp - top of screen, select annotate…)\nToo many people to have an introduction round!\nI know that you probably don’t know each other yet, there will be some break-out sessions where you can get to know each other a bit (and talk about stats!) ;)\nStats Anxiety: It will be packed, but it will be ok (You can always reach me with questions!)!"
  },
  {
    "objectID": "W1_Intro.html#using-the-slides",
    "href": "W1_Intro.html#using-the-slides",
    "title": "01a Introduction to Biostatistics",
    "section": "Using the Slides",
    "text": "Using the Slides\n\nThese slides are created directly in R with the quarto extension.\nYou can jump to a slide by clicking the three dashes in the bottom left.\nYou can conveniently copy R code from the slides with one click and paste it into your RStudio.\n\n\nprint(\"Hello World\")\n\n[1] \"Hello World\""
  },
  {
    "objectID": "W1_Intro.html#organizational-issues",
    "href": "W1_Intro.html#organizational-issues",
    "title": "01a Introduction to Biostatistics",
    "section": "Organizational Issues",
    "text": "Organizational Issues\n\n\n\nAttendance is mandatory! (I don’t make the rules :) )\n\nYou may miss one session without giving reasons (recommendation: don’t waste it early!)\nIf you miss additional sessions, please write an email with an explanation (further proof may be required, e.g. doctor’s certificate)\n\n\n\n\n\nThe course will take place in person\n\nOnly for students who are not in Würzburg yet, there is an option via Zoom\n\n\n\n\n\nWe will use these textbooks (Open Educational Resources - freely available online, also linked in WueCampus):\n\nStatistical Thinking for the 21st Century: https://statsthinking21.github.io/statsthinking21-core-site/index.html\nFor the R part: Fundamentals of Quantitative Analysis: https://psyteachr.github.io/quant-fun-v2/\n\n\n\n\nOnline: Participation, videos, chat…\nOr R Session on Tuesday and video before? Some can’t make it on Wednesdays… (chat or speak out)\nAttendance: If 2x per week sync: max. 3 missed classes, if 1x per week: max. 2 missed classes (unless I know your reasons for missing Wednesdays!) –&gt; if you missed more, I can’t admit you for the exam/report\nThe input and hands-on sessions will be highly based on these two textbooks. You don’t need to read the textbooks, but it will of course help if you either read the chapter before or after the sessions: Repetition is always helpful!"
  },
  {
    "objectID": "W1_Intro.html#contents",
    "href": "W1_Intro.html#contents",
    "title": "01a Introduction to Biostatistics",
    "section": "Contents",
    "text": "Contents\n\n\nFrom basic probability to (Generalized) Linear Mixed Models\n\nSome things may be repetitive for you but this course aims to provide a common starting position for your next semesters\n\nInput (lecture style) with hands-on R sessions \nIn addition, you should read a few pages in the text books\n(Statistical Thinking for the 21st Century, and possibly Fundamentals of Quantitative Analysis)\nProject: Independently analyze a dataset (exam with pass or fail grading)\n\n\n\nFor some, e.g. the psychologists, it will be more of a repetition - but you will also learn R.\nSlides might be text heavy –&gt; so that you can go through the slides afterwards again (but textbook might also be helpful)"
  },
  {
    "objectID": "W1_Intro.html#project",
    "href": "W1_Intro.html#project",
    "title": "01a Introduction to Biostatistics",
    "section": "Project",
    "text": "Project\n\n\nFind a dataset that can answer a question you are interested in\n\nhttps://ourworldindata.org/\nStatistisches Bundesamt\nYour own, e.g., from an internship\n\n\n\n\n\npreprocess/wrangle it,\nanalyze the data,\nand write a short (min. 2-page) report!\n\nshort intro incl. research question and hypothesis\nmethods (both how the data were acquired and how they are analyzed)\nresults (incl. at least one plot)\nand a short discussion.\n\nAll these parts should be at least half a page long."
  },
  {
    "objectID": "W1_Intro.html#calender",
    "href": "W1_Intro.html#calender",
    "title": "01a Introduction to Biostatistics",
    "section": "Calender",
    "text": "Calender\n\n\n\n\n\n\n\n\n\nDate\nTopic\nReading\nProject Deadlines\n\n\n\n\n17.10.\nGeneral & R Intro\nST21: 1-3, QF: 1-3\n\n\n\n24.10.\nProbability\nST21: 4, QF: 4-6\n\n\n\n31.10.\nData Wrangling\nST21: 4, QF: 4-6\n\n\n\n07.11.\nData Visualization\nQF: 7\n\n\n\n14.11.\nSampling\nST21: 7-8, QF: 8\n\n\n\n21.11.\nProbability & Sampling in R\nST21: 7-8, QF: 8\n\n\n\n28.11.\nHypothesis Testing\nST21: 9-10\n\n\n\n05.12.\nComparing Means & Categories\nST21: 12, 15\nDataset\n\n\n12.12.\nExercises (t-Tests, Chi²)\nST21: 12, 15\n\n\n\n19.12.\n(Generalized) Linear Models\nST21: 12-13\nResearch Question & Hypotheses\n\n\n26.12. & 02.01.\nChristmas & New Year’s\n\n\n\n\n09.01.\nExercises (GLM)\nST21: 12-13\n\n\n\n16.01.\nLinear Mixed Models\nST21: 14\n\n\n\n23.01.\nExercises (LMM)\nST21: 14\nAnalysis\n\n\n30.01.\nTroubleshooting Your Report\n\n\n\n\n06.02.\nReproducible Research\nST21: 18\nReport\n\n\n\n\n\nThe first four weeks will be basics, the next 4 will be analyses.\n\n\nST21: Statistical Thinking for the 21st Century\nQF: Fundamentals of Quantitative Analysis (QuantFun)"
  },
  {
    "objectID": "W1_Intro.html#why-is-it-important-that-you-know-statistics",
    "href": "W1_Intro.html#why-is-it-important-that-you-know-statistics",
    "title": "01a Introduction to Biostatistics",
    "section": "Why is it important that YOU know statistics?",
    "text": "Why is it important that YOU know statistics?\n\n\nYou’re doing a research master!\n\nResearch = Reading & understanding papers (esp. the analyses)\nDesigning your own experiments, analyze data, interpret results\n\nWe live in an increasingly data-centric world\n\nKnowing how to wrangle and analyze data is a valuable skill\n\nFacts & data literacy matter more than ever!\n\nFake News, “Lying with stats”, Reproducibility Crisis\nBeing able to call bullshit (https://www.callingbullshit.org/)\n\n“I only believe in statistics that I doctored myself” ― Winston S. Churchill\n\nHowever: “It is easy to lie with statistics, but easier to lie without them” ― Frederick Mosteller\n\n\n\n\nbreak-out session, 3 min.\n–&gt; Afterwards: write in chat or speak, what did you come up with?\npossibly show empirical cycle or the like to indicate that stats are necessary at almost every step"
  },
  {
    "objectID": "W1_Intro.html#what-is-statistical-thinking",
    "href": "W1_Intro.html#what-is-statistical-thinking",
    "title": "01a Introduction to Biostatistics",
    "section": "What is Statistical Thinking?",
    "text": "What is Statistical Thinking?\n\n\n“a systematic way of thinking about how we describe the world and use data [to] make decisions and predictions, all in the context of the inherent uncertainty that exists in the real world.” (Poldrack, Preface of ST21)\n“Statistical thinking is a way of understanding a complex world by describing it in relatively simple terms that nonetheless capture essential aspects of its structure or function, and that also provide us some idea of how uncertain we are about that knowledge.” (Poldrack, Chapter 1)\n\n\n\nbreak down complexity, include uncertainty"
  },
  {
    "objectID": "W1_Intro.html#why-is-statistical-thinking-important",
    "href": "W1_Intro.html#why-is-statistical-thinking-important",
    "title": "01a Introduction to Biostatistics",
    "section": "Why is Statistical Thinking Important?",
    "text": "Why is Statistical Thinking Important?\n\n\ndata literacy vs. intuition/heuristics/anecdotal evidence\n\nPublic discourse about Covid-19, migration, etc. (e.g., “50% of people in intensive care are vaccinated”)\n\n\n\n\n\n\n\nBase Rate Fallacy\n\n\n\nWrite in chat! (after first bullet point!)\nexample availability heuristic from book (or any other example where intuition is wrong, i.e. vaccinations/covid…)\n--&gt; test in class? Ask for opinion/intuition, show data"
  },
  {
    "objectID": "W1_Intro.html#what-can-statistics-do-for-us",
    "href": "W1_Intro.html#what-can-statistics-do-for-us",
    "title": "01a Introduction to Biostatistics",
    "section": "What can Statistics Do For Us?",
    "text": "What can Statistics Do For Us?\n\nDescribe patterns by summarizing/breaking down data (“descriptive statistics”)\nDecide whether one thing is better than another, given the uncertainty (“inferential statistics”)\nPredict how other people would “behave” (generalize to new observations)\n\n\ndescribe: not useful to look at every single data point/person, but we need s.th. like tendencies/trends…"
  },
  {
    "objectID": "W1_Intro.html#the-big-ideas",
    "href": "W1_Intro.html#the-big-ideas",
    "title": "01a Introduction to Biostatistics",
    "section": "The Big Ideas",
    "text": "The Big Ideas\n\n\nLearning from data: Update our beliefs\nAggregation: How to summarize the data to draw meaningful conclusions?\nUncertainty: Probabilistic evidence\nSampling from the population: Which people etc. do we select?\n\n\n\nask for every point what I could mean w/ it?\nLfD: gather new knowledge\nAgg: Can’t look at all ind data points, need to find trends etc. (should not go to far! throwing out data)\nuncert: stats = tools for making decisions under uncert, we can never prove anything but provide evidence, there is no 100% certainty for an outcome (cancer)\nsampling: how do we represent the population? What is the population? how much data do we need? More is better, but payoff decreases…"
  },
  {
    "objectID": "W1_Intro.html#causality",
    "href": "W1_Intro.html#causality",
    "title": "01a Introduction to Biostatistics",
    "section": "Causality",
    "text": "Causality\nCorrelation does not imply causation… but is a hint!\n\nExample: Smoking = less risk for Parkinson’s disease? (Godwin-Austen et al., 1982; Chen et al., 2010)\n\n\n--&gt; confounding factors?\n\n\ne.g., individual dopaminergic activity =&gt; addiction & motor function\n\n\nRandomized Controlled Trials (RCT) as the solution?\n\ngive example! Eat more fat = living longer? Confounders (richer people, healthier diets, less stress, better health care…)\nRCT: exp control and manipulation, removes confounds if done well\nAt least some more causal evidence!\nQUESTIONS so far?"
  },
  {
    "objectID": "W1_Intro.html#what-are-data",
    "href": "W1_Intro.html#what-are-data",
    "title": "01a Introduction to Biostatistics",
    "section": "What are Data?",
    "text": "What are Data?\n\nWhat do you think are data?\n\n\n\nqualitative vs. quantitative\n\nqualitative?\n\nopen questions, descriptions… can potentially be coded into categories\n\nquantitative?\n\nnumeric, can be averaged etc.\n\n\n\n\n\nChat –&gt; after showing slide! Come up with examples for “Data”\nCollect: Do you have ideas? What are data you encounter in your lives/work etc? What are differences between these data?"
  },
  {
    "objectID": "W1_Intro.html#what-are-data-2",
    "href": "W1_Intro.html#what-are-data-2",
    "title": "01a Introduction to Biostatistics",
    "section": "What are Data? (2)",
    "text": "What are Data? (2)\n\n\nData types\n\ncharacter/string: text (qualitative)\nfactors/categories\ntypes of numbers (quantitative)\n\nbinary: 0 or 1, TRUE or FALSE (logical)\nintegers: whole numbers\nreal numbers: decimals/fractions\n\n\ndiscrete vs. continuous\n\ndiscrete: finite set of particular values (0 or 1, scale from 1 to 10)\ncontinuous: real numbers that fall into particular range (e.g., brain activity, visual analoge scale)\n\nWhat data type is eye color?\n\n\n\ndiscrete vs. continuous: question for examples or quiz\nFurther classify data examples mentioned in chat"
  },
  {
    "objectID": "W1_Intro.html#what-is-a-data-set",
    "href": "W1_Intro.html#what-is-a-data-set",
    "title": "01a Introduction to Biostatistics",
    "section": "What is a Data Set?",
    "text": "What is a Data Set?\n\na collection of data\nusually organized into rows and columns (like an excel spreadsheet)\n\nrows: participants/animals/cells…\ncolumns: variables!\n\neach variable contains one type of measurement\n\ntable cells = unique observations of variables per participant etc.\n\n\n\nNHANES dataset\npossibly go through columns and ask for data types?"
  },
  {
    "objectID": "W1_Intro.html#what-makes-a-good-measurement",
    "href": "W1_Intro.html#what-makes-a-good-measurement",
    "title": "01a Introduction to Biostatistics",
    "section": "What Makes a Good Measurement?",
    "text": "What Makes a Good Measurement?\n\n\n\nWhat is being measured?\n\nconstructs vs. proxies: need to be well-defined! (Difficult)\nmeasurement error\n\nrandom: e.g., variation in reaction times of same participant across trials\nsystematic: e.g., miscalibrated eye-tracking device\n\n\nDo we have a “gold standard” to compare the measurement to?\n\n\n\nBreak-Out session: Brainstorm what makes a good vs. bad measurement!\nGroup work/brainstorm:\n\nWhat are problems?\nWhich kind of errors/when is data NOT good\nhow can we minimize error?"
  },
  {
    "objectID": "W1_Intro.html#reliability",
    "href": "W1_Intro.html#reliability",
    "title": "01a Introduction to Biostatistics",
    "section": "Reliability",
    "text": "Reliability\nCorrelation of a measurement with “itself”\n\n\nInternal reliability (consistency)\nTest-retest reliability (stability)\nInter-rater reliability (agreement)\n\n\n\nCorrelation with other variables can’t be higher than reliability (cf., Wilmer et al., 2012, Table 1)!"
  },
  {
    "objectID": "W1_Intro.html#validity",
    "href": "W1_Intro.html#validity",
    "title": "01a Introduction to Biostatistics",
    "section": "Validity",
    "text": "Validity\nAre we measuring the construct we’re interested in?\n\n\nFace validity: Does it intuitively make sense? First reality check!\nConstruct validity\n\nconvergent validity: Related to similar measures that should measure the same construct\ndivergent validity: Is it unrelated to other measures?\n\nPredictive validity: Is it predictive of other outcomes? (e.g., intelligence & job success)\n\n\n\n\n\n\nReliability & Validity"
  },
  {
    "objectID": "W1_Intro.html#summarizing-data",
    "href": "W1_Intro.html#summarizing-data",
    "title": "01a Introduction to Biostatistics",
    "section": "Summarizing Data",
    "text": "Summarizing Data\n\n\nThrowing away (some of the) information!\n\nextract the quintessence of the data (important for forming models)\nmake predictions\n\nCounts, frequencies, percentages, averages"
  },
  {
    "objectID": "W1b_IntroR.html#general-working-with-r-in-this-course",
    "href": "W1b_IntroR.html#general-working-with-r-in-this-course",
    "title": "01b Intro to R",
    "section": "General: Working with R in this course",
    "text": "General: Working with R in this course\nDuring Class\n\nYou should have RStudio open and your Biostats project loaded (we will set up the project today).\nHave the slides open in the background. You will need them to copy R code (top right button on any code chunk) or click on links.\n\n\nprint(\"Hello World\")\n\n[1] \"Hello World\"\n\n\n\nRemember: You can navigate through the slides quickly by clicking on the three dashes in the bottom left.\n\n\n\nAt Home\nIf possible, use two screens with the slides (Zoom) opened on one and RStudio on the other"
  },
  {
    "objectID": "W1b_IntroR.html#why-write-code",
    "href": "W1b_IntroR.html#why-write-code",
    "title": "01b Intro to R",
    "section": "Why write code?",
    "text": "Why write code?\n\n\nDoing statistical calculation by hand? Tedious & error prone! Computer is faster…\nUsing spreadsheets? Limited options, change data accidentally…\nUsing point-and-click software (e.g., SPSS)?\n\nproprietary software = expensive\nR = open, extensible (community)\nreproducible!\n\nScience/Academia is a marathon and not a sprint\n=&gt; it is worthwhile investing in skills with a slow learning curve that will pay off in the long run\n\n\n\nChat: What are advantages (or disadvantages!) of coding?"
  },
  {
    "objectID": "W1b_IntroR.html#install-r-rstudio",
    "href": "W1b_IntroR.html#install-r-rstudio",
    "title": "01b Intro to R",
    "section": "Install R & RStudio",
    "text": "Install R & RStudio\nYou should all have installed R & RStudio by now! Who had problems doing so?"
  },
  {
    "objectID": "W1b_IntroR.html#overview-rstudio",
    "href": "W1b_IntroR.html#overview-rstudio",
    "title": "01b Intro to R",
    "section": "Overview RStudio",
    "text": "Overview RStudio\n\nRStudio Interface\nopen R!"
  },
  {
    "objectID": "W1b_IntroR.html#rstudio-panes",
    "href": "W1b_IntroR.html#rstudio-panes",
    "title": "01b Intro to R",
    "section": "RStudio Panes",
    "text": "RStudio Panes\n\n\n\nScript pane: view, edit, & save your code\nConsole: here the commands are run and rudimentary output may be provided\nEnvironment: which variables/data are available\nFiles, plots, help etc.\n\n\n\n\n\n\n\nRStudio Interface\n\n\n\n\n\nConsole vs. Script (Rmarkdown later)"
  },
  {
    "objectID": "W1b_IntroR.html#using-the-console-as-a-calculator",
    "href": "W1b_IntroR.html#using-the-console-as-a-calculator",
    "title": "01b Intro to R",
    "section": "Using the Console as a Calculator",
    "text": "Using the Console as a Calculator\n\n100 + 1\n\n[1] 101\n\n2*3\n\n[1] 6\n\nsqrt(9)\n\n[1] 3\n\n\n\nConsole used as calculator\ntry it out!\nWe can’t really do much with these values, they will just be written in the console."
  },
  {
    "objectID": "W1b_IntroR.html#saving-the-results-as-a-variableobject",
    "href": "W1b_IntroR.html#saving-the-results-as-a-variableobject",
    "title": "01b Intro to R",
    "section": "Saving the Results as a Variable/Object",
    "text": "Saving the Results as a Variable/Object\n\na &lt;- 100 + 1\n\nmulti &lt;- 2*3\n\nSqrtOfNine &lt;- sqrt(9)\n\nword &lt;- \"Hello\"\n\n\n\n“&lt;-” is used to assign values to variables (“=” is also possible but not common in the R community)\na, multi etc. are the variable names, which can be words, some special characters are allowed but not whitespace\n\nYou can find those now in your Environment! (top right)\nFor saving variables, there is no feedback in the console (2*3 outputs 6 but multi &lt;- 2*3 does not)\n\nas you can see, the variables can contain different types: Numbers, strings/characters (= words) etc.\nthe variables contain the calculated value (i.e. 101) and not the calculation/formula (100+1)\nYou can use those variables for further calculations, e.g., a + multi\n\n\n\nType first command in console, what happens?\nWhy don’t we see anything in the console?\nWhat happens if we type in a in the console?\nIs there anything else that you find interesting?\nWhat is sqrt()?"
  },
  {
    "objectID": "W1b_IntroR.html#functions",
    "href": "W1b_IntroR.html#functions",
    "title": "01b Intro to R",
    "section": "Functions",
    "text": "Functions\nThis code with sqrt(9) looked unfamiliar. sqrt() is an R function that calculates the square root of a number. 9 is the argument that we hand over to the function.\nIf you want to know what a function does, which arguments it takes, or which output it generates, you can type into the console: ?functionname\n\n?sqrt\n\nThis will open the help file in the Help Pane on the lower right of RStudio.\nYou can also click on a function in the script or console pane and press the F1 key.\n\nDo this now! Anything unclear?"
  },
  {
    "objectID": "W1b_IntroR.html#functions-2",
    "href": "W1b_IntroR.html#functions-2",
    "title": "01b Intro to R",
    "section": "Functions 2",
    "text": "Functions 2\nFunctions often take more than one argument:\n\nrnorm(n = 6, mean = 3, sd = 1)\nrnorm(6, 3, 1) #this outputs the same as above\n\n# By the way, # denotes a line-end comment (ignored by R), which are very important for code documentation!\n\nYou can explicitly state which argument you are handing over (check the help file for the argument names!) or just state the values (but these have to be in the correct order then! See help file)."
  },
  {
    "objectID": "W1b_IntroR.html#packages",
    "href": "W1b_IntroR.html#packages",
    "title": "01b Intro to R",
    "section": "Packages",
    "text": "Packages\nThere are a number of functions that are already included with Base R (i.e., R after a new installation), but you can greatly extend the power of R by loading packages (and we will!). Packages are like collections of functions or even data types that someone else wrote.\nOn the top, click on Tools and then Install Packages…. Search for tidyverse and install!\n\n\n\nYou can also download a package using the install.packages() function:\n\ninstall.packages(\"tidyverse\")\n\n(It may be necessary to install Rtools for some packages: https://cran.r-project.org/bin/windows/Rtools/)\n\n\n\n\nBut installing is not enough to be able to actually use the functions from that package directly. Usually, you also want to load the package (i.e., make it directly available) with the library() function. This is the first thing you do on the top of an R script:\n\nlibrary(\"tidyverse\") # or library(tidyverse)\n\n(If you don’t load a package, you have to call functions explicitly by packagename::function)\n\nOpen Source! Anyone can write a package!\nBase R = mobile phone, comes with some functions, packages = apps\npossibly necessary to install Rtools!"
  },
  {
    "objectID": "W1b_IntroR.html#new-project",
    "href": "W1b_IntroR.html#new-project",
    "title": "01b Intro to R",
    "section": "New Project",
    "text": "New Project\nCreate a new project by clicking on “File” in the top left and then “New Project…”\nWe usually want to create a “New Directory” and then choose a standard “New Project” on the top of the list\n(we will only need standard projects during this class)\nChoose the project name, e.g., as “Biostats R” (this will create a folder in which the whole project is living)\nBrowse any kind of path you want to contain your project folder,\ne.g., “D:/Documents/Studies/Translational Neuroscience/Biostats”\n\nNew Project final window"
  },
  {
    "objectID": "W1b_IntroR.html#existing-projects",
    "href": "W1b_IntroR.html#existing-projects",
    "title": "01b Intro to R",
    "section": "Existing Projects",
    "text": "Existing Projects\nYou will find the current project on the top right corner of RStudio\nIf you click on the current project, you can open new projects by choosing “Open Project” and select the .Rproj file of the project.\nYou can also just double click on .Rproj files and RStudio will open with the project loaded.\n\nExisting projects"
  },
  {
    "objectID": "W1b_IntroR.html#using-scripts",
    "href": "W1b_IntroR.html#using-scripts",
    "title": "01b Intro to R",
    "section": "Using Scripts",
    "text": "Using Scripts\nTo open a new script, click File -&gt; New File -&gt; R Script. (Ctrl + Shift + N)\nTo run a line of the script, you can either click Run at the top right of the pane or Ctrl + Enter. It will run the code that is highlighted/selected or automatically select the current line (or the complete multi-line command).\nTo run the whole script/chunk, press Ctrl + Shift + Enter (with full console output) or Ctrl + Shift + S (limited output).\n\n\nUsing scripts"
  },
  {
    "objectID": "W1b_IntroR.html#read-in-data",
    "href": "W1b_IntroR.html#read-in-data",
    "title": "01b Intro to R",
    "section": "Read in Data",
    "text": "Read in Data\nTo read in data files, you need to know which format these files have, e.g. .txt. or .csv files or some other (proprietary) format. There are packages that enable you to read in data of different formats.\nWe will use the files from Fundamentals of Quantitative Analysis: ahi-cesd.csv and participant-info.csv. Save these directly in your project folder on your computer (do not open them!).\n\n\n\nDid you find the files? Here are the direct links:\n\nhttps://psyteachr.github.io/quant-fun-v2/ahi-cesd.csv\nhttps://psyteachr.github.io/quant-fun-v2/participant-info.csv\n\n\n\n\n\nCreate a new script with the following content:\n\n#install.packages(\"tidyverse\") #if you have not yet installed the tidyverse, uncomment and run\nlibrary(tidyverse) # we will use a function from the tidyverse to read in the data\n\ndat &lt;- read_csv(\"ahi-cesd.csv\")\npinfo &lt;- read_csv(\"participant-info.csv\")\n\nRun the code!"
  },
  {
    "objectID": "W1b_IntroR.html#looking-at-the-data",
    "href": "W1b_IntroR.html#looking-at-the-data",
    "title": "01b Intro to R",
    "section": "Looking at the Data",
    "text": "Looking at the Data\n\nThere are several options to get a glimpse at the data:\n\nClick on dat and pinfo in your Environment.\nType View(dat) into the console or into the script pane and run it.\nRun str(dat) or str(pinfo) to get an overview of the data.\nRun summary(dat).\nRun head(dat), print(dat), or even just dat.\nWhat is the difference between these commands?"
  },
  {
    "objectID": "W1b_IntroR.html#looking-at-the-data-2",
    "href": "W1b_IntroR.html#looking-at-the-data-2",
    "title": "01b Intro to R",
    "section": "Looking at the Data 2",
    "text": "Looking at the Data 2\nWhat is the difference to the objects/variables, that you assigned/saved in your Environment earlier and these objects?\n\n\nThe two objects we just read in are data frames, which consist of full datasets. The objects we assigned earlier were simpler variables, which only consisted of single values/words.\nData frames usually have several rows and columns. Remember, the columns are the variables and the rows are the observations."
  },
  {
    "objectID": "W2_Models.html#what-is-a-model",
    "href": "W2_Models.html#what-is-a-model",
    "title": "02 Models",
    "section": "What is a Model?",
    "text": "What is a Model?\n\n“models” are generally simplifications of things in the real world that nonetheless convey the essence of the thing being modeled\nAll models are wrong but some are useful (George Box)\n\n(ST21, Ch 5)\nAim: Find the model that efficiently and accurately summarizes the data.\nBasic structure of statistical models:\n\\[\ndata=model+error\n\\]\n\na statistical model is generally much simpler than the data being described; it is meant to capture the structure of the data as simply as possible.\nTwo parts:\n\none portion that is described by a statistical model, which expresses the values that we expect the data to take given our knowledge,\nerror that reflects the difference between the model’s predictions and the observed data."
  },
  {
    "objectID": "W2_Models.html#statistical-models",
    "href": "W2_Models.html#statistical-models",
    "title": "02 Models",
    "section": "Statistical Models",
    "text": "Statistical Models\nIn general, we want to predict single observations (denoted by i) from the model. The fact that we are looking at predictions of observations and not actual values of the data is denoted by the “hat”:\n\\[\n\\widehat{data_i} = model_i\n\\] The error is then simply the deviation of the actual data from the predicted values:\n\\[\nerror_i = data_i - \\widehat{data_i}\n\\] If this doesn’t make much sense yet, let’s look at an example.\n\nThis means that the predicted value of the data for observation i is equal to the value of the model for that observation."
  },
  {
    "objectID": "W2_Models.html#some-data",
    "href": "W2_Models.html#some-data",
    "title": "02 Models",
    "section": "Some Data",
    "text": "Some Data\nLet’s say we want to have a model of height of children based on the NHANES dataset (also used in ST21). What would you do?\nHere is how (the beginning of) the dataset looks like:"
  },
  {
    "objectID": "W2_Models.html#a-simple-model",
    "href": "W2_Models.html#a-simple-model",
    "title": "02 Models",
    "section": "A Simple Model",
    "text": "A Simple Model\nWhat do you think would be a good model for the height of a person?\n(Or: Which value should we guess for a particular child?)"
  },
  {
    "objectID": "W2_Models.html#a-simple-model-2",
    "href": "W2_Models.html#a-simple-model-2",
    "title": "02 Models",
    "section": "A Simple Model 2",
    "text": "A Simple Model 2\nThe simplest model would be to predict the mean of the height values for every child! This would imply that individual deviations of the mean would be interpreted to be (prediction) errors in such a model.\nWe can write such a simple model as a formula:\n\\[\ny_i = \\beta + \\epsilon\n\\]\n\\(y_i\\) denotes the individual observations (hence the \\(i\\)) of heights, \\(\\beta\\) is a so-called parameter, and \\(\\epsilon\\) is the error term. In this example, the parameter \\(\\beta\\) would be the same value (= the mean height) for everyone (hence it doesn’t need an \\(i\\) subscript). Parameters are values that we optimize to find the best model."
  },
  {
    "objectID": "W2_Models.html#a-simple-model-3",
    "href": "W2_Models.html#a-simple-model-3",
    "title": "02 Models",
    "section": "A Simple Model 3",
    "text": "A Simple Model 3\nHow do we find parameters that belong to the best fitting model?\n\nWe try to minimize the error!\nRemember, the error is the difference between the actual and predicted values of \\(y\\) (height):\n\\[\nerror_i = y_i - \\hat{y_i}\n\\]\nIf we select a predicted value of 400cm, all individuals’ errors would hugely deviate (because no one is 4m tall). If we average these errors, it would still be a big value.\nA better candidate for such a simple model is thus the arithmetic mean or average:\n\\[\n\\bar{X} = \\frac{\\sum_{i=1}^{n}x_i}{n}\n\\]\nSumming up all individual’s heights and dividing that number by the number of individuals gives us the mean. By definition, the average (directed) error is now 0 (see book for proof, the individual errors cancel out)! This means that the average has no bias to over- or underestimate observations (while 4m would have been a clear overestimation)."
  },
  {
    "objectID": "W2_Models.html#a-note-on-errors",
    "href": "W2_Models.html#a-note-on-errors",
    "title": "02 Models",
    "section": "A Note on Errors",
    "text": "A Note on Errors\nWe usually don’t simply average across the individual (signed) errors, but across the squared errors.\nThe reason is that we do not want positive and negative errors to cancel each other out.\nThe mean squared error would be in a different unit than the data (e.g., cm2), which is why we usually take the square root of that value to bring it back to the original unit: This leaves us with the root mean squared error (RMSE)!\nNote: We could also use the absolute values of errors, sum those up, and avoid any of these problems. For historical reasons, we do not."
  },
  {
    "objectID": "W2_Models.html#a-slightly-more-complex-model",
    "href": "W2_Models.html#a-slightly-more-complex-model",
    "title": "02 Models",
    "section": "A Slightly More Complex Model",
    "text": "A Slightly More Complex Model\nObviously, the model for predicting height from the average is not very good (RMSE = 27 cm). How can we improve this model?\n\nWe can account for other information that we might have!\nFor example, to account for age might be a good idea: Older children are likely taller than younger ones. We plot height against age to visually inspect the relationship:\n\n\n\n\n\n\n\n\nRMSE: On average, 27 cm “wrong” per individual!\nA: raw data, visible strong relationship\nB: only age (linear relationship)\nC: intercept/constant\nD: also account for gender\n--&gt; line fits data increasingly better!"
  },
  {
    "objectID": "W2_Models.html#a-slightly-more-complex-model-2",
    "href": "W2_Models.html#a-slightly-more-complex-model-2",
    "title": "02 Models",
    "section": "A Slightly More Complex Model 2",
    "text": "A Slightly More Complex Model 2\nAs we can see, the line (~ model) fits the data points increasingly well, e.g. if we include a constant (also called “intercept”) and age. We would write this as this formula:\n\\[\n\\hat{y_i} = \\hat{\\beta_0} + \\hat{\\beta_1} * age_i\n\\]\nRemember from linear algebra that this defines a line:\n\\[\ny = intercept + slope * x\n\\]\nThus \\(\\beta_0\\) is the parameter for the intercept and \\(\\beta_1\\) for the slope of age!\nThe model fit is now much better: RMSE = 8.36 cm.\n\nAdding gender? Does not improve model too much! (compared to age)\n\nw/o intercept: A, no \\(\\beta_0\\)\nStats Software will estimate best values for \\(\\beta\\)’s"
  },
  {
    "objectID": "W2_Models.html#what-is-a-good-model",
    "href": "W2_Models.html#what-is-a-good-model",
    "title": "02 Models",
    "section": "What is a “Good” Model?",
    "text": "What is a “Good” Model?\nTwo aims:\n\nDescribe data well (= low error/RMSE)\nGeneralize to new data (low error when applied to new data)\n\nCan be conflicting!\n\nWhere does error come from?\n\n\nmeasurement error (noise): random variation in data\n\ndependent variable is hard to measure precisely (difficult/noisy conditions)\ncheap/inadequate equipment for measuring\n\nwrong model specification\n\nimportant variable is missing from model (age!)\ne.g., height has a quadratic relationship with age (old people shrink again)"
  },
  {
    "objectID": "W2_Models.html#examples-measurement-error",
    "href": "W2_Models.html#examples-measurement-error",
    "title": "02 Models",
    "section": "Examples Measurement Error",
    "text": "Examples Measurement Error\n\nSimulated relationship between blood alcohol content and reaction time on a driving test, with best-fitting linear model represented by the line. A: linear relationship with low measurement error. B: linear relationship with higher measurement error. C: Nonlinear relationship with low measurement error and (incorrect) linear model\nA: very little error, all points close to fitted line\nB: same relationship much more variability across individuals\nC. wrongly specified model (caffeine!), not a linear relationship. Error high (deviations points - line)"
  },
  {
    "objectID": "W2_Models.html#can-a-model-be-too-good",
    "href": "W2_Models.html#can-a-model-be-too-good",
    "title": "02 Models",
    "section": "Can a Model be “too Good”?",
    "text": "Can a Model be “too Good”?\nYes! This is called overfitting.\n\nIf we fit a line too closely to the data (e.g., with an 8th degree polynomial), the model might not be able to generalize to other data well.\n\n\n\n\n\nAn example of overfitting. Both datasets were generated using the same model, with different random noise added to generate each set. The left panel shows the data used to fit the model, with a simple linear fit in blue and a complex (8th order polynomial) fit in red. The root mean square error (RMSE) values for each model are shown in the figure; in this case, the complex model has a lower RMSE than the simple model. The right panel shows the second dataset, with the same model overlaid on it and the RMSE values computed using the model obtained from the first dataset. Here we see that the simpler model actually fits the new dataset better than the more complex model, which was overfitted to the first dataset.\n\n\n\n\n\nsame formula, different noise (simulation) ~ different individuals\nsimpler model fits new data better!"
  },
  {
    "objectID": "W2_Models.html#central-tendency",
    "href": "W2_Models.html#central-tendency",
    "title": "02 Models",
    "section": "Central Tendency",
    "text": "Central Tendency\nWhy summarize data?\n\nA summary is a model & describes the data! E.g., mean = central tendency of the data\n\n\nMean, Median, Mode?\n\n\nMean = “Balance point” of data; minimizes sum of squared error, but highly influenced by outliers!\nMedian = “middle” of ranked data; minimizes sum of absolute error, less influenced by extreme values\nMode = most often occurring value (i.e., absolute peak)\n\n\nExample:\nIf 4 people earn 50,000 Euros per year and 1 person earns 1,000,000:\nMean: 240,000 Euros\nMedian: (Rank order: 10,000; 10,000; 10,000; 10,000; 1,000,000) -&gt; middle value = 10,000 Euros\nMode: 10,000 Euros\n\nexamples\nmean: income –&gt; if one person earns a million and 3 only 10.000 –&gt; mean = 257.500"
  },
  {
    "objectID": "W2_Models.html#variability",
    "href": "W2_Models.html#variability",
    "title": "02 Models",
    "section": "Variability",
    "text": "Variability\nHow widespread are the data?\n\nVariance and Standard Deviation\nVariance = Mean Squared Error\n\\[\n\\sigma^2 = \\frac{SSE}{N} = \\frac{\\sum_{i=1}^n (x_i - \\mu)^2}{N}\n\\]\n(Note: \\(x_i\\) = value of ind. observation, \\(\\mu\\) = population mean instead of \\(\\hat{X}\\) = sample mean)\n\n\nStandard Deviation = Root Mean Squared Error\n\\[\nSD = \\sigma = \\sqrt{\\sigma^2}\n\\]\n\n\nWe usually don’t know the population mean \\(\\mu\\), that’s why we estimate the sample variance using the sample mean \\(\\hat{X}\\) (both with the “hat”) and the sample size \\(n\\) instead of the population size \\(N\\):\n\\[\n\\hat\\sigma^2 = \\frac{\\sum_{i=1}^n (x_i - \\hat{X})^2}{n-1}\n\\]\nNote: \\(n-1\\) is used to make the estimate more robust/less biased (I cannot give you an intuition why this is better… please trust me!).\n\nRemember plot above: Points either close to line or wide spread\nVariance = sigma^2, deviations of data points from mean (\\(\\mu\\)) squared and summed, divided by number of oberservations\n\\(n-1\\) = Degrees of Freedom, one value is fixed if we know the mean."
  },
  {
    "objectID": "W2_Models.html#comparing-apples-and-oranges-z-scores",
    "href": "W2_Models.html#comparing-apples-and-oranges-z-scores",
    "title": "02 Models",
    "section": "Comparing apples and oranges: Z-Scores",
    "text": "Comparing apples and oranges: Z-Scores\n\\[\nZ_i(x) = \\frac{x_i - \\mu}{\\sigma}\n\\]\n\n\nstandardizes the distribution: How far is any data point from the mean in units of SD?\ndoesn’t change original relationship of data points!\n\nshifts distribution to have a mean = 0 and scales it to have SD = 1.\n\nuseful if we compare (or use in a model) variables on different scales/units!\n\n\n\n\n\n\n\n\nDensity (top) and cumulative distribution (bottom) of a standard normal distribution, with cutoffs at one standard deviation above/below the mean.\n\n\n\n\n\nZ of x\nx_i is single value/data point\nmu, sigma\nz-scores: Now you can compare apples to oranges! Imagine two siblings from different classes battling it out on test results:\n- I got 70 points, you got only 60!\n- My test only had 70 points in total, yours had 85. I got 86% correct, you only 82%!\n- My test was harder! The average result was 60 with a standard deviation of 10, so I am 1 SD above the class average!\n- On my test, an average of 52 was achieved with an SD of 8, so I am also 1 SD above the class average. But people in your class are really stupid so that’s a very low standard to compare yourself.\nNarrator: And this was the end of comparability between apples and oranges :)"
  },
  {
    "objectID": "W3_DataWranglingR.html#accessing-variablescolumns",
    "href": "W3_DataWranglingR.html#accessing-variablescolumns",
    "title": "03 Data Wrangling",
    "section": "Accessing Variables/Columns",
    "text": "Accessing Variables/Columns\nWhen wrangling your data in R, you often want to access/use different columns, e.g. to calculate new ones. There are a number of ways you can do that:\n\n# create a small data set for this example:\ntestdata &lt;- data.frame(a = c(1, 2, 3),  # c() creates a (column)vector!\n                       b = c(\"a\", \"b\", \"c\"),\n                       c = c(4, 5, 6),\n                       d = c(7, 8, 9),\n                       e = c(10, 11, 12))\n\nprint(testdata)\n\n  a b c d  e\n1 1 a 4 7 10\n2 2 b 5 8 11\n3 3 c 6 9 12\n\nstr(testdata)\n\n'data.frame':   3 obs. of  5 variables:\n $ a: num  1 2 3\n $ b: chr  \"a\" \"b\" \"c\"\n $ c: num  4 5 6\n $ d: num  7 8 9\n $ e: num  10 11 12\n\n\n\ndata.frame() = function to create a data.frame, which is what holds a data set! (tibbles..)\nc() = function to make a vector. A vector is just like one single column of a data frame: It can hold several values, but all of the same type."
  },
  {
    "objectID": "W3_DataWranglingR.html#accessing-variablescolumns-2",
    "href": "W3_DataWranglingR.html#accessing-variablescolumns-2",
    "title": "03 Data Wrangling",
    "section": "Accessing Variables/Columns 2",
    "text": "Accessing Variables/Columns 2\nWhen wrangling your data in R, you often want to access/use different columns, e.g. to calculate new ones. There are a number of ways you can do that:\n\n# in baseR, we access elements of a data.frame with square brackets\ntestdata[1, 2] #get cell that is in first row and second column\n\n[1] \"a\"\n\ntestdata[1:2, 4:5] #use a colon to create ranges of values: first two rows and column numbers 4 and 5\n\n  d  e\n1 7 10\n2 8 11\n\n# we can leave one part empty to select ALL available columns/rows\ntestdata[1:2,] #first two rows, all columns\n\n  a b c d  e\n1 1 a 4 7 10\n2 2 b 5 8 11\n\ntestdata[, 4:5] #columns number 4 and 5, all rows\n\n  d  e\n1 7 10\n2 8 11\n3 9 12\n\n# it is usually better to access columns by their column name:\ntestdata[, c(\"d\", \"e\")] #columns with names \"d\" and \"e\", all rows\n\n  d  e\n1 7 10\n2 8 11\n3 9 12\n\n\n\nsubsetting: rows, columns –&gt; leave empty!\nSelect range!\nUse either name or index of column!"
  },
  {
    "objectID": "W3_DataWranglingR.html#accessing-variablescolumns-3",
    "href": "W3_DataWranglingR.html#accessing-variablescolumns-3",
    "title": "03 Data Wrangling",
    "section": "Accessing Variables/Columns 3",
    "text": "Accessing Variables/Columns 3\nWhen wrangling your data in R, you often want to access/use different columns, e.g. to calculate new ones. There are a number of ways you can do that:\n\n# access a column only:\ntestdata[, \"a\"] #if possible, R will give you just a vector instead of a data.frame\n\n[1] 1 2 3\n\ntestdata$a #short notation to get column \"a\" as a vector\n\n[1] 1 2 3\n\n# tidy versions (see next slides)\n#library(tidyverse) #load tidyverse (if not already done)\npull(testdata, a) #same as testdata$a but can be used better in pipes (see next slide)\n\n[1] 1 2 3\n\nselect(testdata, a) #get column(s) as a data.frame (we usually want this only if we select several columns)\n\n  a\n1 1\n2 2\n3 3\n\n\n\ndata.frame() = function to create a data.frame, which is what holds a data set! (tibbles..)\nc() = function to make a vector. A vector is just like one single column of a data frame: It can hold several values, but all of the same type.\nsubsetting: rows, columns –&gt; leave empty!\nSelect range!\nUse either name or index of column!\nselect –&gt; tidyverse"
  },
  {
    "objectID": "W3_DataWranglingR.html#tidyverse-2",
    "href": "W3_DataWranglingR.html#tidyverse-2",
    "title": "03 Data Wrangling",
    "section": "Tidyverse 2",
    "text": "Tidyverse 2\nBase R:\noutput_data1 &lt;- function1(data)\noutput_data2 &lt;- function2(output_data1, param1)\noutput_data3 &lt;- function3(output_data2, param2, param3)\n\nOr:\noutput_data &lt;- function3(function2(function1(data), param1), param2, param3)\n\n\nTidyverse:\noutput_data &lt;- data %&gt;% function1() %&gt;% function2(param1) %&gt;% function3(param2, param3)\n\n\nYou can insert a pipe %&gt;% (including spaces) by pressing Ctrl + Shift + M\n\n\nBe aware, though, that coding in the tidyverse style is very different than in Base R!\nBase R is more similar to “traditional” programming and other programming languages.\nFor example, you could wrap functions, which would then be carried out from the most nested to the outer function:\noutput_data &lt;- function3(function2(function1(data)))\nfunction1() will be carried out first, followed by function2(), then function3() .\n. . .\nIn the tidyverse, the same would look like this:\noutput_data &lt;- data %&gt;% function1() %&gt;% function2() %&gt;% function(3)\n%&gt;% is called “the pipe” and will “hand over” whatever has been done to the next part. In this example, the data is handed over to function1(), which is then carried out, the result of which is handed over to function2() etc.\nTidyverse style programming is thus a bit easier to read!\nThere’s also the new pipe Base R |&gt;, which is similar to %&gt;%.\n\n\n\n%&gt;% is called the pipe. It takes the output of whatever happens to its left and “hands it over” to the right. There’s also a new base-R-pipe: |&gt;. It is very similar, but sometimes the functionality differs."
  },
  {
    "objectID": "W3_DataWranglingR.html#tidyverse-3",
    "href": "W3_DataWranglingR.html#tidyverse-3",
    "title": "03 Data Wrangling",
    "section": "Tidyverse 3",
    "text": "Tidyverse 3\nlibrary(tidyverse) will load a number of packages, such as dplyr, ggplot2, readr, forcats, tibble etc., which are all usefuls for data wrangling.\nWe will work mainly with functions from the dplyr package, but also use readr to read in data. We will also use ggplot2 to visualize data.\nThe most important dplyr functions for data wrangling are:\n\n\n\n\n\n\n\nFunction\nDescription\n\n\n\n\nselect()\nInclude or exclude certain columns (variables)\n\n\nfilter()\nInclude or exclude certain rows (observations)\n\n\nmutate()\nCreate new columns (variables)\n\n\nsummarize()\nCreate new columns that aggregate data/create summary variables for groups of observations (data frame will become smaller)\n\n\ngroup_by()\nOrganize the rows (observations) into groups\n\n\narrange()\nChange the order of rows (observations)\n\n\n\n\nfunction names very self-explanatory!\nWe don’t create new observations in R - this is job of the data acquisition - we just read the existing data"
  },
  {
    "objectID": "W3_DataWranglingR.html#setting-up-libraries",
    "href": "W3_DataWranglingR.html#setting-up-libraries",
    "title": "03 Data Wrangling",
    "section": "Setting up libraries",
    "text": "Setting up libraries\n\nOpen your Biostats R project.\nCreate a new R script and save it, e.g. as “DataWrangling1.R”.\nInsert code to make sure the packages “tidyverse” and “babynames” are installed and loaded.\n\n\n\n# install.packages(\"tidyverse\")\n# install.packages(\"babynames\")\n\nlibrary(babynames)\nlibrary(tidyverse)\n\n\nload tidyverse last, otherwise functions with same name will be masked from package that is loaded first. Since we often need tidyverse functions, it’s safest to load it last!"
  },
  {
    "objectID": "W3_DataWranglingR.html#look-at-the-data",
    "href": "W3_DataWranglingR.html#look-at-the-data",
    "title": "03 Data Wrangling",
    "section": "Look at the Data",
    "text": "Look at the Data\n\n\nType the word babynames into your console pane and press enter. What kind of information do you get?\n\n“A tibble: 1,924,665 x 5”\n\ntibble is an extension of the data.frame with more convenient output (e.g., values rounded to significant digits)\n~1.9 million rows/observations\n5 columns/variables\n\n\nWhat kind of columns/variables do we have?\n\ndbl = double/numeric (can take decimals)\nchr = character/string (letters or words)\nint = integer (only whole numbers)\n\n\n\n\nask first for 1 and 2"
  },
  {
    "objectID": "W3_DataWranglingR.html#selecting-variables-of-interest",
    "href": "W3_DataWranglingR.html#selecting-variables-of-interest",
    "title": "03 Data Wrangling",
    "section": "Selecting Variables of Interest",
    "text": "Selecting Variables of Interest\nUse select() to choose only the columns year, sex, name, and prop and store it as a new tibble called babynames_reduced.\nRemember that you can run ?select in the console if you need help about, e.g., input/arguments to the function.\n\n\n# my favorite:\nbabynames_reduced &lt;- babynames %&gt;% \n  select(year, sex, name, prop)\n\n# or without the pipe operator:\nbabynames_reduced &lt;- select(.data = babynames, year, sex, name, prop)\n\n# or alternatively:\nbabynames_reduced &lt;- babynames %&gt;% \n  select(-n) # remove columns by using -\n\nRemoving columns vs. selecting columns: Results may change if the data get updated!"
  },
  {
    "objectID": "W3_DataWranglingR.html#arranging-data",
    "href": "W3_DataWranglingR.html#arranging-data",
    "title": "03 Data Wrangling",
    "section": "Arranging Data",
    "text": "Arranging Data\nChange the order of the data (oberservations/rows)!\n\n\nUsing arrange(), try sorting the data according to the names column. What happens?\nHow can you sort a column in a descending fashion? Check out the help file (?arrange).\nLet’s sort by year descendingly and within each year, sort names alphabetically.\n\n\n\n\nsort_asc &lt;- babynames %&gt;% arrange(name)\n\nsort_desc &lt;- babynames %&gt;% arrange(desc(year)) \n\nbabynames %&gt;% arrange(desc(year), name) \n\n# A tibble: 1,924,665 × 5\n    year sex   name          n       prop\n   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;     &lt;int&gt;      &lt;dbl&gt;\n 1  2017 M     Aaban        11 0.0000056 \n 2  2017 F     Aabriella     6 0.0000032 \n 3  2017 M     Aadam        18 0.00000917\n 4  2017 M     Aadan         8 0.00000407\n 5  2017 M     Aadarsh      15 0.00000764\n 6  2017 M     Aaden       240 0.000122  \n 7  2017 M     Aadesh        7 0.00000357\n 8  2017 M     Aadhav       31 0.0000158 \n 9  2017 M     Aadhavan      6 0.00000306\n10  2017 M     Aadhi        10 0.00000509\n# ℹ 1,924,655 more rows\n\n\n\nremember to save data in new tibble/data frame!"
  },
  {
    "objectID": "W3_DataWranglingR.html#filter-observations",
    "href": "W3_DataWranglingR.html#filter-observations",
    "title": "03 Data Wrangling",
    "section": "Filter Observations",
    "text": "Filter Observations\nWe have already used select() to keep only certain variables (columns), but often we also want to keep only certain observations (rows), e.g. babies born in the year 2000 and later.\nWe use the function filter() for this.\n\nLook at the following code and think about what it might do.\n\nbabynames %&gt;% filter(year &gt; 2000)\n\n\n\n\n\n# A tibble: 562,156 × 5\n    year sex   name          n    prop\n   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;     &lt;int&gt;   &lt;dbl&gt;\n 1  2001 F     Emily     25055 0.0127 \n 2  2001 F     Madison   22164 0.0112 \n 3  2001 F     Hannah    20712 0.0105 \n 4  2001 F     Ashley    16526 0.00835\n 5  2001 F     Alexis    16401 0.00828\n 6  2001 F     Sarah     15896 0.00803\n 7  2001 F     Samantha  15862 0.00801\n 8  2001 F     Abigail   14807 0.00748\n 9  2001 F     Elizabeth 14784 0.00747\n10  2001 F     Olivia    13978 0.00706\n# ℹ 562,146 more rows\n\n\nThe data starts at 2001! :("
  },
  {
    "objectID": "W3_DataWranglingR.html#detour-boolean-expressions",
    "href": "W3_DataWranglingR.html#detour-boolean-expressions",
    "title": "03 Data Wrangling",
    "section": "Detour: Boolean Expressions",
    "text": "Detour: Boolean Expressions\nThe second argument, year &gt; 2000, is a Boolean or logical expression, which means that it results in a value of either TRUE or FALSE. filter() runs this expression and then removes all values/rows that contain FALSE.\nThere are also other Boolean expressions:\n\nBoolean expressions\n\n\n\n\n\n\n\nOperator\nName\nis TRUE if and only if\n\n\n\n\nA &lt; B\nless than\nA is less than B\n\n\nA &lt;= B\nless than or equal\nA is less than or equal to B\n\n\nA &gt; B\ngreater than\nA is greater than B\n\n\nA &gt;= B\ngreater than or equal\nA is greater than or equal to B\n\n\nA == B\nequivalence\nA exactly equals B\n\n\nA != B\nnot equal\nA does not exactly equal B\n\n\nA %in% B\nin\nA is an element of vector B\n\n\n\n\n\nA double equality sign == is a comparison, a single equals = is a variable or parameter assignment.\nThis is why R users like to make the distinction even bigger by using &lt;- for variable assignment (your environment in the top right pane) and = for parameter assignment in functions (a hidden so-called local environment only visible to the function)."
  },
  {
    "objectID": "W3_DataWranglingR.html#filter-some-more",
    "href": "W3_DataWranglingR.html#filter-some-more",
    "title": "03 Data Wrangling",
    "section": "Filter some more",
    "text": "Filter some more\n\nKeep only those observations with the name “Mary”.\nDiscard all observations with name “Mary” and keep only those from year &gt; 2000.\nKeep only those with names of former Queens (Mary, Elizabeth, Victoria).\nDiscard the ones with the Queen names!\n\n\nFirst task:\n\nmarys &lt;- babynames %&gt;% filter(name == \"Mary\")\n\n\n\nThe second task might be difficult because you have two expressions, name != \"Mary\" and year &gt; 2000. You can simply add several expressions separated by commas in filter (commas are treated like a “logical and” &):\n\nno_marys_young &lt;- babynames %&gt;% filter(name != \"Mary\", year &gt; 2000)\n\n\n\nThird task:\n\nqueens &lt;- babynames %&gt;% filter(name %in% c(\"Mary\", \"Elizabeth\", \"Victoria\"))\n\n\n\nThe fourth task is tricky! You could use three filters in a row with name != \"Mary\", name != \"Elizabeth\", name != \"Victoria\". Or you could use %in%, but then you can’t use the ! in front of the %in%. The trick is to negate the whole expression with == FALSE:\n\nno_queens &lt;- babynames %&gt;% filter(name %in% c(\"Mary\", \"Elizabeth\", \"Victoria\") == FALSE)"
  },
  {
    "objectID": "W3_DataWranglingR.html#your-first-plot",
    "href": "W3_DataWranglingR.html#your-first-plot",
    "title": "03 Data Wrangling",
    "section": "Your First Plot",
    "text": "Your First Plot\nIn your script, insert and run the following code:\n\ndat &lt;- babynames %&gt;% \n  filter(sex==\"F\", #only female babies\n         name %in% c(\"Emily\", \"Kathleen\", \"Alexandra\", \"Beverly\")) #reduce to these 4 names\n\ndat %&gt;% ggplot(aes(x = year, y = prop, colour = name)) +\n  geom_line(size=2) #plot data as a line (with increased size)\n\n\n\nAlter the code to check for male babies with the same names (change sex==\"F\" to sex==\"M\").\nOptional: Plot the absolute number n instead of the relative proportion prop."
  },
  {
    "objectID": "W3_DataWranglingR.html#create-new-variables",
    "href": "W3_DataWranglingR.html#create-new-variables",
    "title": "03 Data Wrangling",
    "section": "Create New Variables",
    "text": "Create New Variables\nIf we want to create variables that do not exist yet (i.e. by calculating values, combining other variables, etc.), we can use mutate()!\n\nAdd a variable called “country” that contains the value “USA” for all observations\n\n\n\nbaby_where &lt;- babynames %&gt;% mutate(country = \"USA\")\n\n\n\nBut mutate is much more powerful and can create variables that differ per observation, depending on other values in the tibble/data frame:\n\nCreate a variable that denotes the decade a baby was born:\n\n\n\n\n#we can only use floor to round down to full numbers =&gt; divide year by 10, floor it, and then multiply by 10 again\nbaby_decades &lt;- babynames %&gt;% mutate(decade = floor(year/10) *10) #round(year, -1) works but not floor(year, -1) :(\n\n\n\n# A tibble: 10 × 2\n    year decade\n   &lt;dbl&gt;  &lt;dbl&gt;\n 1  1884   1880\n 2  1911   1910\n 3  1921   1920\n 4  1923   1920\n 5  1945   1940\n 6  1949   1940\n 7  1965   1960\n 8  1972   1970\n 9  1983   1980\n10  2016   2010"
  },
  {
    "objectID": "W3_DataWranglingR.html#summarizing",
    "href": "W3_DataWranglingR.html#summarizing",
    "title": "03 Data Wrangling",
    "section": "Summarizing",
    "text": "Summarizing\nThe goal of data wrangling is often to summarize (or aggregate) the data, e.g. to have an average value per condition. Sometimes you’d also want to calculate descriptive statistics to report.\n\nYou can do so using the function summarise():\n\n# run the filter function just like above again:\ndat &lt;- babynames %&gt;% \n  filter(name %in% c(\"Emily\", \"Kathleen\", \"Alexandra\", \"Beverly\"), \n         sex == \"F\")\n\n# summarize the data, calculating the number of oberservations:\ndat_sum &lt;- dat %&gt;% summarise(total = sum(n))\ndat_sum\n\n# A tibble: 1 × 1\n    total\n    &lt;int&gt;\n1 2161374\n\n\nAs you can see, a new variable named total is created, which contains the total number of observations (in this case, it is different from the number of rows because each row already contains a count n).\nThere’s just one row in the resulting data frame, because summarise() reduces the data frame (to only include the necessary information)!"
  },
  {
    "objectID": "W3_DataWranglingR.html#grouping-and-summarizing",
    "href": "W3_DataWranglingR.html#grouping-and-summarizing",
    "title": "03 Data Wrangling",
    "section": "Grouping and Summarizing",
    "text": "Grouping and Summarizing\nOften, we want to summarize data for specific subgroups. For this aim, summarise() has the .by parameter:\n\ngroup_sum &lt;- dat %&gt;% summarise(total = sum(n), .by=name) \ngroup_sum\n\n# A tibble: 4 × 2\n  name       total\n  &lt;chr&gt;      &lt;int&gt;\n1 Emily     841491\n2 Kathleen  711605\n3 Beverly   376914\n4 Alexandra 231364\n\n\n\nYou can also subgroup by a combination of variables:\n\nbabynames %&gt;% filter(name %in% c(\"Emily\", \"Kathleen\", \"Alexandra\", \"Beverly\")) %&gt;% #we start with the 4 names regardless of sex\n  summarise(total = sum(n), .by=c(name, sex)) #and then summarise by name, separated for sex\n\n# A tibble: 8 × 3\n  name      sex    total\n  &lt;chr&gt;     &lt;chr&gt;  &lt;int&gt;\n1 Emily     F     841491\n2 Kathleen  F     711605\n3 Beverly   M       4633\n4 Beverly   F     376914\n5 Alexandra F     231364\n6 Emily     M       1744\n7 Kathleen  M       1692\n8 Alexandra M        859"
  },
  {
    "objectID": "W3_DataWranglingR.html#grouping-and-summarizing-2",
    "href": "W3_DataWranglingR.html#grouping-and-summarizing-2",
    "title": "03 Data Wrangling",
    "section": "Grouping and Summarizing 2",
    "text": "Grouping and Summarizing 2\nIn earlier versions, we had to use summarise() together with group_by():\n\ngroup_sum &lt;- dat %&gt;% group_by(name) %&gt;% summarise(total = sum(n)) \ngroup_sum\n\n# A tibble: 4 × 2\n  name       total\n  &lt;chr&gt;      &lt;int&gt;\n1 Alexandra 231364\n2 Beverly   376914\n3 Emily     841491\n4 Kathleen  711605\n\n\nWe avoid using group_by() because it can have unintended side effects.\nIt is just part of this class because you will likely encounter it in somebody else’s (old) code.\n\nIf you do have to use it, make sure to ungroup() after summarise() (or mutate()) to avoid unintended effects:\n\ngroup_sum &lt;- dat %&gt;% group_by(name) %&gt;% summarise(total = sum(n)) %&gt;% ungroup()"
  },
  {
    "objectID": "W3_DataWranglingR.html#grouping-and-summarizing-3",
    "href": "W3_DataWranglingR.html#grouping-and-summarizing-3",
    "title": "03 Data Wrangling",
    "section": "Grouping and Summarizing 3",
    "text": "Grouping and Summarizing 3\nUse the baby_decades data frame to calculate the mean and median number of observations, grouped by sex & decade.\n\n\nbaby_decades %&gt;% summarise(mean_year = mean(n),\n                           median_year = median(n),\n                           .by=c(sex, decade))\n\n# A tibble: 28 × 4\n   sex   decade mean_year median_year\n   &lt;chr&gt;  &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;\n 1 F       1880     111.           13\n 2 M       1880     101.           12\n 3 F       1890     128.           13\n 4 M       1890      93.6          12\n 5 F       1900     131.           12\n 6 M       1900      94.4          12\n 7 F       1910     187.           12\n 8 M       1910     181.           12\n 9 F       1920     211.           12\n10 M       1920     227.           13\n# ℹ 18 more rows"
  },
  {
    "objectID": "W3_DataWranglingR.html#counting-data",
    "href": "W3_DataWranglingR.html#counting-data",
    "title": "03 Data Wrangling",
    "section": "Counting Data",
    "text": "Counting Data\nThere are several ways to get the number of rows per group. You can use the function n() within a call to summarise() (or mutate()). A shortcut is to use count():\n\ndat %&gt;% summarise(n = n(), .by=name)\n\n# A tibble: 4 × 2\n  name          n\n  &lt;chr&gt;     &lt;int&gt;\n1 Emily       138\n2 Kathleen    138\n3 Beverly     122\n4 Alexandra   117\n\ndat %&gt;% count(name)\n\n# A tibble: 4 × 2\n  name          n\n  &lt;chr&gt;     &lt;int&gt;\n1 Alexandra   117\n2 Beverly     122\n3 Emily       138\n4 Kathleen    138\n\n\nInterestingly, the order of the output may vary. summarise() leaves the data in the original order (i.e., by prop, which (likely) translates to an order by n()). count() arranges the output by the variables for which the counting is done (here: alphabetically by name)."
  },
  {
    "objectID": "W3_DataWranglingR.html#bigger-pipes",
    "href": "W3_DataWranglingR.html#bigger-pipes",
    "title": "03 Data Wrangling",
    "section": "Bigger Pipes!",
    "text": "Bigger Pipes!\nSo far we have often saved intermediate steps in tibbles and used those as input for the next function. With the pipe, we can chain several functions and save relevant results only, no need for crowding the environment with intermediate data.frames or tibbles!\n\npipe_summary &lt;- babynames %&gt;%\n  mutate(decade = floor(year/10) *10) %&gt;%\n  filter(name %in% c(\"Emily\", \"Kathleen\", \"Alexandra\", \"Beverly\"), \n         sex==\"F\") %&gt;%\n  summarise(mean_decade = mean(n), .by=c(name, decade))\n\nIt’s not easy to decide which intermediate steps to save and which not. Usually, it involves some sort of trial and error. Sometimes you go back and break a pipe apart. Sometimes you get overwhelmed by the number of variables in your environment and create bigger pipes.\nAs a rule of thumb: If an intermediate step is only used once, you should probably delete it (unless it makes the code easier to comprehend)."
  },
  {
    "objectID": "W3_DataWranglingR.html#tidy-data",
    "href": "W3_DataWranglingR.html#tidy-data",
    "title": "03 Data Wrangling",
    "section": "Tidy Data",
    "text": "Tidy Data\nTidy data: Data that is easily processed by tidyverse functions (also for visualizations and statistical analyses).\nThree principles:\n\nEach variable has its own column.\nEach observation has its own row.\nEach value has its own cell.\n\n\nWide vs. long format data?\n\n\nWide format: Each participant/animal has one row;\nrepeated observations are in several columns\n\n\n\nID\nTime_1\nTime_2\n\n\n\n\na1\n230\n310\n\n\na2\n195\n220\n\n\na3\n245\n290\n\n\n\n\nLong format: Each observation has its own row;\nthere are (usually) several rows per participant\n\n\n\nID\nTime\nValue\n\n\n\n\na1\n1\n230\n\n\na1\n2\n310\n\n\na2\n1\n195\n\n\na3\n2\n220\n\n\na3\n1\n245\n\n\na3\n2\n290\n\n\n\n\n\n\n\nWide format implements a sparser representation of the data but less tidy!\nIf you want to convert Time from milliseconds into seconds, what do you have to do in both formats?\n\nData often does not come in this format but is rather messy! That’s why we wrangle.\nTidy data is in between wide and long (you can always go longer! :D)"
  },
  {
    "objectID": "W3_DataWranglingR.html#tidy-data-2",
    "href": "W3_DataWranglingR.html#tidy-data-2",
    "title": "03 Data Wrangling",
    "section": "Tidy Data 2",
    "text": "Tidy Data 2\nWhat do you think, which of the following data sets is tidy?\n\n\n\n\n# A tibble: 12 × 4\n   country      year type            count\n   &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt;\n 1 Afghanistan  1999 cases             745\n 2 Afghanistan  1999 population   19987071\n 3 Afghanistan  2000 cases            2666\n 4 Afghanistan  2000 population   20595360\n 5 Brazil       1999 cases           37737\n 6 Brazil       1999 population  172006362\n 7 Brazil       2000 cases           80488\n 8 Brazil       2000 population  174504898\n 9 China        1999 cases          212258\n10 China        1999 population 1272915272\n11 China        2000 cases          213766\n12 China        2000 population 1280428583\n\n\n\n\n\n# A tibble: 6 × 4\n  country      year  cases population\n  &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n1 Afghanistan  1999    745   19987071\n2 Afghanistan  2000   2666   20595360\n3 Brazil       1999  37737  172006362\n4 Brazil       2000  80488  174504898\n5 China        1999 212258 1272915272\n6 China        2000 213766 1280428583\n\n\n\n\n\n\n\n\n# A tibble: 6 × 3\n  country      year rate             \n  &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;            \n1 Afghanistan  1999 745/19987071     \n2 Afghanistan  2000 2666/20595360    \n3 Brazil       1999 37737/172006362  \n4 Brazil       2000 80488/174504898  \n5 China        1999 212258/1272915272\n6 China        2000 213766/1280428583\n\n\n\n\n\n# A tibble: 3 × 5\n  country     `1999_cases` `2000_cases` `1999_population` `2000_population`\n  &lt;chr&gt;              &lt;dbl&gt;        &lt;dbl&gt;             &lt;dbl&gt;             &lt;dbl&gt;\n1 Afghanistan          745         2666          19987071          20595360\n2 Brazil             37737        80488         172006362         174504898\n3 China             212258       213766        1272915272        1280428583\n\n\n\n\n\nTable1 is tidy! (upper)"
  },
  {
    "objectID": "W3_DataWranglingR.html#analyzing-the-autism-spectrum-quotient",
    "href": "W3_DataWranglingR.html#analyzing-the-autism-spectrum-quotient",
    "title": "03 Data Wrangling",
    "section": "Analyzing the Autism Spectrum Quotient",
    "text": "Analyzing the Autism Spectrum Quotient\nFor the following activities, we will need the following files:\n\nresponses.csv containing the AQ survey responses to each of the 10 questions for the 66 participants\nqformats.csv containing information on how a question should be coded - i.e. forward or reverse coding\nscoring.csv containing information on how many points a specific response should get; depending on whether it is forward or reverse coded\npinfo.csv containing participant information such as Age, Sex and importantly ID number."
  },
  {
    "objectID": "W3_DataWranglingR.html#set-up",
    "href": "W3_DataWranglingR.html#set-up",
    "title": "03 Data Wrangling",
    "section": "Set Up",
    "text": "Set Up\nFor the following activities, we will need the following files:\n\nresponses.csv containing the AQ survey responses to each of the 10 questions for the 66 participants\nqformats.csv containing information on how a question should be coded - i.e. forward or reverse coding\nscoring.csv containing information on how many points a specific response should get; depending on whether it is forward or reverse coded\npinfo.csv containing participant information such as Age, Sex and importantly ID number.\n\n\nCreate a new script, e.g. as “DataWrangling3.R” (remember we skipped #2 in the book).\nDownload the data into your project folder.\nOptional: Clear your environment (the brush in the top right pane) and/or restart the R session (Session -&gt; Restart R)."
  },
  {
    "objectID": "W3_DataWranglingR.html#load-the-data",
    "href": "W3_DataWranglingR.html#load-the-data",
    "title": "03 Data Wrangling",
    "section": "Load the Data",
    "text": "Load the Data\nFor the following activities, we will need the following files:\n\nresponses.csv containing the AQ survey responses to each of the 10 questions for the 66 participants\nqformats.csv containing information on how a question should be coded - i.e. forward or reverse coding\nscoring.csv containing information on how many points a specific response should get; depending on whether it is forward or reverse coded\npinfo.csv containing participant information such as Age, Sex and importantly ID number.\n\n\nLoad the four .csv files into your environment, e.g.:\n\n\nresponses &lt;- read_csv(\"responses.csv\") \nqformats &lt;- read_csv(\"qformats.csv\")\nscoring &lt;- read_csv(\"scoring.csv\")\npinfo &lt;- read_csv(\"pinfo.csv\")"
  },
  {
    "objectID": "W3_DataWranglingR.html#look-at-the-data-1",
    "href": "W3_DataWranglingR.html#look-at-the-data-1",
    "title": "03 Data Wrangling",
    "section": "Look at the Data",
    "text": "Look at the Data\nIs the data (responses) in a tidy format?\n\n\n# A tibble: 6 × 11\n     Id Q1                 Q2    Q3    Q4    Q5    Q6    Q7    Q8    Q9    Q10  \n  &lt;dbl&gt; &lt;chr&gt;              &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;\n1    16 Slightly Disagree  Defi… Slig… Defi… Slig… Slig… Slig… Defi… Slig… Slig…\n2    17 Definitely Agree   Slig… Slig… Defi… Defi… Defi… Slig… Slig… Slig… Slig…\n3    18 Definitely Agree   Defi… Slig… Defi… Defi… Defi… Slig… Defi… Defi… Defi…\n4    19 Definitely Agree   Defi… Defi… Slig… Defi… Defi… Slig… Slig… Defi… Slig…\n5    20 Definitely Disagr… Slig… Defi… Slig… Slig… Slig… Slig… Slig… Slig… Slig…\n6    21 Slightly Disagree  Slig… Defi… Slig… Slig… Slig… Defi… Defi… Slig… Slig…\n\n\n\nWhy is it not tidy?\n\nwide format"
  },
  {
    "objectID": "W3_DataWranglingR.html#reformatting-the-data",
    "href": "W3_DataWranglingR.html#reformatting-the-data",
    "title": "03 Data Wrangling",
    "section": "Reformatting the Data",
    "text": "Reformatting the Data\nLet’s bring the wide data in a longer, tidy format!\n\nThere are several functions in R to reformat data, but the newest ones are pivot_longer() and pivot_wider().\nRun the code and see what changes:\n\nrlong &lt;- responses %&gt;% pivot_longer(cols = Q1:Q10, #we can select a range of column names\n                                    #cols = starts_with(\"Q\"), #alternative\n                                    names_to = \"Question\", \n                                    values_to = \"Response\")\n\n\n\nDescribe what the function does, what does the input/the arguments mean?"
  },
  {
    "objectID": "W3_DataWranglingR.html#joining-the-data",
    "href": "W3_DataWranglingR.html#joining-the-data",
    "title": "03 Data Wrangling",
    "section": "Joining the Data",
    "text": "Joining the Data\nWe now want to combine the different data sets: We want to have the information how the questionnaire has to be scored included with the items.\nWe can find the scoring information (i.e. how the questions are framed, positive or negative/whether they need to be reversed) in the qformats tibble. Furthermore, we can find how many points are given to each item/response in scoring.\nWe can use the function inner_join() to merge the tibbles into one bigger tibble.\n\nActivity: Replace the NULL values in the below code with the necessary variable names to join rlong and qformats by Question.\n\nrlong2 &lt;- inner_join(x = NULL, \n                     y = NULL, \n                     by = \"NULL\")\n\n\n\n\nrlong2 &lt;- inner_join(x = rlong, \n                     y = qformats, \n                     by = \"Question\")\n\n\nDescribe what happened?\nwhat is forward and reverse scoring?"
  },
  {
    "objectID": "W3_DataWranglingR.html#combining-more-data",
    "href": "W3_DataWranglingR.html#combining-more-data",
    "title": "03 Data Wrangling",
    "section": "Combining more Data",
    "text": "Combining more Data\nYou can only join two data frames/tibbles at once.\nNow add the scoring data:\n\nrscores &lt;- rlong2 %&gt;% inner_join(scoring, \n                                 c(\"QFormat\", \"Response\"))"
  },
  {
    "objectID": "W3_DataWranglingR.html#calculate-the-questionnaire-scores",
    "href": "W3_DataWranglingR.html#calculate-the-questionnaire-scores",
    "title": "03 Data Wrangling",
    "section": "Calculate the Questionnaire Scores",
    "text": "Calculate the Questionnaire Scores\nHow do we need to group and summarize the data to get a sum score per person? (Ignoring the reverse coding for now!) Add the correct column names instead of the NULL.\n\naq_scores &lt;- rscores %&gt;% summarise(AQ = sum(NULL), .by=NULL)\n\n\n\naq_scores &lt;- rscores %&gt;% summarise(AQ = sum(Score), .by=Id) # sum column Score to obtain AQ scores."
  },
  {
    "objectID": "W3_DataWranglingR.html#pipe-it-all-together",
    "href": "W3_DataWranglingR.html#pipe-it-all-together",
    "title": "03 Data Wrangling",
    "section": "Pipe it all together!",
    "text": "Pipe it all together!\n\naq_scores2 &lt;- responses %&gt;% \n  pivot_longer(cols = Q1:Q10,\n               names_to = \"Question\", \n               values_to = \"Response\") %&gt;%  \n  inner_join(qformats, \"Question\") %&gt;% \n  inner_join(scoring, c(\"QFormat\", \"Response\")) %&gt;% \n  summarise(AQ = sum(Score), .by=Id)"
  },
  {
    "objectID": "W3_DataWranglingR.html#background",
    "href": "W3_DataWranglingR.html#background",
    "title": "03 Data Wrangling",
    "section": "Background",
    "text": "Background\nWe’ll use data from a paper that investigates whether the ability to perform an action influences perception. In particular, the authors wondered whether participants who played Pong would perceive the ball to move faster when they have a small paddle.\n\n\nDownload the data, create a new script.\nClear the environment if you prefer.\nLook at the data."
  },
  {
    "objectID": "W3_DataWranglingR.html#solutions",
    "href": "W3_DataWranglingR.html#solutions",
    "title": "03 Data Wrangling",
    "section": "Solutions",
    "text": "Solutions\n\nlibrary(\"tidyverse\")\npong_data &lt;- read_csv(\"Data/PongBlueRedBack 1-16 Codebook.csv\")\nsummary(pong_data)\n\n  Participant     JudgedSpeed      PaddleLength   BallSpeed    TrialNumber    \n Min.   : 1.00   Min.   :0.0000   Min.   : 50   Min.   :2.0   Min.   :  1.00  \n 1st Qu.: 4.75   1st Qu.:0.0000   1st Qu.: 50   1st Qu.:3.0   1st Qu.: 72.75  \n Median : 8.50   Median :1.0000   Median :150   Median :4.5   Median :144.50  \n Mean   : 8.50   Mean   :0.5471   Mean   :150   Mean   :4.5   Mean   :144.50  \n 3rd Qu.:12.25   3rd Qu.:1.0000   3rd Qu.:250   3rd Qu.:6.0   3rd Qu.:216.25  \n Max.   :16.00   Max.   :1.0000   Max.   :250   Max.   :7.0   Max.   :288.00  \n BackgroundColor      HitOrMiss       BlockNumber   \n Length:4608        Min.   :0.0000   Min.   : 1.00  \n Class :character   1st Qu.:0.0000   1st Qu.: 3.75  \n Mode  :character   Median :1.0000   Median : 6.50  \n                    Mean   :0.6866   Mean   : 6.50  \n                    3rd Qu.:1.0000   3rd Qu.: 9.25  \n                    Max.   :1.0000   Max.   :12.00  \n\n# look at the data (can also use summary(), str(), head() etc.)\nglimpse(pong_data)\n\nRows: 4,608\nColumns: 8\n$ Participant     &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ JudgedSpeed     &lt;dbl&gt; 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, …\n$ PaddleLength    &lt;dbl&gt; 50, 250, 50, 250, 250, 50, 250, 50, 250, 50, 50, 250, …\n$ BallSpeed       &lt;dbl&gt; 5, 3, 4, 3, 7, 5, 6, 2, 4, 4, 7, 7, 3, 6, 5, 7, 2, 5, …\n$ TrialNumber     &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,…\n$ BackgroundColor &lt;chr&gt; \"red\", \"blue\", \"red\", \"red\", \"blue\", \"blue\", \"red\", \"r…\n$ HitOrMiss       &lt;dbl&gt; 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, …\n$ BlockNumber     &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …"
  },
  {
    "objectID": "W3_DataWranglingR.html#solutions-2",
    "href": "W3_DataWranglingR.html#solutions-2",
    "title": "03 Data Wrangling",
    "section": "Solutions 2",
    "text": "Solutions 2\n\nnew_pong_data &lt;- pong_data %&gt;% \n  select(BallSpeed, HitOrMiss, JudgedSpeed, Participant, \n         TrialNumber) %&gt;% \n  arrange(desc(HitOrMiss), desc(JudgedSpeed)) %&gt;% \n  filter(JudgedSpeed == 1,\n         BallSpeed %in% c(\"2\", \"4\", \"5\", \"7\"),\n         HitOrMiss == 0) %&gt;% \n  filter(TrialNumber &gt; 2) %&gt;% \n  mutate(TrialNumber = TrialNumber -1) \n  \n  # summarize (use old data frame because we removed variables)\npong_data_hits &lt;- pong_data %&gt;% \n  summarise(total_hits = sum(HitOrMiss, na.rm = TRUE),\n            meanhits = mean(HitOrMiss, na.rm = TRUE),\n            .by=c(BackgroundColor, PaddleLength))"
  },
  {
    "objectID": "W4_DataVizR.html#ggplot",
    "href": "W4_DataVizR.html#ggplot",
    "title": "04 Data Visualization",
    "section": "ggplot",
    "text": "ggplot\nWe will use a package called ggplot2 (which is part of the tidyverse). ggplot2 is a very versatile package and allows us to make beautiful figure, which are immediately ready for publication.\nThe main function to “start” plotting is ggplot() - we will then add layers of data and layers to tweak the appearance.\n\nLayers of a ggplot"
  },
  {
    "objectID": "W4_DataVizR.html#activity-1-set-up",
    "href": "W4_DataVizR.html#activity-1-set-up",
    "title": "04 Data Visualization",
    "section": "Activity 1: Set Up",
    "text": "Activity 1: Set Up\n\nOpen RStudio and load your Biostats R project. Create a new script called DataVisualisation1.R.\nMake sure you have the following two files downloaded into your project folder (we already used them in Intro to R presentation): ahi-cesd.csv and participant-info.csv.\nCopy and run the code below to load the tidyverse package and the data files:\n\n\nlibrary(tidyverse) \n\ndat &lt;- read_csv(\"ahi-cesd.csv\")\npinfo &lt;- read_csv(\"participant-info.csv\")\n\n\nRun the following code to combine both files and select our variables of interest:\n\n\nall_dat &lt;- dat %&gt;% inner_join(pinfo, \n                              by=c(\"id\", \"intervention\")) %&gt;% \n  arrange(id, occasion) #joining messes up the order of the data frame =&gt; arrange again\n\n#we throw out several variables even though they would be important for a comprehensive data analysis\nsummarydata &lt;- all_dat %&gt;% select(id, ahiTotal, cesdTotal, #ID & questionnaire scores\n                                  sex, age, educ, income) #demographic variables\n\n\nwhat happens in the code chunk?"
  },
  {
    "objectID": "W4_DataVizR.html#look-at-the-data",
    "href": "W4_DataVizR.html#look-at-the-data",
    "title": "04 Data Visualization",
    "section": "Look at the Data",
    "text": "Look at the Data\nHave a look at the types of data:\n\nglimpse(summarydata)\n\nRows: 992\nColumns: 7\n$ id        &lt;dbl&gt; 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5, 5, 6, …\n$ ahiTotal  &lt;dbl&gt; 63, 73, 73, 89, 89, 93, 80, 77, 77, 85, 60, 67, 56, 61, 41, …\n$ cesdTotal &lt;dbl&gt; 14, 6, 7, 10, 13, 8, 15, 12, 3, 5, 31, 31, 41, 35, 27, 32, 2…\n$ sex       &lt;dbl&gt; 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 1, …\n$ age       &lt;dbl&gt; 35, 35, 59, 59, 59, 59, 59, 59, 51, 51, 50, 50, 50, 50, 58, …\n$ educ      &lt;dbl&gt; 5, 5, 1, 1, 1, 1, 1, 1, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, …\n$ income    &lt;dbl&gt; 3, 3, 1, 1, 1, 1, 1, 1, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, …\n\n\nWhat do you see?\n\nAll variables are loaded as numeric. However, are all of those numeric?\n\n\nsex, educ and income don’t seem to really be numbers but factors with individual categories (factor levels)!\nWe should convert these data to factor. Checking and adjusting data types (as part of data wrangling) will be important for plotting and analyzing the data, you might otherwise get strange/wrong results!"
  },
  {
    "objectID": "W4_DataVizR.html#activity-2-transform-data-type",
    "href": "W4_DataVizR.html#activity-2-transform-data-type",
    "title": "04 Data Visualization",
    "section": "Activity 2: Transform Data Type",
    "text": "Activity 2: Transform Data Type\nCopy and run the below code to change the categories to factors.\n\nSo for example, the 1s in sex change to categorical factors instead of numerical 1s.\nIf you mutate a new column with the same name as the old one, it will overwrite the column.\nYou can read each line of the mutate as “overwrite the data that is in that column with the same values now considered factors and not doubles”\n\n\nsummarydata1 &lt;- summarydata %&gt;%\n  mutate(sex = as_factor(sex),\n         educ = as_factor(educ),\n         income = as_factor(income))\n\nglimpse(summarydata1)\n\nRows: 992\nColumns: 7\n$ id        &lt;dbl&gt; 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5, 5, 6, …\n$ ahiTotal  &lt;dbl&gt; 63, 73, 73, 89, 89, 93, 80, 77, 77, 85, 60, 67, 56, 61, 41, …\n$ cesdTotal &lt;dbl&gt; 14, 6, 7, 10, 13, 8, 15, 12, 3, 5, 31, 31, 41, 35, 27, 32, 2…\n$ sex       &lt;fct&gt; 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 1, …\n$ age       &lt;dbl&gt; 35, 35, 59, 59, 59, 59, 59, 59, 51, 51, 50, 50, 50, 50, 58, …\n$ educ      &lt;fct&gt; 5, 5, 1, 1, 1, 1, 1, 1, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, …\n$ income    &lt;fct&gt; 3, 3, 1, 1, 1, 1, 1, 1, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, …\n\nsummarydata1 %&gt;% pull(educ) %&gt;% unique()\n\n[1] 5 1 4 2 3\nLevels: 1 2 3 4 5"
  },
  {
    "objectID": "W4_DataVizR.html#transform-data-type-2",
    "href": "W4_DataVizR.html#transform-data-type-2",
    "title": "04 Data Visualization",
    "section": "Transform Data Type 2",
    "text": "Transform Data Type 2\nA simple change to a factor is not always helpful. We still don’t know what a 1 in sex or a 5 in educ stands for:\n\n\nsex: 1 = female, 2 = male\neduc: 1 = no graduation, 2 = school graduation, 3 = vocational training, 4 = bachelor’s degree, 5 = post graduate\nincome: 1 = low, 2 = middle, 3 = high\n\n\nsummarydata2 = summarydata1 %&gt;% mutate(sex = sex %&gt;% recode(\"1\" = \"female\", \"2\" = \"male\"),\n                                       educ = educ %&gt;% recode(\"1\" = \"no graduation\",\n                                                              \"2\" = \"school graduation\",\n                                                              \"3\" = \"vocational training\",\n                                                              \"4\" = \"bachelor's degree\",\n                                                              \"5\" = \"post grad\"),\n                                       income = income %&gt;% recode(\"1\" = \"low\",\n                                                                  \"2\" = \"middle\",\n                                                                  \"3\" = \"high\"))\n\n#glimpse(summarydata2)\nsummarydata2 %&gt;% pull(educ) %&gt;% unique()\n\n[1] post grad           no graduation       bachelor's degree  \n[4] school graduation   vocational training\n5 Levels: no graduation school graduation ... post grad\n\n\n\nThere is very sparse information on the variables at https://doi.org/10.5334/jopd.35, so I guesstimated some of the factor levels."
  },
  {
    "objectID": "W4_DataVizR.html#transform-data-type-3",
    "href": "W4_DataVizR.html#transform-data-type-3",
    "title": "04 Data Visualization",
    "section": "Transform Data Type 3",
    "text": "Transform Data Type 3\nrecode() only works with data that are already factors. What if we start with the original data (i.e., summarydata instead of summarydata1)?\n\n# no factors yet =&gt; refer to original data as numbers WITHOUT quotation marks\nsummarydata3 = summarydata %&gt;% mutate(sex = if_else(sex==1, \"female\", \"male\") %&gt;% as_factor(), #could also use case_match\n                       educ = educ %&gt;% case_match(1 ~ \"no graduation\",\n                                                  2 ~ \"school graduation\",\n                                                  3 ~ \"vocational training\",\n                                                  4 ~ \"bachelor's degree\",\n                                                  5 ~ \"post grad\") %&gt;% as_factor(),\n                       income = income %&gt;% case_match(1 ~ \"low\", 2 ~ \"middle\", 3 ~ \"high\") %&gt;% as_factor())\n\n#glimpse(summarydata3)\nsummarydata3 %&gt;% pull(educ) %&gt;% unique()\n\n[1] post grad           no graduation       bachelor's degree  \n[4] school graduation   vocational training\n5 Levels: post grad no graduation bachelor's degree ... vocational training\n\n\n\nFactor is now ordered by occurrence in data! :(\n\n\n\nIf data are factors coded as numbers, do as_factor() first and then recode()\nIf data are factors coded as characters, make sure that the ordering is correct (use arrange() for alphabetical order) or create a new factor() with explicit ordering: income %&gt;% factor(levels=c(\"low\", \"middle\", \"high\")))"
  },
  {
    "objectID": "W4_DataVizR.html#the-first-layer",
    "href": "W4_DataVizR.html#the-first-layer",
    "title": "04 Data Visualization",
    "section": "The First Layer",
    "text": "The First Layer\n\nThe first line (or layer) sets up the base of the graph: the data to use and the aesthetics (what will go on the x and y axis, how the plot will be grouped).\naes() can take both an x and y argument, however, with a bar plot you are just asking R to plot the number of data points in each group onto the y-axis, so you do not specify y here.\n\n\n\nggplot(summarydata1, aes(x = sex))"
  },
  {
    "objectID": "W4_DataVizR.html#the-second-layer",
    "href": "W4_DataVizR.html#the-second-layer",
    "title": "04 Data Visualization",
    "section": "The Second Layer",
    "text": "The Second Layer\nThe next layer adds a geom or a shape. In this case we use geom_bar() as we want to draw a bar plot.\n\nNote that we are adding layers, using a + between layers. This is a very important difference between pipes and visualization.\n\n\n\nggplot(summarydata1, aes(x = sex)) +\n  geom_bar()"
  },
  {
    "objectID": "W4_DataVizR.html#the-second-layer-with-color",
    "href": "W4_DataVizR.html#the-second-layer-with-color",
    "title": "04 Data Visualization",
    "section": "The Second Layer with color",
    "text": "The Second Layer with color\n\nAdding fill to the first layer will separate the data into each level of the grouping variable and give it a different color. In this case, there is a different colored bar for each level of sex.\nWe can get rid of the (in this case redundant legend) with show.legend = FALSE.\n\n\n\nggplot(summarydata1, aes(x = sex, fill = sex)) +\n  geom_bar() #geom_bar(show.legend = FALSE)"
  },
  {
    "objectID": "W4_DataVizR.html#the-next-layers---improving-the-plot",
    "href": "W4_DataVizR.html#the-next-layers---improving-the-plot",
    "title": "04 Data Visualization",
    "section": "The Next Layers - Improving the Plot",
    "text": "The Next Layers - Improving the Plot\nWe might want to make the plot a bit prettier and easier to read. What would you improve?\n\nWe might want to add better axis labels and change the colors of the bars. We can do so with the functions scale_x_discrete() and scale_y_continuous(), which will adjust the x and y axes.\nWe will use these two arguments in those functions:\n\nname controls/overwrites the axis name (e.g. Groups)\nlabels controls the break points on the axis, i.e. what are the conditions called? The order is important here!\n\n\n\nggplot(summarydata1, aes(x = sex, fill = sex)) +\n  geom_bar(show.legend = FALSE) +\n  scale_x_discrete(name = \"Participant Sex\", \n                   labels = c(\"Female\", \"Male\")) + #if we didn't set factor labels, we can do here manually (not recommended!)\n  scale_y_continuous(name = \"Number of participants\")\n\n\n\n\n\n\n\nThere’s also the counterparts scale_x_continuous() and scale_y_discrete(). What do you think, why do we use the ones mentioned above and when would we use the other ones?"
  },
  {
    "objectID": "W4_DataVizR.html#themes-changing-the-appearance",
    "href": "W4_DataVizR.html#themes-changing-the-appearance",
    "title": "04 Data Visualization",
    "section": "Themes: Changing the Appearance",
    "text": "Themes: Changing the Appearance\nThere are a number of built-in themes that you can use to change the appearance (background, whether axes are shown etc.), but you can also tweak the themes further manually.\nWe will now change the default theme to theme_minimal(), but you can also try other themes (just type “theme_” and see what the autocomplete brings up).\n\n\nggplot(summarydata2, aes(x = sex, fill = sex)) + #now with summarydata2\n  geom_bar(show.legend = FALSE) +\n  scale_x_discrete(name = \"Participant Sex\") + #no need to set labels\n  scale_y_continuous(name = \"Number of participants\") +\n  theme_minimal()"
  },
  {
    "objectID": "W4_DataVizR.html#colors",
    "href": "W4_DataVizR.html#colors",
    "title": "04 Data Visualization",
    "section": "Colors",
    "text": "Colors\nThere are various ways to change the colors of the bars. You can manually indicate the colors you want to use but you can also easily use pre-determined color palettes that are already checked for color-blind friendliness.\nA popular palette is viridis. We can simply add a function/layer to your ggplot named scale_fill_viridis_d() (d for discrete). The function has an option parameter that takes 5 different values (A - E).\n\nType and run the below code into a new code chunk. Try changing the option to either A, B, C or D and see which one you like!\n\n\n\nggplot(summarydata1, aes(x = sex, fill = sex)) +\n  geom_bar(show.legend = FALSE) +\n  scale_x_discrete(name = \"Participant Sex\", \n                   labels = c(\"Female\", \"Male\")) +\n  scale_y_continuous(name = \"Number of participants\") +\n  theme_minimal() +\n  scale_fill_viridis_d(option = \"E\")"
  },
  {
    "objectID": "W4_DataVizR.html#transparency",
    "href": "W4_DataVizR.html#transparency",
    "title": "04 Data Visualization",
    "section": "Transparency",
    "text": "Transparency\nYou can also add transparency to your plot, which can be helpful if you plot several layers of data.\nTo do so, you can simply add alpha to the geom_bar():\n\n\nggplot(summarydata1, aes(x = sex, fill = sex)) +\n  geom_bar(show.legend = FALSE, \n           alpha = .8) +\n  scale_x_discrete(name = \"Participant Sex\", \n                   labels = c(\"Female\", \"Male\")) +\n  scale_y_continuous(name = \"Number of participants\") +\n  theme_minimal() +\n  scale_fill_viridis_d(option = \"E\")"
  },
  {
    "objectID": "W4_DataVizR.html#grouped-plots",
    "href": "W4_DataVizR.html#grouped-plots",
    "title": "04 Data Visualization",
    "section": "Grouped Plots",
    "text": "Grouped Plots\nLet’s go back to the bar plot (but works similarly for other plots as well): Imagine that you have several factors that you want to use to group your data, such as gender and income. In this case, you could use a grouped bar plot:\n\n\nggplot(summarydata1, aes(x = sex, fill = income)) +\n  geom_bar(position = \"dodge\", #prevents \"stacked\" barplots\n           show.legend = TRUE, \n           alpha = .8) +\n  scale_x_discrete(name = \"Participant Sex\", \n                   labels = c(\"Female\", \"Male\")) +\n  scale_y_continuous(name = \"Number of participants\") +\n  theme_minimal() +\n  scale_fill_viridis_d(option = \"E\")\n\n\n\n\n\n\n\nWithout position = dodge, you would get a stacked barplot"
  },
  {
    "objectID": "W4_DataVizR.html#facetting",
    "href": "W4_DataVizR.html#facetting",
    "title": "04 Data Visualization",
    "section": "Facetting",
    "text": "Facetting\nYou could also use facets to divide your data visualizations into several subplots: facet_wrap for one variable.\n\nggplot(summarydata2, aes(x = sex, fill = sex)) +\n  geom_bar(show.legend = FALSE, \n           alpha = .8) +\n  scale_x_discrete(name = \"Participant Sex\") +\n  scale_y_continuous(name = \"Number of participants\") +\n  theme_minimal() +\n  scale_fill_viridis_d(option = \"E\")  +\n  facet_wrap(vars(income)) #in this function, you need to use vars() around variable names\n\n\n\nWhat is problematic here? We needed percentages for females and males for a better comparison"
  },
  {
    "objectID": "W4_DataVizR.html#facetting-2",
    "href": "W4_DataVizR.html#facetting-2",
    "title": "04 Data Visualization",
    "section": "Facetting 2",
    "text": "Facetting 2\nYou could also use facets to divide your data visualizations into several subplots: facet_grid for a matrix of (combinations of) two variables.\n\nggplot(summarydata2, aes(x = sex, fill = sex)) +\n  geom_bar(show.legend = FALSE, \n           alpha = .8) +\n  scale_x_discrete(name = \"Participant Sex\") +\n  scale_y_continuous(name = \"Number of participants\") +\n  theme_minimal() +\n  scale_fill_viridis_d(option = \"E\")  +\n  facet_grid(rows=vars(income), \n             cols=vars(educ),\n             labeller = \"label_both\") #this adds the variable name into the facet legends"
  },
  {
    "objectID": "W4_DataVizR.html#violin-boxplot",
    "href": "W4_DataVizR.html#violin-boxplot",
    "title": "04 Data Visualization",
    "section": "Violin-Boxplot",
    "text": "Violin-Boxplot\nLet’s look at the code. How does the code differ from the one for the barplot above?\n\n\nggplot(summarydata1, aes(x = income, \n                        y = ahiTotal, #new variable!\n                        fill = income)) +\n  geom_violin(trim = FALSE, #smooth on edges\n              show.legend = FALSE, \n              alpha = .4) +\n  geom_boxplot(width = .2, #small boxplot contained in violin\n               show.legend = FALSE, \n               alpha = .7)+\n  scale_x_discrete(name = \"Income\",\n                   labels = c(\"Below Average\", \n                              \"Average\", \n                              \"Above Average\")) +\n  scale_y_continuous(name = \"Authentic Happiness Inventory Score\")+\n  theme_minimal() +\n  scale_fill_viridis_d()\n\n\n\n\n\n\n\nIn this case, not the count on the y-axis, but another cont. variable!"
  },
  {
    "objectID": "W4_DataVizR.html#layer-order",
    "href": "W4_DataVizR.html#layer-order",
    "title": "04 Data Visualization",
    "section": "Layer Order",
    "text": "Layer Order\nThe order of layers is crucial, as the plot will be built up in that order (later layers on top):\n\n\n\nggplot(summarydata1, aes(x = income, y = ahiTotal)) +\n  geom_violin() +\n  geom_boxplot()\n\n\n\n\n\n\nggplot(summarydata1, aes(x = income, y = ahiTotal)) +\n  geom_boxplot() +\n  geom_violin()"
  },
  {
    "objectID": "W4_DataVizR.html#scatterplot",
    "href": "W4_DataVizR.html#scatterplot",
    "title": "04 Data Visualization",
    "section": "Scatterplot",
    "text": "Scatterplot\nIf we have continuous data of two variables, we often want to make a scatter plot:\n\n\nggplot(summarydata1, aes(x = age, y = cesdTotal)) +\n  geom_point() +\n  geom_smooth(method=lm) # if you don't want the shaded CI, add se = FALSE to this"
  },
  {
    "objectID": "W4_DataVizR.html#saving-your-figures",
    "href": "W4_DataVizR.html#saving-your-figures",
    "title": "04 Data Visualization",
    "section": "Saving your Figures",
    "text": "Saving your Figures\nYou can use ggsave() to save your plots. If you don’t tell ggsave() which plot you want to save, by default it will save the last plot you created.\nYou just have to enter the name of the file to be saved (in your working directory) like this:\n\nggsave(\"violin-boxplot.png\")\n\nCheck whether indeed the last plot was saved!\n\n\n\nYou can also specify the dimensions of your plot to be saved:\n\nggsave(\"violin-boxplot.png\",\n       width = 8.5, #width of a typical page in inches (according to APA format)\n       height = 8.5 / sqrt(2), #golden ratio :)\n       units = \"in\")\n\nor\n\nggsave(\"violin-boxplot.png\",\n       width = 1920,\n       height = 1080,\n       units = \"px\") #full HD picture in pixels: 1920 x 1080"
  },
  {
    "objectID": "W4_DataVizR.html#saving-your-figures-2",
    "href": "W4_DataVizR.html#saving-your-figures-2",
    "title": "04 Data Visualization",
    "section": "Saving your Figures 2",
    "text": "Saving your Figures 2\nYou can also assign the plot to a variable in your environment (just like we did with the tibbles previously) and then tell ggsave() which object to save. This is a bit safer.\nRun the code for the violin-boxplot again and save the plot in an object called viobox. You’d then have to explicitly tell ggsave() to save the object viobox:\n\nviobox &lt;- summarydata1 %&gt;%\n  ggplot(aes(x = income,\n             y = ahiTotal,\n             fill = income)) +\n  geom_violin(trim = FALSE, \n              show.legend = FALSE, \n              alpha = .4) +\n  geom_boxplot(width = .2, \n               show.legend = FALSE, \n               alpha = .7) +\n  scale_x_discrete(name = \"Income\",\n                   labels = c(\"Below Average\", \n                              \"Average\", \n                              \"Above Average\")) +\n  scale_y_continuous(name = \"Authentic Happiness Inventory Score\")+\n  theme_minimal() +\n  scale_fill_viridis_d()\n\n\nggsave(\"violin-boxplot-stored.png\", plot = viobox)\n\n\nDo not add ggsave() to the plot with a +. Instead run it on a separate line!\nIf plot is assigned to object, it won’t be displayed unless you type viobox in the console!"
  },
  {
    "objectID": "W6_ProbabilityR.html#the-uniform-distribution",
    "href": "W6_ProbabilityR.html#the-uniform-distribution",
    "title": "06 Probability & Sampling in R",
    "section": "The Uniform Distribution",
    "text": "The Uniform Distribution\nIn a uniform distribution, each possible outcome has an equal chance of occurring.\n\nIf we have a hat with 12 paper slips with names, each name has an equal chance (\\(p = \\frac{1}{12} \\approx .08\\)) of being drawn:"
  },
  {
    "objectID": "W6_ProbabilityR.html#the-binomial-distribution",
    "href": "W6_ProbabilityR.html#the-binomial-distribution",
    "title": "06 Probability & Sampling in R",
    "section": "The Binomial Distribution",
    "text": "The Binomial Distribution\nThe binomial (“two categories”) distribution is used for discrete data with two possible outcomes (e.g., flipping a coin). It models the number of successes being observed (e.g., heads), given the probability of success (0.5 for fair coins) and the number of observations (flips of a coin, e.g., 10).\n\nHow many heads (successes) should we expect and with what probability?\nWe can simulate 10 coin flips (or dice) each 10.000 times and count the number of heads (out of the 10). We can use this distribution to work out the probability of different outcomes, e.g., getting at least 3 heads (or 6s) out of 10 tosses (dice rolls).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsuccess: whatever you want, also an “artificial” dichotomization like “roll a 6” vs. “roll no 6”\nadd up probabilities &gt;=3 or 1 - P(X &lt;= 2)"
  },
  {
    "objectID": "W6_ProbabilityR.html#the-normal-distribution",
    "href": "W6_ProbabilityR.html#the-normal-distribution",
    "title": "06 Probability & Sampling in R",
    "section": "The Normal Distribution",
    "text": "The Normal Distribution\nThe normal distribution is very common in statistics (i.e., in the real world). It (roughly) reflects the probability of any value occurring for a continuous variable, such as height.\n\nNormal distribution of height\nThe normal distribution is always symmetrical\n=&gt; equal probability of observations above and below the mean.\n=&gt; the mean, median, and mode are all equal!"
  },
  {
    "objectID": "W6_ProbabilityR.html#normal-distribution-2",
    "href": "W6_ProbabilityR.html#normal-distribution-2",
    "title": "06 Probability & Sampling in R",
    "section": "Normal Distribution 2",
    "text": "Normal Distribution 2\nWe can also use simulations to approximate a normal distribution. This simulation shows that increasing the sample size will get closer to a normal distribution:\n\nA simulation of an experiment collecting height data from 5000 participants."
  },
  {
    "objectID": "W6_ProbabilityR.html#using-the-uniform-distribution",
    "href": "W6_ProbabilityR.html#using-the-uniform-distribution",
    "title": "06 Probability & Sampling in R",
    "section": "Using the Uniform Distribution",
    "text": "Using the Uniform Distribution\nWe can use runif for a uniform distribution of continuous (numerical) values or\nsample for a uniform distribution of discrete (numerical or text) values.\n\nDraw a random decimal number between 0 and 1\nDraw 100 random decimal numbers between 0 and 10 and calculate their mean.\nDraw 50 random integer numbers between 1 and 10 and calculate their mean.\nDraw who may have the first turn during board game night: Anna, Barbara, Christina, or Dana.\nProduce a random order of all the names of Anna, Barbara, Christina, and Dana.\n\n\n\n#1\nrunif(n=1) #default parameters are already set to min=0 and max=1\n\n[1] 0.4226502\n\n#2\nrunif(n=100, max=10) %&gt;% mean()\n\n[1] 4.641486\n\n#3\nsample(1:10, size=50, replace=T) %&gt;% mean()\n\n[1] 5.78\n\n#4\nsample(c(\"Anna\", \"Barbara\", \"Christina\", \"Dana\"), size=1)\n\n[1] \"Christina\"\n\n#5\nsample(c(\"Anna\", \"Barbara\", \"Christina\", \"Dana\")) #by default, sample draws all elements in random order\n\n[1] \"Dana\"      \"Christina\" \"Anna\"      \"Barbara\""
  },
  {
    "objectID": "W6_ProbabilityR.html#using-the-binomial-distribution",
    "href": "W6_ProbabilityR.html#using-the-binomial-distribution",
    "title": "06 Probability & Sampling in R",
    "section": "Using the Binomial Distribution",
    "text": "Using the Binomial Distribution\n\n\n\n\ndbinom(): the density distribution gives you the probability of exactly \\(x_i\\) successes given the number of trials and the probability of success on a single trial: \\(P(X = x_i)\\) (e.g., what’s the probability of flipping 8/10 heads with a fair coin?)\n\n\n\n\npbinom(): the (cumulative) probability distribution gives you the cumulative probability of getting a maximum of \\(x_i\\) successes: \\(P(X \\le x_i)\\) (e.g., getting 5 or fewer heads out of 10 flips).\nqbinom(): the quantile function gives you the number of success (x-axis) corresponding to a given cumulative probability: \\(X(P = P_i)\\) (e.g., A maximum of how many heads do you have to expect if you want at least an event probability of 25%?). This is the inverse function of pbinom().\n\nNote: Be aware of the difference between success probability \\(p\\), the probability of the density distribution \\(P(X = x_i)\\), and the probability of the cumulative probability distribution \\(P(X \\le x_i)\\)\n\n\n\n\n\n\n\n\n\n\nWhat’s the probability of getting exactly 5 heads on 10 flips?\nWhat’s the probability of getting 0 to 2 heads on 10 flips?\nWhat’s the probability of getting at least 8 heads on 10 flips?"
  },
  {
    "objectID": "W6_ProbabilityR.html#using-the-binomial-distribution-2",
    "href": "W6_ProbabilityR.html#using-the-binomial-distribution-2",
    "title": "06 Probability & Sampling in R",
    "section": "Using the Binomial Distribution 2",
    "text": "Using the Binomial Distribution 2\nNow in R:\n\nWhat’s the probability of getting exactly 5 heads on 10 flips?\nWhat’s the probability of getting 0 to 2 heads on 10 flips?\nWhat’s the probability of getting at least 8 heads on 10 flips?\n\n\n\n#1\ndbinom(x = 5, size = 10, prob = 0.5)\n\n[1] 0.2460938\n\n#2 (indirect way)\ndbinom(x = 0:2, size=10, prob=0.5) %&gt;% sum()\n\n[1] 0.0546875\n\n#2 (direct way)\npbinom(q = 2, size = 10, prob = 0.5) #P(X &lt;= q)\n\n[1] 0.0546875\n\n#3 (careful: minus 1)\npbinom(q = 8-1, size = 10, prob = 0.5, lower.tail = F) #P(X &gt; q) = 1 - P(X &lt;= q)\n\n[1] 0.0546875\n\n\n\n\nNote: The binomial distribution is only symmetrical for \\(p = 50\\%\\).\nThat’s why “a maximum of 2 heads” (0, 1, or 2) has the same probability as “at least 8 heads” (8, 9, or 10).\n\n\nx: the number of ‘heads’ of which we want to know the probability.\nsize: the number of trials (flips) we are simulating; in this case, 10 flips.\nprob: the probability of ‘heads’ on one trial. 50% far a fair coin."
  },
  {
    "objectID": "W6_ProbabilityR.html#using-the-binomial-distribution-3",
    "href": "W6_ProbabilityR.html#using-the-binomial-distribution-3",
    "title": "06 Probability & Sampling in R",
    "section": "Using the Binomial Distribution 3",
    "text": "Using the Binomial Distribution 3\nNow let’s use the quantile function qbinom()!\nImagine a friend wants to bet you on tossing a coin and bets that it will be tails. You suspect she has a coin that is not fair and it will be more likely that it turns up as tails.\nYour friend agrees that you can toss the coin 10 times to test it before you have to give your bet.\nyou want to find out whether the coin is fair and ask yourself: What is the minimum number of heads that is acceptable if it is fair?\nWe choose a probability so low that it is unlikely to get a result lower than that if the coin was fair. We will use a typical value for statistical significance: 0.05 (or in 5% of cases we will find a lower value by chance if the coin was fair).\n\n\nqbinom(p = .05, size = 10, prob = .5)\n\n[1] 2\n\n\nIn this case, the probability of success is now called prob because p is now used for the probability cut-off we want to set. This is unnecessarily confusing :(\n\n\nIf we run the code, we get a value of 2. This means that 2 heads is the first time that the cumulative probability function reaches at least 5% . Thus, if we get less than two heads out of the ten tosses, we could conclude that the coin is likely biased (there is only a 5% chance that this would happen if the coin was fair). You would probably not bet with your friend!\n\n\nNote: This is a bad way to test the fairness of a coin (it’s just for illustration of qbinom)! Rather demand that you win at tails (if the coin is fair, your friend shouldn’t mind)."
  },
  {
    "objectID": "W6_ProbabilityR.html#using-the-normal-distribution",
    "href": "W6_ProbabilityR.html#using-the-normal-distribution",
    "title": "06 Probability & Sampling in R",
    "section": "Using the Normal Distribution",
    "text": "Using the Normal Distribution\nFor every probability distribution, R provides similar functions starting with d, p, or q. For the normal distribution, those are:\n\n\ndnorm(): density function, for calculating the density (not probability!) of a specific value. Only useful for plotting.\npnorm(): cumulative probability or distribution function, for calculating the probability of getting at least (or at most) a specific value.\nqnorm(): quantile function, for calculating the specific value associated with a given cumulative probability.\n\n\n\nFor the next activities, we will use means and SDs of height from the Scottish Health Survey (2008)."
  },
  {
    "objectID": "W6_ProbabilityR.html#using-the-normal-distribution-2",
    "href": "W6_ProbabilityR.html#using-the-normal-distribution-2",
    "title": "06 Probability & Sampling in R",
    "section": "Using the Normal Distribution 2",
    "text": "Using the Normal Distribution 2\nCalculate the probability of meeting a Scottish woman who is as tall or taller than the average Scottish man.\nWhich function would you use?\nHint: You want to know how likely it is that you get a value as extreme or extremer than a specific value (the average height of men) within the distribution for women.\nWhat we know:\n\naverage female height = 163.8, SD = 6.931\naverage male height = 176.2, SD = 6.748\nWe want to know “at least as tall as”\n\n\nWe need the pnorm() function. Fill in the values instead of the NULLs.\n\npnorm(q = NULL, mean = NULL, sd = NULL, lower.tail = NULL)\n\n\n\n\npnorm(q = 176.2, mean = 163.8, sd = 6.931, lower.tail = FALSE)\n\n[1] 0.03680228\n\n\n\n\nNote: For continuous distributions, we don’t have to distinguish between “taller than” and “at least as tall”. \\(P(X \\le x_i) = P(X &lt; x_i)\\)! (Because the probability of one specific value like 176.2000000 m is exactly 0)\n\n\nAlso note: The SD of the distribution of males is not relevant for this question.\n\nDon’t do this:\n\npnorm(q = 176.2, mean = 163.8, sd = 6.931, lower.tail = F) + \n  dnorm(x = 176.2, mean = 163.8, sd = 6.931)\n\n[1] 0.04841892\n\n\nYou cannot add a probability and a density together! Just pnorm(q = 176.2, mean = 163.8, sd = 6.931, lower.tail = F) is the correct answer!\nWhen we say “What’s the probability of a woman being 1.80 m tall?”, we actually mean: “What’s the probability of a person being between 1.795 and 1.805 m tall?” (since height is usually measured within steps of 1 cm). We can calculate this, e.g. for Scottish women, using: pnorm(q = 180 + c(-.5, .5), mean = 163.8, sd = 6.931) %&gt;% diff() = 0.3%"
  },
  {
    "objectID": "W6_ProbabilityR.html#using-the-normal-distribution-3",
    "href": "W6_ProbabilityR.html#using-the-normal-distribution-3",
    "title": "06 Probability & Sampling in R",
    "section": "Using the Normal Distribution 3",
    "text": "Using the Normal Distribution 3\nFiona is a very tall Scottish woman (181.12 cm) who will only date men who are as tall or taller than her. What is the probability of Fiona finding a taller man?\nWhat we know:\n\naverage female height = 163.8, SD = 6.931\naverage male height = 176.2, SD = 6.748\nWe want to know “tall or taller”\nFiona’s height = 181.12\n\n\n\npnorm(q = 181.12, mean = 176.2, sd = 6.748, lower.tail = FALSE)\n\n[1] 0.2329687\n\n\n\n\nConclusion: Fiona shrinks her dating pool to roughly a quarter and may thus consider abandoning beauty standards imposed by society. (Same advice for men, of course)\n\n\n\n\nHow tall would a Scottish man have to be in order to be in the tallest 5% of the height distribution for Scottish men?\n\n\n\nqnorm(p = .05, mean = 176.2, sd = 6.748, lower.tail = FALSE)\n\n[1] 187.2995"
  },
  {
    "objectID": "W6_ProbabilityR.html#simulate-different-distributions",
    "href": "W6_ProbabilityR.html#simulate-different-distributions",
    "title": "06 Probability & Sampling in R",
    "section": "Simulate Different Distributions",
    "text": "Simulate Different Distributions\nIt is possible to draw data from different distributions:\n\n\nnsamples &lt;- 10000\nnhistbins &lt;- 100\n\n# uniform distribution\np1 &lt;- tibble(x = runif(nsamples)) %&gt;% \n  ggplot((aes(x))) + geom_histogram(bins = nhistbins) + \n  labs(title = \"Uniform\")\n\n# binomial distribution\np2 &lt;- tibble(x = rbinom(nsamples, 20, 0.25)) %&gt;% \n  ggplot(aes(x)) + geom_histogram(bins = nhistbins) +\n  labs(title = \"Binomial (p=0.25, 20 trials)\")\n\n# normal distribution\np3 &lt;- tibble(x = rnorm(nsamples)) %&gt;% \n  ggplot(aes(x)) + geom_histogram(bins = nhistbins) +\n  labs(title = \"Normal\")\n\n# Chi-squared distribution\np4 &lt;- tibble(x = rchisq(nsamples, df=1)) %&gt;% \n  ggplot(aes(x)) + geom_histogram(bins = nhistbins) +\n  labs(title = \"Chi-squared\")\n\ncowplot::plot_grid(p1, p2, p3, p4, ncol = 1)"
  },
  {
    "objectID": "W6_ProbabilityR.html#simulate-a-fake-dataset",
    "href": "W6_ProbabilityR.html#simulate-a-fake-dataset",
    "title": "06 Probability & Sampling in R",
    "section": "Simulate a Fake Dataset",
    "text": "Simulate a Fake Dataset\nLet’s simulate data of 120 participants of different heights and genders flipping a coin. To do so, we need to know how to simulate a) heights, b) flip a coin, and c) assign genders.\n\nSimulate heights:\n\nheights &lt;- rnorm(120, mean = 170, sd = 10)\n\n\n\nSimulate coin flips:\n(Instead of drawing from a probability distribution function that starts with an r, e.g. rnorm(), we use sample(), which randomly (uniformly) draws from values you determine. We need to set replace=TRUE)\n\ncoin_flips &lt;- sample(c(\"Head\", \"Tail\"), 120, replace=TRUE)\n\n\n\nWe could of course use similar code to simulate gender or the like.\nBut if we want to predetermine which genders we want to collect data from (i.e. same number in each gender group), we can also use the function rep(). This function will simply repeat an observation for a number of rows:\n\ngenders &lt;- rep(x = c(\"man\", \"woman\", \"nonbinary\"), each = 40) #%&gt;% sample() #you can run just \"sample\" on a vector to randomize its order\n\n\n\nDetermine participant numbers and combining everything into one dataframe:\n\nsim_data &lt;- tibble(\n  participant_number = 1:120,\n  gender = rep(x = c(\"man\", \"woman\", \"nonbinary\"), each = 40),\n  height = rnorm(120, mean = 170, sd = 10),\n  coin_flip = sample(c(\"Head\", \"Tail\"), 120, replace=TRUE)\n)\n\n# or if you have run all the code before, you could also use the objects you have already in your Environment:\n# sim_data &lt;- tibble(participant_number = 1:120, genders, heights, coin_flips)\n\n\nBecause the values of heights and coin flips are random, the assignment of"
  },
  {
    "objectID": "W6_ProbabilityR.html#resampling",
    "href": "W6_ProbabilityR.html#resampling",
    "title": "06 Probability & Sampling in R",
    "section": "Resampling",
    "text": "Resampling\nIf we wanted to do a Monte Carlo Simulation, we could use the code from the last slide and put the simulation into a for loop. Inside the for loop, we would also calculate some value of interest (one estimate per subsample, such as the mean)."
  },
  {
    "objectID": "W6_ProbabilityR.html#resampling-2",
    "href": "W6_ProbabilityR.html#resampling-2",
    "title": "06 Probability & Sampling in R",
    "section": "Resampling 2",
    "text": "Resampling 2\nBut for now, let’s look at the Bootstrap. Remember that we usually use real data for bootstrapping and draw samples with replacement of the same size as the original dataset. We can use this to quantify uncertainty, such as with the Standard Error of the Mean (SEM) or Confidence Intervals.\nLet’s say we have a very small sample of “sweets consumed”. We don’t know the underlying distribution. We make up a dataset, but let’s pretend it is our real data:\n\ndata_10 &lt;- tibble(\n  participant = 1:10,\n  sweets = c(5, 5, 5, 7, 3, 3, 4, 6, 8, 4)\n)\n\n\nWe want to know how sample size influences the SEM, so we also have a sample with twice 10x as many observations:\n\ndata_20 &lt;- tibble(\n  participant = 1:100,\n  sweets = rpois(100,4) #random Poisson distribution\n)"
  },
  {
    "objectID": "W6_ProbabilityR.html#resampling-3",
    "href": "W6_ProbabilityR.html#resampling-3",
    "title": "06 Probability & Sampling in R",
    "section": "Resampling 3",
    "text": "Resampling 3\nLet’s resample 1000 times. To get the sampling distributions of the means, we have to save each mean of each iteration:\n\nset.seed(8465123)\niterations &lt;- 1000 \n\n# initialize empty matrices\nmean_bootstrap_10 &lt;- matrix(NA, nrow = iterations, ncol = 1)\nmean_bootstrap_20 &lt;- matrix(NA, nrow = iterations, ncol = 1)\n\nfor (i in 1:iterations) {\n  # draw exactly the same amount of datapoints as in the original dataset, but with replacement\n  bootstrap_data_10 &lt;- sample_n(data_10, 10, replace=TRUE)\n  bootstrap_data_20 &lt;- sample_n(data_20, 20, replace=TRUE)\n  \n  # calculate the mean for each of the subsamples, put it into matrix in subsequent rows\n  mean_bootstrap_10[i,1] &lt;- mean(bootstrap_data_10$sweets)\n  mean_bootstrap_20[i,1] &lt;- mean(bootstrap_data_20$sweets)\n}\n\n# calculate the SEMs\nsd(mean_bootstrap_10)\n\n[1] 0.4948013\n\nsd(mean_bootstrap_20)\n\n[1] 0.4646242\n\n\nWe can conclude that the SEM is smaller with a larger sample size, which means we can be more certain about our estimate! (We already knew this from the \\(\\sqrt{n}\\) in the denominator of the SE formular but it’s nice to see it confirmed.)"
  },
  {
    "objectID": "W7_Hypothesis.html#null-hypothesis-significance-testing",
    "href": "W7_Hypothesis.html#null-hypothesis-significance-testing",
    "title": "07 Hypothesis Testing",
    "section": "Null Hypothesis Significance Testing",
    "text": "Null Hypothesis Significance Testing\nExample: We have two groups (treatment and control). We also have a hypothesis: The treatment group has lower scores on measure X (e.g., symptoms). We have the data (X for both groups), now what?\n\n\nWe take the hypothesis (treatment = lower X than control) and negate it (Treatment not lower/equal X compared to control). This is our null hypothesis.\nThen we look at the data and determine how likely they would be if the null hypothesis were true.\nI.e., we want to know the conditional probability: \\(P(Data|H_0)\\)\nIf the data are very unlikely we reject the null hypothesis in favor of the alternative hypothesis (our hypothesis).\n(If the data are not very unlikely, we stick with - or fail to reject - the null hypothesis.)\n\n\n\nHow would you compare the two groups? Calculate the likelihood that there is a reduction in X between the groups? No, more complicated! And counterintuitive!"
  },
  {
    "objectID": "W7_Hypothesis.html#the-process-of-nhst",
    "href": "W7_Hypothesis.html#the-process-of-nhst",
    "title": "07 Hypothesis Testing",
    "section": "The Process of NHST",
    "text": "The Process of NHST\nTo be more precise, we can break down the process of null hypothesis testing in six steps:\n\n\nFormulate a hypothesis that embodies our prediction (before seeing the data)\nSpecify null and alternative hypotheses that reflect the hypothesis formulated in step 1\nCollect some data relevant to the hypothesis\nFit a model to the data that represents the alternative hypothesis and compute a test statistic\nCompute the probability of the observed value of that statistic assuming that the null hypothesis is true\nAssess the “statistical significance” of the result\n\n\n\nLet’s go through these steps, using the NHANES dataset and the research question: Is physical activity related to body mass index (BMI)?"
  },
  {
    "objectID": "W7_Hypothesis.html#step-1-formulate-a-hypothesis-of-interest",
    "href": "W7_Hypothesis.html#step-1-formulate-a-hypothesis-of-interest",
    "title": "07 Hypothesis Testing",
    "section": "Step 1: Formulate a Hypothesis of Interest",
    "text": "Step 1: Formulate a Hypothesis of Interest\nHypothesis:\n\n“BMI is greater for people who do not engage in physical activity than for those who do.”\n\nask for hypothesis?"
  },
  {
    "objectID": "W7_Hypothesis.html#step-2-specify-the-null-and-alternative-hypotheses",
    "href": "W7_Hypothesis.html#step-2-specify-the-null-and-alternative-hypotheses",
    "title": "07 Hypothesis Testing",
    "section": "Step 2: Specify the Null and Alternative Hypotheses",
    "text": "Step 2: Specify the Null and Alternative Hypotheses\nThe null hypothesis (\\(H_0\\)) is the baseline against which we test our hypothesis of interest.\nThe alternative hypothesis (\\(H_A\\)) describes what we expect if there is an effect.\nNHST works under the assumption that the \\(H_0\\) is true (unless the evidence shows otherwise).\n\nWe also have to decide whether we want to test a non-directional (\\(A \\neq B\\)) or directional (\\(A&gt;B\\) or \\(A&lt;B\\)) hypothesis.\nWhat do we specify if we hypothesize that “BMI is greater…”?\n\n\n\\(H_0 = BMI_{active} \\ge BMI_{inactive}\\)\n\\(H_A = BMI_{active} &lt; BMI_{inactive}\\)\n\nTest against \\(H_0\\): What would we expect the data to look like if there was no effect?\nNon-directional: no direction! :D Directional: prior knowledge"
  },
  {
    "objectID": "W7_Hypothesis.html#step-3-collect-data",
    "href": "W7_Hypothesis.html#step-3-collect-data",
    "title": "07 Hypothesis Testing",
    "section": "Step 3: Collect Data",
    "text": "Step 3: Collect Data\nFor this example, we sample 250 individuals from the NHANES dataset.\n\n\n\nSummary of BMI data for active versus inactive individuals\n\n\nPhysActive\nN\nmean\nsd\n\n\n\n\nNo\n131\n30.1942\n8.9851\n\n\nYes\n119\n26.6386\n5.2499"
  },
  {
    "objectID": "W7_Hypothesis.html#step-4-fit-a-model",
    "href": "W7_Hypothesis.html#step-4-fit-a-model",
    "title": "07 Hypothesis Testing",
    "section": "Step 4: Fit a Model",
    "text": "Step 4: Fit a Model\nWe want to compute a test statistic that helps us decide whether to reject \\(H_0\\) or not.\n\nThe model we fit needs to quantify (= provide the test statistic) the amount of evidence in favor of \\(H_A\\) relative to the variability of the data.\nThe test statistic will have a probability distribution, allowing us to determine how likely our observed value of the statistic is under \\(H_0\\).\n\n\nIn general, we want to relate an effect (e.g., a mean or a difference of means) to the amount of uncertainty in the data (e.g., the SEM).\n\n\nIn the example, we need a test statistic that tests the difference between two means (we have one BMI mean for each group): The t statistic.\n\\[t = \\frac{\\bar{X_1} - \\bar{X_2}}{\\sqrt{\\frac{S_1^2}{n_1} + \\frac{S_2^2}{n_2}}}\\]\n\\(\\bar{X_1}\\) and \\(\\bar{X_2}\\) are the means of the two group, \\(S_1^2\\) and \\(S_2^2\\) are the estimated variances of the groups, \\(n_1\\) and \\(n_2\\) are the sizes of the two groups.\n\n\n\\(\\sqrt{\\frac{S_1^2}{n_1} + \\frac{S_2^2}{n_2}}\\) is something like the pooled (“averaged”) SEM of both groups.\n\nThe t statistic is appropriate for comparing the means of two groups when the sample sizes are relatively small and the population standard deviation is unknown.\n\\(\\sqrt{\\frac{S_1^2}{n_1}}\\) alone would be the SEM of subsample 1. But we have to add first before we take the square root.\nReason for adding: The variance of a difference between (or sum of) two independent variables is the sum of the variances of each individual variable (\\(Var(A−B)=Var(A)+Var(B)\\))\none can view the the t statistic as a way of quantifying how large the difference between groups is in relation to the sampling variability of the difference between means"
  },
  {
    "objectID": "W7_Hypothesis.html#the-t-distribution",
    "href": "W7_Hypothesis.html#the-t-distribution",
    "title": "07 Hypothesis Testing",
    "section": "The t Distribution",
    "text": "The t Distribution\nThe t statistic is distributed according to the t distribution, which looks similar to a normal distribution (the more degrees of freedom, the more “normal”).\nDegrees of freedom for the t test: \\(observations - 2\\) = \\(n_1 + n_2 - 2\\) (when the groups are the same size).\nDegrees of freedom: values that can freely vary when estimating parameters. Usually sample size minus values that you already calculated (e.g. means for the test statistic).\n\n\n\n\n\n\nIf the group sizes are unequal: \\(\\mathrm{d.f.} = \\frac{\\left(\\frac{S_1^2}{n_1} + \\frac{S_2^2}{n_2}\\right)^2}{\\frac{\\left(S_1^2/n_1\\right)^2}{n_1-1} + \\frac{\\left(S_2^2/n_2\\right)^2}{n_2-1}}\\)\nFortunately, R does all these calculations for us :)\n\nDF: we have calculated two means and have thus given up two DFs (these are fixed already)\nWill be smaller if sample sizes unequal (here 241.12 vs 248 for equal)\nDegrees of freedom are the number of independent values that a statistical analysis can estimate. You can also think of it as the number of values that are free to vary as you estimate parameters"
  },
  {
    "objectID": "W7_Hypothesis.html#step-5-determine-the-probability-of-the-observed-result-under-the-null-hypothesis",
    "href": "W7_Hypothesis.html#step-5-determine-the-probability-of-the-observed-result-under-the-null-hypothesis",
    "title": "07 Hypothesis Testing",
    "section": "Step 5: Determine the Probability of the Observed Result under the Null Hypothesis",
    "text": "Step 5: Determine the Probability of the Observed Result under the Null Hypothesis\nWe do not check likelihood of the alternative distribution or likelihood that the null hypothesis is true, but rather:\nHow likely is it, given that we assume \\(H_0\\) is true, to observe a statistic at least as extreme as the one we observed.\n--&gt; We need to know the distribution of the expected statistic, assuming \\(H_0\\) is true. Then we can calculate how (un-)likely it is to find the statistic (or a more extreme value) we found in our data.\n\n\n\n\n\n\n\ncounter-intuitive: We check the nulldistribution not the one of \\(H_A\\)! But we also don’t check how likely it is that \\(H_0\\) is true, but rather the likelihood under the null hypothesis of observing a statistic at least as extreme as the one we observed.\nat least as extreme: Prob of each particular value = 0\ntry to find out how weird statistic found is (or weirder) –&gt; count all weird(er) possibilities"
  },
  {
    "objectID": "W7_Hypothesis.html#a-simple-example",
    "href": "W7_Hypothesis.html#a-simple-example",
    "title": "07 Hypothesis Testing",
    "section": "A Simple Example",
    "text": "A Simple Example\nIs a coin biased if we flip a coin 100x and we get 70 heads?\n\\(H_0: P(heads) \\le 0.5\\) and \\(H_A: P(heads) &gt; 0.5\\)\nTest statistic = number of heads counted.\nHow likely is it that we would observe 70 or more heads if the coin is unbiased (chance of 50% for heads)?\n\nIf we flip a (fair) coin 100 times, we would get the following distribution (100000 replications):\n\n\n\n\n\nIt is very unlikely to get 70 heads if the coin is fair!\n\nfair coin: null distribution"
  },
  {
    "objectID": "W7_Hypothesis.html#p-value",
    "href": "W7_Hypothesis.html#p-value",
    "title": "07 Hypothesis Testing",
    "section": "P-Value",
    "text": "P-Value\nLet’s go back to out BMI example.\nWe first need to calculate the t statistic:\n\n\n\nSummary of BMI data for active versus inactive individuals\n\n\nPhysActive\nN\nmean\nsd\n\n\n\n\nNo\n131\n30.1942\n8.9851\n\n\nYes\n119\n26.6386\n5.2499\n\n\n\n\n\n\\[t = \\frac{\\bar{X_1} - \\bar{X_2}}{\\sqrt{\\frac{S_1^2}{n_1} + \\frac{S_2^2}{n_2}}}\\]\n\\[t = \\frac{30 - 27}{\\sqrt{\\frac{9^2}{131} + \\frac{5.2^2}{119}}}\\]\n\\[t = 3.86\\]\n\nThe question is: What is the likelihood that we would find a t statistic of this size or more extreme given the number of degrees of freedom and if the true difference between the groups is zero.\n\n\n\n\\(t\\) statistics if we use the unrounded values!"
  },
  {
    "objectID": "W7_Hypothesis.html#p-value-2",
    "href": "W7_Hypothesis.html#p-value-2",
    "title": "07 Hypothesis Testing",
    "section": "P-Value 2",
    "text": "P-Value 2\nWe can use the t distribution to calculate this probability. We just need the degrees of freedom, which are \\(DF = 241.12\\), and we can then use all these values (e.g. in a function in R):\n\n\n[1] 7.282672e-05\n\n\nThis small probability tells us that our observed t value is relatively unlikely if \\(H_0\\) is really true.\n\nThis is the p-Value for a directional hypothesis. In this case, we only looked at the upper tail probability. With a non-directional hypothesis, we would want to account for both tail probabilities, i.e. how likely it is that a \\(t &gt; 3.86\\) OR \\(t &lt; -3.86\\) is found. In this case, we can simply multiply the p-Value found above by 2 (since it is a symmetric distribution):\n\\(p = 0.000145\\)"
  },
  {
    "objectID": "W7_Hypothesis.html#p-value-using-randomization",
    "href": "W7_Hypothesis.html#p-value-using-randomization",
    "title": "07 Hypothesis Testing",
    "section": "P-Value using Randomization",
    "text": "P-Value using Randomization\nWe can also use our simulation skills to determine the null distribution!\n\nWe can randomly rearrange (or permute) data so that no relationship is present, e.g. assigning group membership to the participants randomly. In this case, \\(H_0\\) should thus be true.\nWe would do this a large amount of times (e.g. 10000), calculate the t statistics for each iteration, and draw a histogram to show the distribution."
  },
  {
    "objectID": "W7_Hypothesis.html#p-values-using-randomization-2",
    "href": "W7_Hypothesis.html#p-values-using-randomization-2",
    "title": "07 Hypothesis Testing",
    "section": "P-Values using Randomization 2",
    "text": "P-Values using Randomization 2\n\n# create function to shuffle BMI data\nshuffleBMIstat &lt;- function() {\n  bmiDataShuffled &lt;- \n    NHANES_sample %&gt;%\n    select(BMI, PhysActive) %&gt;%\n    mutate(\n      BMI = sample(BMI) #randomly shuffle BMI values\n    )\n  # compute the difference\n  simResult &lt;- t.test( #t.test function is more convenient than pt function!\n    BMI ~ PhysActive,\n    data = bmiDataShuffled,\n  )\n  return(simResult$statistic)\n}\n# run function 5000 times and save output\nnRuns &lt;- 5000\nmeanDiffSimDf &lt;- tibble(meanDiffSim = replicate(nRuns, shuffleBMIstat()))\n#run t test of actual data\nbmtTTest &lt;- \n  t.test(\n  BMI ~ PhysActive,\n  data = NHANES_sample,\n  alternative = \"greater\"\n)\n#compare actual data with simulation\nbmiPvalRand &lt;- \n  mean(meanDiffSimDf$meanDiffSim &gt;= bmtTTest$statistic)\n\n#plot everything\nplot = meanDiffSimDf %&gt;% \n  ggplot(aes(meanDiffSim)) +\n  geom_histogram(bins = 200) +\n  geom_vline(xintercept = bmtTTest$statistic, color = \"blue\") +\n  xlab(\"T stat: BMI difference between groups\") +\n  geom_histogram(\n    data = meanDiffSimDf %&gt;% \n      filter(meanDiffSim &gt;= bmtTTest$statistic), \n    aes(meanDiffSim), \n    bins = 200, \n    fill = \"gray\"\n  )\nprint(plot)"
  },
  {
    "objectID": "W7_Hypothesis.html#p-values-using-randomization-3",
    "href": "W7_Hypothesis.html#p-values-using-randomization-3",
    "title": "07 Hypothesis Testing",
    "section": "P-Values using Randomization 3",
    "text": "P-Values using Randomization 3\n\nThe blue line is the observed t statistic. We can calculate a p-Value by counting how many of the simulated t-values are at least as extreme as our observed one and dividing it by the number of simulations. The p-value obtained from randomization (0.000000) is kind of similar to the one obtained using the t distribution (0.000075).\n\nUsing simulations to get the null distribution can be helpful if the assumptions (normal distribution in each group) are violated or if we don’t know the theoretical distribution of the test statistic!\n\nexchangeability: We can use permutations if all observsations are distributed in the same way, such that we can shuffle them without changing the overall distribution.\nNot the case if we have dependent observations, e.g. siblings…"
  },
  {
    "objectID": "W7_Hypothesis.html#step-6-assess-the-statistical-significance-of-the-result",
    "href": "W7_Hypothesis.html#step-6-assess-the-statistical-significance-of-the-result",
    "title": "07 Hypothesis Testing",
    "section": "Step 6: Assess the “Statistical Significance” of the Result",
    "text": "Step 6: Assess the “Statistical Significance” of the Result\nIs the p-value determined small enough to reject the null hypothesis (and thus conclude that the alternative hypothesis is true)?\n\nTraditionally, we reject \\(H_0\\) if the p-value is less than 0.05. (Fisher’s approach)\n\n\n(Either there is an effect/\\(H_A\\) is true or there is a small chance (5%) that there is actually no effect but we coincidentally found such a large value –&gt; false positive)\n\n\nNeyman-Pearson approach: In the long run, we will know how often we are wrong:\n\n\\(\\alpha = .05\\) (false positives or Type I error: We reject \\(H_0\\) although it is correct),\n\\(\\beta = .2\\) (false negatives or Type II error: We accept \\(H_0\\) although it is wrong),\nWe will be correct if we reject \\(H_0\\) when it is wrong (there is actually a difference/an effect) or if we do not reject \\(H_0\\) when it is correct (and there is no difference between groups).\n\nIn both cases, a significance level of \\(\\alpha = .05\\) is usually used.\n\nHow much evidence do we require?\n0.05 Fisher never intended it to be fixed\nBefore computers, tables were used, and all tables had .05 in it!\nFisher: Evidence for hypothesis, NP: long-run error rate"
  },
  {
    "objectID": "W7_Hypothesis.html#what-does-a-significant-result-mean",
    "href": "W7_Hypothesis.html#what-does-a-significant-result-mean",
    "title": "07 Hypothesis Testing",
    "section": "What does a significant result mean?",
    "text": "What does a significant result mean?\nThere is a lot of discussion about the usefulness of using \\(\\alpha = .05\\) as well as about the interpretation of a significant result/certain p-value!\n\n\nA p-value of .01 does….\n\nNOT mean that the probability that \\(H_0\\) is true is 1%!\n\nWe tested \\(P(data|H_0)\\) not \\(P(H_0|data)\\)!\n\nNOT mean that the probability that you’re making a wrong decision is 1%!\n\nThis would also be \\(P(H_0|data)\\)! p-values are probabilities of data (under \\(H_0\\)), not probabilities of hypotheses! And we cannot easily use Bayes to turn the condition because we would need additional information like the prior probability of an alternative hypothesis being true.\n\nNOT mean that you would get the same significance 99% of the time if you repeated the study.\n\nThe p-value is a statement about the likelihood of one particular dataset under the null.\n\nNOT mean that you found a practically important effect.\n\nDifference between statistical significance and practical significance! Effect sizes more important. (Statistical significance depends on sample size!)"
  },
  {
    "objectID": "W7_Hypothesis.html#multiple-testing",
    "href": "W7_Hypothesis.html#multiple-testing",
    "title": "07 Hypothesis Testing",
    "section": "Multiple Testing",
    "text": "Multiple Testing\nNowadays, we often have huge datasets in neuroscience, e.g. collecting brain imaging data of thousands of voxels or quantifying the entire genome.\n\nLet’s look at genome-wide associations studies (GWAS). We have more than a million places in where the genome could differ. If we want to know whether schizophrenia was associated with any of these differences, we would do ~1.000.000 tests! If we simply used \\(\\alpha \\le .05\\) as a threshold, we would get a lot of (\\(1000000 * .05 = 500\\)!) false positives, even if no true effect is present at all.\n\n\nIn this case, we have a lot of dependent tests, which form a family of tests. In such a case, we need to control the family-wise error rate, e.g. by fixing it to a total of \\(\\alpha \\le .05\\) (i.e. the probability of making any Type I error in our study is controlled at .05).\n\n\nOne option is to use the Bonferroni correction, in which we divide .05 by the number of tests (e.g. 1.000.000) and use the new value (\\(\\alpha \\le .000005\\)) as threshold for each individual test.\n\n\nThis is extremely conservative and often results in false negative test results.\n\n\nFor an interesting example what can happen when not correcting for multiple comparisons, see this dead fish showing significant brain activity (Link)."
  },
  {
    "objectID": "W7_Hypothesis.html#confidence-intervals",
    "href": "W7_Hypothesis.html#confidence-intervals",
    "title": "07 Hypothesis Testing",
    "section": "Confidence Intervals",
    "text": "Confidence Intervals\nSingle value statistic (e.g. t-value, mean…) = point estimate\n\nWe know from the sampling error discussion that each point estimate comes with some uncertainty, described by the standard error.\nRemember, the SEM (standard error of the mean) was calculated with the sample standard deviation \\(\\hat{\\sigma}\\) and the square root of the sample size \\(n\\):\n\\[SEM = \\frac{\\hat{\\sigma}}{\\sqrt{n}}\\]\n\\(n\\) is generally under our control (\\(\\hat{\\sigma}\\) is unknown but fixed*), and we can thus decrease our uncertainty by increasing the sample size.\n\n\n\nWe can more directly describe our uncertainty with confidence intervals (CI), which provides a range of values for our parameter estimate that are consistent with our data! The wider the CI, the more uncertain we are about our estimate.\n\n\n* In practice, we can acquire homogeneous samples (like students) or heterogeneous samples (like extreme groups). This will, however, influence our generalizability."
  },
  {
    "objectID": "W7_Hypothesis.html#confidence-intervals-2",
    "href": "W7_Hypothesis.html#confidence-intervals-2",
    "title": "07 Hypothesis Testing",
    "section": "Confidence Intervals 2",
    "text": "Confidence Intervals 2\nBecause the CI depends on the SEM, which decreases with sample size, the CI also gets narrower with increasing sample size:\n\n\nif we sample the whole population, we know the population parameter and there is no uncertainty at all!\nRange of possible values (estimate) in concordance with the data"
  },
  {
    "objectID": "W7_Hypothesis.html#confidence-intervals-3",
    "href": "W7_Hypothesis.html#confidence-intervals-3",
    "title": "07 Hypothesis Testing",
    "section": "Confidence Intervals 3",
    "text": "Confidence Intervals 3\nJust like p-values, confidence intervals can be confusing because they are counter-intuitive: A 95% CI for a statistic does NOT mean that we can have 95% confidence that the true parameter falls within this interval!\nIt is, again, the long-run probability: It will contain the true population parameter 95% of the time in the long-run.\nLet’s sample 100 times with \\(n = 250\\) from the NHANES data and calculate CIs. We use the NHANES mean as true score (dashed line) and check how often the CI misses to include it: 5% of the time.\n\n\nbut: new experiment = new estimate (under exact same conditions would work)!"
  },
  {
    "objectID": "W7_Hypothesis.html#calculating-the-ci",
    "href": "W7_Hypothesis.html#calculating-the-ci",
    "title": "07 Hypothesis Testing",
    "section": "Calculating the CI",
    "text": "Calculating the CI\nWe calculate the CI as follows:\n\\(CI = \\text{point estimate} \\pm \\text{critical value} * \\text{standard error}\\)\nThe “critical value” depends on the sampling distribution. Most of the time, we will use the normal distribution.\n\nCI depends on the confidence level (95%, critical value), the sample size and the variability in the data (both SE)"
  },
  {
    "objectID": "W7_Hypothesis.html#ci-using-the-normal-distribution",
    "href": "W7_Hypothesis.html#ci-using-the-normal-distribution",
    "title": "07 Hypothesis Testing",
    "section": "CI using the Normal Distribution",
    "text": "CI using the Normal Distribution\nThe critical value are the values of the standard normal distribution that capture 95% (in case of a 95% CI) of the distribution, i.e. the 2.5th and 97.5th percentile.\n\nqnorm(p=c(.025,.975))\n\n[1] -1.959964  1.959964\n\n\nThe CI of the mean would thus be:\n\\(CI = \\bar{X} \\pm 1.96*SE\\)\nOur mean weight in the NHANES sample was 79.92 kg and the SE was \\(\\frac{SD_{weight}}{\\sqrt{n}} = 1.35\\) (for a random \\(n=250\\) subsample)*.\n\nThe lower boundary of the CI of the mean would then be \\(CI = 79.92 - 1.96 * 1.35 = 77.28\\) and the upper \\(CI = 79.92 + 1.96 * 1.35 = 82.56\\). We would write this as [77.28, 82.56].\n\n* In practice, both the mean and standard deviation will be subject to sampling error, i.e., CIs will differ both in location and size despite having the same sample size (cf. plot 2 slides ago)"
  },
  {
    "objectID": "W7_Hypothesis.html#ci-using-the-t-distribution",
    "href": "W7_Hypothesis.html#ci-using-the-t-distribution",
    "title": "07 Hypothesis Testing",
    "section": "CI using the t Distribution",
    "text": "CI using the t Distribution\nIf we don’t know the population standard deviation, which is usually the case, it is more appropriate to use the t distribution.\nIn this case, we use the critical value of the t distribution:\n\nqt(p = c(.025, .975), 250) #t distribution depends on sample size! =&gt; n = 250\n\n[1] -1.969498  1.969498\n\n\nFor the NHANES weight example, the CI would be: \\(79.92 \\pm 1.97 * 1.35 = [77.15, 82.58]\\).\nThe CI for a t distribution is always larger than for a normal distribution. In practice, this difference is negligible at \\(n \\ge 30\\) (1.984 vs. 1.960).\n\nThe t distribution is wider than the normal distribution (especially for smaller samples), which means that the CI will be slightly wider -&gt; extra uncertainty smaller samples.\npopulation parameter hast a fixed value, so it either falls into CI or not. (Doesn’t make sense to talk about probability of it)"
  },
  {
    "objectID": "W7_Hypothesis.html#ci-using-the-bootstrap",
    "href": "W7_Hypothesis.html#ci-using-the-bootstrap",
    "title": "07 Hypothesis Testing",
    "section": "CI using the Bootstrap",
    "text": "CI using the Bootstrap\nIf we can’t assume normality or don’t know the sampling distribution, we can also use the bootstrap to compute the CI.\n\nReminder: bootstrap = resampling with replacement, using this distribution as the sampling distribution!\n\n\nIf we use an R function for bootstrapping (boot()), we get CI estimates that are fairly close to the ones calculated:\n\n\nBOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\nBased on 1000 bootstrap replicates\n\nCALL : \nboot.ci(boot.out = bs, type = \"perc\")\n\nIntervals : \nLevel     Percentile     \n95%   (78.41, 83.81 )  \nCalculations and Intervals on Original Scale\n\n\n\n\n\n\n\n\n\nMethod\nCI\n\n\n\n\nnormal distribution\n[77.28, 82.56]\n\n\nt distribution\n[77.15, 82.58]\n\n\nbootstrap\n[78.41, 83.81]\n\n\n\n\n\nNote: CIs from the normal and t distribution are always symmetrical around the mean (here: 79.92), bootstrapped intervals need not be."
  },
  {
    "objectID": "W7_Hypothesis.html#relationship-of-cis-to-hypothesis-tests",
    "href": "W7_Hypothesis.html#relationship-of-cis-to-hypothesis-tests",
    "title": "07 Hypothesis Testing",
    "section": "Relationship of CIs to Hypothesis Tests",
    "text": "Relationship of CIs to Hypothesis Tests\nIf the CI does not include the value of the null hypothesis (usually 0), then the associated two-sided one sample test would be significant.\n\nIf we want to compare two conditions, it gets trickier.\n\nIf each mean is contained within the CI of the other mean, then there’s definitely no significant difference.\nIf there is no overlap between CIs, then there is certainly a significant difference (two-sidedly).\nIf the CIs overlap (but don’t contain the other mean), it depends on the relative variability of the two variables\n\nIn general, avoid this “eyeball test” and look at the p value!\nNote: Slightly overlapping CIs can still be significant (e.g., one sided hypothesis)"
  },
  {
    "objectID": "W7_Hypothesis.html#effect-sizes",
    "href": "W7_Hypothesis.html#effect-sizes",
    "title": "07 Hypothesis Testing",
    "section": "Effect Sizes",
    "text": "Effect Sizes\nPractical significance!\nWe need a standard way to describe the size of an effect.\nAn effect size is a standardized measurement that compares the size of an effect to e.g. the variability of the statistic. This is also referred to as signal-to-noise ratio.\nThere are many different variants of effect sizes!"
  },
  {
    "objectID": "W7_Hypothesis.html#cohens-d",
    "href": "W7_Hypothesis.html#cohens-d",
    "title": "07 Hypothesis Testing",
    "section": "Cohen’s d",
    "text": "Cohen’s d\nCohen’s d is used to quantify the difference between two means, in terms of their SD:\n\\[d = \\frac{\\bar{X_1} - \\bar{X}_2}{s}\\]\nwhere \\(\\hat{X_1}\\) and \\(\\hat{X_2}\\) are the means of the two groups and \\(s\\) is the pooled SD:\n\\[s = \\sqrt{\\frac{(n_1 - 1)s^2_1 + (n_2 - 1)s^2_2 }{n_1 +n_2 -2}}\\]\nwhich is a combination of both groups’ SDs (\\(s_1\\) and \\(s_2\\)) weighted by their sample size (\\(n_1\\) and \\(n_2\\)).\n\nNote that this is very similar to the t statistic, only the denominator differs:\nt = SEM, d = SD of the data\n–&gt; d will not grow with sample size but remain stable!"
  },
  {
    "objectID": "W7_Hypothesis.html#cohens-d-2",
    "href": "W7_Hypothesis.html#cohens-d-2",
    "title": "07 Hypothesis Testing",
    "section": "Cohen’s d 2",
    "text": "Cohen’s d 2\nThere is a commonly used interpretation of Cohen’s d (although it is criticized to use these cutoffs!):\n\n\n\nInterpetation of Cohen’s d\n\n\nd\nInterpretation\n\n\n\n\n0.0 - 0.2\nnegligible\n\n\n0.2 - 0.5\nsmall\n\n\n0.5 - 0.8\nmedium\n\n\n&gt; 0.8\nlarge\n\n\n\n\n\n\nEven with a large effect of \\(d = 1.78\\) (as in the NHANES data set), the distributions still overlap greatly!\n\n\n\n\n\n\nSmall effect but huge total impact: Psychosocial stress due to Covid-19 (deployed to a whole population)"
  },
  {
    "objectID": "W7_Hypothesis.html#pearsons-r",
    "href": "W7_Hypothesis.html#pearsons-r",
    "title": "07 Hypothesis Testing",
    "section": "Pearson’s r",
    "text": "Pearson’s r\nPearson’s r is a correlation coefficient, and thus a measure of the strength of a linear relationship between two continuous variables.\nr can vary from -1 to 1: -1 is a perfect negative relationship, 0 no (linear) relationship, and 1 a perfect positive relationship. Try your skills eye-balling the size of a correlation: https://www.guessthecorrelation.com/\n\n\nmore on correlations later in the semester!"
  },
  {
    "objectID": "W7_Hypothesis.html#odds-ratio",
    "href": "W7_Hypothesis.html#odds-ratio",
    "title": "07 Hypothesis Testing",
    "section": "Odds Ratio",
    "text": "Odds Ratio\nFor binary variables, the odds ratio is a useful effect size.\nOdds describes the relative likelihood of some event happening versus not happening:\n\\[\n\\text{odds of A} = \\frac{P(A)}{P(\\neg{A})}\n\\]\nOdds ratio is simply the ratio of two odds, i.e. \\(\\frac{\\text{odds of A}}{\\text{odds of B}}\\).\n\nExample:\n\n\n\nLung cancer occurrence separately for current smokers and those who have never smoked\n\n\nStatus\nNeverSmoked\nCurrentSmoker\n\n\n\n\nNo Cancer\n2883\n3829\n\n\nCancer\n220\n6784\n\n\n\n\n\n\\[OR = \\frac{\\frac{220}{220+2883}}{\\frac{6784}{6784+3829}} = 23.22 \\]\nThe odds ratio of 23.22 tells us that the odds of lung cancer in smokers are roughly 23x higher than that of non-smokers!"
  },
  {
    "objectID": "W7_Hypothesis.html#statistical-power",
    "href": "W7_Hypothesis.html#statistical-power",
    "title": "07 Hypothesis Testing",
    "section": "Statistical Power",
    "text": "Statistical Power\nRemember: Type I and Type II error!\nTolerance for Type I errors set to \\(\\alpha = 0.05\\), which is very low –&gt; we want to avoid this error!\nWhat about Type II errors?\n\nType II = failing to reject \\(H_0\\) although an effect exists (often set at \\(\\beta = 0.20\\), i.e., a statistical power of \\(80\\%\\)).\nBut \\(\\beta\\) also depends on the effect size: The likelihood of finding a large effect is higher than finding a small effect (at constant \\(n\\))!\n\n\nStatistical power is the complement of the Type II error:\nThe likelihood of finding a positive result given that it exits!\n\\[power = 1 - \\beta\\]\n\n\nStatistical power is affected by three factors:\n\nsample size (larger n = more power)\neffect size (larger effect = more power)\nType I error rate (smaller Type I error = less power)\n\n\nType I: false positives\nType II: false negatives"
  },
  {
    "objectID": "W7_Hypothesis.html#statistical-power-2",
    "href": "W7_Hypothesis.html#statistical-power-2",
    "title": "07 Hypothesis Testing",
    "section": "Statistical Power 2",
    "text": "Statistical Power 2\nHere, we can see how these three factors influence the power (i.e. the proportion of significant results found):\n\nThe black dotted line denotes the standard 80% power that is often aimed at.\n\nEven with \\(n = 96\\), we have only little power to detect a small effect (\\(d = 0.2\\)): Only ~25% of studies would find the true effect. This means doing this study would be futile, we would likely fail to find the true effect.\n\n\nTherefore, we would do a power analysis before we even run the study - to determine the necessary sample size for a well-powered study that would be able to find an effect if the effect is true.\nFurthermore, positive findings from an underpowered study are more likely to be false positive!\n\nMore on that in Chapter 18 of ST21!"
  }
]